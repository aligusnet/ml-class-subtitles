1
00:00:00,080 --> 00:00:01,360
In this video, I'd like

2
00:00:01,370 --> 00:00:03,380
to start adapting support vector

3
00:00:03,390 --> 00:00:07,620
machines in order to develop complex nonlinear classifiers.

4
00:00:07,630 --> 00:00:11,720
The main technique for doing that is something called kernels.

5
00:00:11,730 --> 00:00:15,190
Let's see what this kernels are and how to use them.

6
00:00:15,860 --> 00:00:17,020
If you have a training set that

7
00:00:17,030 --> 00:00:18,390
looks like this, and you

8
00:00:18,400 --> 00:00:20,140
want to find a

9
00:00:20,150 --> 00:00:22,260
nonlinear decision boundary to distinguish

10
00:00:22,270 --> 00:00:24,340
the positive and negative examples, maybe

11
00:00:24,350 --> 00:00:27,030
a decision boundary that looks like that.

12
00:00:27,040 --> 00:00:28,220
One way to do so is

13
00:00:28,230 --> 00:00:29,960
to come up with a set

14
00:00:29,970 --> 00:00:32,330
of complex polynomial features, right? So, set of

15
00:00:32,340 --> 00:00:34,130
features that looks like this,

16
00:00:34,140 --> 00:00:35,130
so that you end up

17
00:00:35,140 --> 00:00:38,040
with a hypothesis X that

18
00:00:38,050 --> 00:00:40,560
predicts 1 if you know

19
00:00:40,570 --> 00:00:41,850
that theta 0 and plus theta 1 X1

20
00:00:41,860 --> 00:00:45,170
plus dot dot dot all those polynomial features is

21
00:00:45,180 --> 00:00:47,530
greater than 0, and

22
00:00:47,540 --> 00:00:50,670
predict 0, otherwise.

23
00:00:51,070 --> 00:00:52,970
And another way

24
00:00:52,980 --> 00:00:54,830
of writing this, to introduce

25
00:00:54,840 --> 00:00:56,490
a level of new notation that

26
00:00:56,500 --> 00:00:58,190
I'll use later, is that

27
00:00:58,200 --> 00:00:59,720
we can think of a hypothesis

28
00:00:59,730 --> 00:01:02,110
as computing a decision boundary

29
00:01:02,120 --> 00:01:03,810
using this. So, theta

30
00:01:03,820 --> 00:01:05,060
0 plus theta 1 f1 plus

31
00:01:05,070 --> 00:01:06,600
theta 2, f2 plus theta

32
00:01:06,610 --> 00:01:09,580
3, f3 plus and so on.

33
00:01:09,590 --> 00:01:13,040
Where I'm going to

34
00:01:13,050 --> 00:01:14,720
use this new denotation

35
00:01:14,730 --> 00:01:16,260
f1, f2, f3 and so

36
00:01:16,270 --> 00:01:19,110
on to denote these new sort of features

37
00:01:19,350 --> 00:01:21,360
that I'm computing, so f1 is

38
00:01:21,370 --> 00:01:24,590
just X1, f2 is equal

39
00:01:24,600 --> 00:01:27,130
to X2, f3 is

40
00:01:27,140 --> 00:01:28,760
equal to this one

41
00:01:28,770 --> 00:01:29,890
here. So, X1X2. So,

42
00:01:29,900 --> 00:01:33,700
f4 is equal to

43
00:01:33,840 --> 00:01:35,670
X1 squared where f5 is

44
00:01:35,680 --> 00:01:38,510
to be x2 squared and so

45
00:01:38,520 --> 00:01:40,340
on and we seen previously that

46
00:01:40,350 --> 00:01:41,360
coming up with these high

47
00:01:41,370 --> 00:01:43,100
order polynomials is one

48
00:01:43,110 --> 00:01:45,460
way to come up with lots more features,

49
00:01:45,470 --> 00:01:47,240
the question is, is

50
00:01:47,250 --> 00:01:48,660
there a different choice of

51
00:01:48,670 --> 00:01:51,680
features or is there better sort of features than this high order

52
00:01:51,690 --> 00:01:53,820
polynomials because you know

53
00:01:53,830 --> 00:01:55,110
it's not clear that this high

54
00:01:55,120 --> 00:01:56,850
order polynomial is what we want,

55
00:01:56,860 --> 00:01:58,160
and what we talked about

56
00:01:58,170 --> 00:01:59,770
computer vision talk about when

57
00:01:59,780 --> 00:02:02,530
the input is an image with lots of pixels.

58
00:02:02,540 --> 00:02:05,130
We also saw how using high order polynomials

59
00:02:05,140 --> 00:02:07,310
becomes very computationally

60
00:02:07,320 --> 00:02:08,270
expensive because there are

61
00:02:08,280 --> 00:02:11,230
a lot of these higher order polynomial terms.

62
00:02:11,240 --> 00:02:12,420
So, is there a different or

63
00:02:12,430 --> 00:02:14,100
a better choice of the features

64
00:02:14,110 --> 00:02:15,400
that we can use to plug

65
00:02:15,410 --> 00:02:17,490
into this sort of

66
00:02:17,500 --> 00:02:19,410
hypothesis form.

67
00:02:19,420 --> 00:02:20,570
So, here is one idea for how to

68
00:02:20,580 --> 00:02:24,960
define new features f1, f2, f3.

69
00:02:24,970 --> 00:02:26,090
On this line I am

70
00:02:26,100 --> 00:02:27,880
going to define only three new

71
00:02:27,890 --> 00:02:29,490
features, but for real problems

72
00:02:29,500 --> 00:02:31,050
we can get to define a much larger number.

73
00:02:31,060 --> 00:02:32,250
But here's what I'm going to do

74
00:02:32,260 --> 00:02:33,630
in this phase

75
00:02:33,640 --> 00:02:35,390
of features X1, X2, and

76
00:02:35,400 --> 00:02:36,710
I'm going to leave X0

77
00:02:36,720 --> 00:02:38,050
out of this, the

78
00:02:38,060 --> 00:02:39,320
interceptor X0, but

79
00:02:39,330 --> 00:02:41,130
in this phase X1 X2, I'm going to just,

80
00:02:41,140 --> 00:02:42,540


81
00:02:42,550 --> 00:02:43,740
you know, manually pick a few points, and then

82
00:02:43,750 --> 00:02:45,440
call these points l1, we

83
00:02:45,450 --> 00:02:46,810
are going to pick

84
00:02:46,820 --> 00:02:50,070
a different point, let's call

85
00:02:50,080 --> 00:02:51,700
that l2 and let's pick

86
00:02:51,710 --> 00:02:53,160
the third one and call

87
00:02:53,170 --> 00:02:55,890
this one l3, and for

88
00:02:55,900 --> 00:02:56,920
now let's just say that I'm

89
00:02:56,930 --> 00:02:59,860
going to choose these three points manually.

90
00:02:59,870 --> 00:03:03,710
I'm going to call these three points line ups, so line up one, two, three.

91
00:03:03,720 --> 00:03:04,780
What I'm going to do is

92
00:03:04,790 --> 00:03:07,500
define my new features as follows, given

93
00:03:07,510 --> 00:03:10,160
an example X, let me

94
00:03:10,170 --> 00:03:13,320
define my first feature f1

95
00:03:13,330 --> 00:03:16,250
to be some

96
00:03:16,260 --> 00:03:19,320
measure of the similarity between

97
00:03:19,330 --> 00:03:21,670
my training example X and

98
00:03:21,680 --> 00:03:26,510
my first landmark and

99
00:03:26,520 --> 00:03:27,940
this specific formula that I'm

100
00:03:27,950 --> 00:03:30,150
going to use to measure similarity is

101
00:03:30,160 --> 00:03:31,930
going to be this is E to

102
00:03:31,940 --> 00:03:34,460
the minus the length of

103
00:03:34,470 --> 00:03:38,310
X minus l1, squared, divided

104
00:03:38,320 --> 00:03:40,720
by two sigma squared.

105
00:03:40,730 --> 00:03:41,770
So, depending on whether or not

106
00:03:41,780 --> 00:03:44,380
you watched the previous optional video,

107
00:03:44,390 --> 00:03:48,450
this notation, you know, this is

108
00:03:48,460 --> 00:03:49,670
the length of the vector

109
00:03:49,680 --> 00:03:51,450
W. And so, this thing

110
00:03:51,460 --> 00:03:54,010
here, this X

111
00:03:54,020 --> 00:03:56,090
minus l1, this is

112
00:03:56,100 --> 00:03:58,600
actually just the euclidean distance

113
00:03:58,610 --> 00:04:00,400
squared, is the euclidean

114
00:04:00,410 --> 00:04:03,520
distance between the point x and the landmark l1.

115
00:04:03,530 --> 00:04:06,110
We will see more about this later.

116
00:04:06,440 --> 00:04:08,110
But that's my first feature, and

117
00:04:08,120 --> 00:04:09,740
my second feature f2 is

118
00:04:09,750 --> 00:04:12,360
going to be, you know,

119
00:04:12,370 --> 00:04:14,390
similarity function that measures

120
00:04:14,400 --> 00:04:17,360
how similar X is to l2 and the game is going to be defined as

121
00:04:17,370 --> 00:04:19,590
the following function.

122
00:04:19,600 --> 00:04:21,350


123
00:04:21,360 --> 00:04:25,750


124
00:04:25,970 --> 00:04:28,140
This is E to the minus of the square of the euclidean distance

125
00:04:28,150 --> 00:04:29,810
between X and the second

126
00:04:29,820 --> 00:04:31,500
landmark, that is what the enumerator is and

127
00:04:31,510 --> 00:04:33,510
then divided by 2 sigma squared

128
00:04:33,520 --> 00:04:35,840
and similarly f3 is, you know,

129
00:04:35,850 --> 00:04:39,830
similarity between X

130
00:04:39,840 --> 00:04:41,970
and l3, which is

131
00:04:41,980 --> 00:04:46,010
equal to, again, similar formula.

132
00:04:46,550 --> 00:04:48,820
And what this similarity

133
00:04:48,830 --> 00:04:50,720
function is, the mathematical term

134
00:04:50,730 --> 00:04:52,150
for this, is that this is

135
00:04:52,160 --> 00:04:55,330
going to be a kernel function.

136
00:04:55,340 --> 00:04:57,130
And the specific kernel I'm using

137
00:04:57,140 --> 00:05:00,620
here, this is actually called a Gaussian kernel.

138
00:05:00,630 --> 00:05:02,490
And so this formula, this particular

139
00:05:02,500 --> 00:05:05,760
choice of similarity function is called a Gaussian kernel.

140
00:05:05,770 --> 00:05:07,350
But the way the terminology goes is that, you know, in

141
00:05:07,360 --> 00:05:09,590
the abstract these different

142
00:05:09,600 --> 00:05:11,590
similarity functions are called kernels and

143
00:05:11,600 --> 00:05:13,740
we can have different similarity functions

144
00:05:13,750 --> 00:05:17,100
and the specific example I'm giving here is called the Gaussian kernel.

145
00:05:17,110 --> 00:05:18,830
We'll see other examples of other kernels.

146
00:05:18,840 --> 00:05:22,460
But for now just think of these as similarity functions.

147
00:05:22,470 --> 00:05:24,490
And so, instead of writing similarity between

148
00:05:24,500 --> 00:05:26,470
X and l, sometimes we

149
00:05:26,480 --> 00:05:29,060
also write this a kernel denoted

150
00:05:29,070 --> 00:05:33,860
you know, lower case k between x and one of my landmarks all right.

151
00:05:34,120 --> 00:05:36,640
So let's see what a

152
00:05:36,650 --> 00:05:38,800
criminals actually do and

153
00:05:38,810 --> 00:05:41,270
why these sorts of similarity

154
00:05:41,280 --> 00:05:46,040
functions, why these expressions might make sense.

155
00:05:46,690 --> 00:05:48,320
So let's take my first landmark. My

156
00:05:48,330 --> 00:05:49,340
landmark l1, which is

157
00:05:49,350 --> 00:05:52,870
one of those points I chose on my figure just now.

158
00:05:53,000 --> 00:05:55,660
So the similarity of the kernel between x and l1 is given by this expression.

159
00:05:57,530 --> 00:05:58,680
Just to make sure, you know, we

160
00:05:58,690 --> 00:05:59,770
are on the same page about what

161
00:05:59,780 --> 00:06:01,950
the numerator term is, the

162
00:06:01,960 --> 00:06:03,320
numerator can also be

163
00:06:03,330 --> 00:06:04,870
written as a sum from

164
00:06:04,880 --> 00:06:06,990
J equals 1 through N on sort of the distance.

165
00:06:07,000 --> 00:06:09,260
So this is the component wise distance

166
00:06:09,270 --> 00:06:11,060
between the vector X and

167
00:06:11,070 --> 00:06:12,370
the vector l. And again

168
00:06:12,380 --> 00:06:14,710
for the purpose of these

169
00:06:14,720 --> 00:06:16,670
slides I'm ignoring X0.

170
00:06:16,680 --> 00:06:18,210
So just ignoring the intercept

171
00:06:18,220 --> 00:06:21,420
term X0, which is always equal to 1.

172
00:06:21,430 --> 00:06:22,620
So, you know, this is

173
00:06:22,630 --> 00:06:27,260
how you compute the kernel with similarity between X and a landmark.

174
00:06:27,270 --> 00:06:29,100
So let's see what this function does.

175
00:06:29,110 --> 00:06:33,310
Suppose X is close to one of the landmarks.

176
00:06:33,320 --> 00:06:35,350
Then this euclidean distance

177
00:06:35,360 --> 00:06:36,980
formula and the numerator will

178
00:06:36,990 --> 00:06:38,880
be close to 0, right.

179
00:06:38,890 --> 00:06:40,570
So, that is this term

180
00:06:40,580 --> 00:06:42,160
here, the distance was great,

181
00:06:42,170 --> 00:06:43,230
the distance using X and 0

182
00:06:43,240 --> 00:06:46,380
will be close to zero, and so

183
00:06:46,390 --> 00:06:47,700
f1, this is a simple

184
00:06:47,710 --> 00:06:50,280
feature, will be approximately E

185
00:06:50,290 --> 00:06:52,790
to the minus 0 and

186
00:06:52,800 --> 00:06:55,640
then the numerator squared over 2 is equal to squared

187
00:06:55,650 --> 00:06:56,760
so that E to the

188
00:06:56,770 --> 00:06:58,360
0, E to minus 0,

189
00:06:58,370 --> 00:07:01,310
E to 0 is going to be close to one.

190
00:07:01,640 --> 00:07:03,690
And I'll put the approximation symbol here

191
00:07:03,700 --> 00:07:05,520
because the distance may

192
00:07:05,530 --> 00:07:07,110
not be exactly 0, but

193
00:07:07,120 --> 00:07:08,330
if X is closer to landmark

194
00:07:08,340 --> 00:07:09,430
this term will be close

195
00:07:09,440 --> 00:07:13,390
to 0 and so f1 would be close 1.

196
00:07:13,400 --> 00:07:15,510
Conversely, if X is

197
00:07:15,520 --> 00:07:17,540
far from 01 then this

198
00:07:17,550 --> 00:07:19,810
first feature f1 will

199
00:07:19,820 --> 00:07:21,530
be E to the minus

200
00:07:21,540 --> 00:07:24,950
of some large number squared,

201
00:07:24,960 --> 00:07:26,250
divided divided by two sigma

202
00:07:26,260 --> 00:07:27,800
squared and E to

203
00:07:27,810 --> 00:07:29,620
the minus of a large number

204
00:07:29,630 --> 00:07:32,950
is going to be close to 0.

205
00:07:33,320 --> 00:07:34,740
So what these

206
00:07:34,750 --> 00:07:36,280
features do is they measure how

207
00:07:36,290 --> 00:07:37,660
similar X is from one

208
00:07:37,670 --> 00:07:39,520
of your landmarks and the feature

209
00:07:39,530 --> 00:07:40,530
f is going to be close

210
00:07:40,540 --> 00:07:42,530
to one when X is

211
00:07:42,540 --> 00:07:44,010
close to your landmark and is

212
00:07:44,020 --> 00:07:45,370
going to be 0 or close

213
00:07:45,380 --> 00:07:46,780
to zero when X is

214
00:07:46,790 --> 00:07:49,310
far from your landmark.

215
00:07:49,320 --> 00:07:50,580
Each of these landmarks.

216
00:07:50,590 --> 00:07:52,240
On the previous line, I drew

217
00:07:52,250 --> 00:07:55,760
three landmarks, l1, l2,l3.

218
00:07:56,190 --> 00:08:00,650
Each of these landmarks, defines a new feature

219
00:08:00,660 --> 00:08:02,670
f1, f2 and f3.

220
00:08:02,680 --> 00:08:03,700
That is, given the the

221
00:08:03,710 --> 00:08:05,370
training example X, we can

222
00:08:05,380 --> 00:08:06,920
now compute three new

223
00:08:06,930 --> 00:08:09,510
features: f1, f2, and

224
00:08:09,520 --> 00:08:11,330
f3, given, you know, the three

225
00:08:11,340 --> 00:08:13,750
landmarks that I wrote just now.

226
00:08:13,760 --> 00:08:15,230
But first, let's look

227
00:08:15,240 --> 00:08:16,700
at this exponentiation function, let's look

228
00:08:16,710 --> 00:08:18,560
at this similarity function and plot

229
00:08:18,570 --> 00:08:21,220
in some figures and just, you know, understand

230
00:08:21,230 --> 00:08:23,500
better what this really looks like.

231
00:08:23,510 --> 00:08:26,560
For this example, let's say I have two features X1 and X2.

232
00:08:26,570 --> 00:08:27,810
And let's say my first

233
00:08:27,820 --> 00:08:29,510
landmark, l1 is at

234
00:08:29,520 --> 00:08:33,640
a location, 3 5. So

235
00:08:33,650 --> 00:08:36,490
and let's say I set sigma squared equals one for now.

236
00:08:36,500 --> 00:08:37,880
If I plot what this feature

237
00:08:37,890 --> 00:08:41,200
looks like, what I get is this figure.

238
00:08:41,210 --> 00:08:42,750
So the vertical axis, the height

239
00:08:42,760 --> 00:08:45,230
of the surface is the value

240
00:08:45,240 --> 00:08:46,620
of f1 and down here

241
00:08:46,630 --> 00:08:48,700
on the horizontal axis are, if

242
00:08:48,710 --> 00:08:51,650
I have some training example, and there

243
00:08:51,660 --> 00:08:53,310
is x1 and there is x2.

244
00:08:53,320 --> 00:08:55,110
Given a certain training example, the

245
00:08:55,120 --> 00:08:56,970
training example here which shows

246
00:08:56,980 --> 00:08:58,130
the value of x1 and x2

247
00:08:58,140 --> 00:08:59,940
at a height above the surface,

248
00:08:59,950 --> 00:09:02,400
shows the corresponding value of

249
00:09:02,410 --> 00:09:03,950
f1 and down below this is

250
00:09:03,960 --> 00:09:05,030
the same figure I had showed,

251
00:09:05,040 --> 00:09:06,800
using a quantifiable plot, with

252
00:09:06,810 --> 00:09:09,080
x1 on horizontal

253
00:09:09,090 --> 00:09:10,810
axis, x2 on horizontal

254
00:09:10,820 --> 00:09:12,810
axis and so, this

255
00:09:12,820 --> 00:09:13,930
figure on the bottom is just

256
00:09:13,940 --> 00:09:16,530
a contour plot of the 3D surface.

257
00:09:16,540 --> 00:09:18,020
You notice that when

258
00:09:18,030 --> 00:09:19,810
X is equal to

259
00:09:19,820 --> 00:09:24,370
3 5 exactly, then we

260
00:09:24,380 --> 00:09:25,750
the f1 takes on the

261
00:09:25,760 --> 00:09:27,160
value 1, because that's at

262
00:09:27,170 --> 00:09:29,850
the maximum and X

263
00:09:29,860 --> 00:09:31,670
moves away as X goes

264
00:09:31,680 --> 00:09:33,850
further away then this

265
00:09:33,860 --> 00:09:36,450
feature takes on values

266
00:09:36,460 --> 00:09:38,660
that are close to 0.

267
00:09:38,750 --> 00:09:40,390
And so, this is really a feature,

268
00:09:40,400 --> 00:09:42,390
f1 measures, you know, how

269
00:09:42,400 --> 00:09:44,030
close X is to the first

270
00:09:44,040 --> 00:09:46,510
landmark and if

271
00:09:46,520 --> 00:09:47,780
varies between 0 and one

272
00:09:47,790 --> 00:09:49,150
depending on how close X

273
00:09:49,160 --> 00:09:52,150
is to the first landmark l1.

274
00:09:52,360 --> 00:09:53,910
Now the other was due on

275
00:09:53,920 --> 00:09:56,080
this slide is show the effects

276
00:09:56,090 --> 00:10:00,030
of varying this parameter sigma squared.

277
00:10:00,040 --> 00:10:02,520
So, sigma squared is the parameter of the

278
00:10:02,530 --> 00:10:05,140
Gaussian kernel and as you vary it, you get slightly different effects.

279
00:10:05,150 --> 00:10:06,640
Let's set sigma squared to be

280
00:10:06,650 --> 00:10:07,700
equal to 0.5 and see

281
00:10:07,710 --> 00:10:10,080
what we get. We set sigma square to 0.5,

282
00:10:10,090 --> 00:10:11,420
what you find is that the

283
00:10:11,430 --> 00:10:12,720
kernel looks similar, except for the

284
00:10:12,730 --> 00:10:14,780
width of the bump becomes narrower.

285
00:10:14,790 --> 00:10:17,110
The contours shrink a bit too.

286
00:10:17,120 --> 00:10:18,730
So if sigma squared equals to 0.5

287
00:10:18,740 --> 00:10:20,240
then as you start

288
00:10:20,250 --> 00:10:21,900
from X equals 3

289
00:10:21,910 --> 00:10:24,640
5 and as you move away,

290
00:10:24,750 --> 00:10:27,040
then the feature f1

291
00:10:27,050 --> 00:10:28,720
falls to zero much more

292
00:10:28,730 --> 00:10:32,080
rapidly and conversely,

293
00:10:32,090 --> 00:10:34,660
if you has increase since

294
00:10:34,670 --> 00:10:36,500
where three in that

295
00:10:36,510 --> 00:10:37,790
case and as I

296
00:10:37,800 --> 00:10:39,620
move away from, you know l. So

297
00:10:39,630 --> 00:10:41,100
this point here is really

298
00:10:41,110 --> 00:10:42,600
l, right, that's l1 is at

299
00:10:42,610 --> 00:10:46,710
location 3 5, right. So it's shown up here.

300
00:10:48,190 --> 00:10:49,650
And if sigma squared is

301
00:10:49,660 --> 00:10:50,680
large, then as you move

302
00:10:50,690 --> 00:10:54,310
away from l1, the

303
00:10:54,320 --> 00:10:56,730
value of the feature falls

304
00:10:56,740 --> 00:10:59,170
away much more slowly.

305
00:11:03,590 --> 00:11:05,280
So, given this definition of

306
00:11:05,290 --> 00:11:06,950
the features, let's see what

307
00:11:06,960 --> 00:11:09,540
source of hypothesis we can learn.

308
00:11:09,550 --> 00:11:11,470
Given the training example X, we

309
00:11:11,480 --> 00:11:14,430
are going to compute these features

310
00:11:14,670 --> 00:11:17,540
f1, f2, f3 and a

311
00:11:17,550 --> 00:11:19,030
hypothesis is going to

312
00:11:19,040 --> 00:11:20,750
predict one when theta 0

313
00:11:20,760 --> 00:11:22,320
plus theta 1 f1 plus theta 2 f2,

314
00:11:22,330 --> 00:11:26,240
and so on is greater than or equal to 0.

315
00:11:26,250 --> 00:11:27,280
For this particular example, let's say

316
00:11:27,290 --> 00:11:28,610
that I've already found a learning

317
00:11:28,620 --> 00:11:30,180
algorithm and let's say that, you know,

318
00:11:30,190 --> 00:11:31,890
somehow I ended up with

319
00:11:31,900 --> 00:11:33,500
these values of the parameter.

320
00:11:33,510 --> 00:11:34,820
So if theta 0 equals

321
00:11:34,830 --> 00:11:36,380
minus 0.5, theta 1 equals

322
00:11:36,390 --> 00:11:38,170
1, theta 2 equals

323
00:11:38,180 --> 00:11:40,360
1, and theta 3

324
00:11:40,370 --> 00:11:42,710
equals 0 And what

325
00:11:42,720 --> 00:11:44,660
I want to do is consider what

326
00:11:44,670 --> 00:11:46,190
happens if we have a

327
00:11:46,200 --> 00:11:49,250
training example that takes

328
00:11:49,260 --> 00:11:52,500
has location at this

329
00:11:52,510 --> 00:11:55,370
magenta dot, right where I just drew this dot over here.

330
00:11:55,380 --> 00:11:56,280
So let's say I have a training

331
00:11:56,290 --> 00:11:58,990
example X, what would my hypothesis predict?

332
00:11:59,000 --> 00:12:02,930
Well, If I look at this formula.

333
00:12:04,580 --> 00:12:06,040
Because my training example X

334
00:12:06,050 --> 00:12:08,220
is close to l1, we have

335
00:12:08,230 --> 00:12:10,240
that f1 is going

336
00:12:10,250 --> 00:12:12,240
to be close to 1 the because

337
00:12:12,250 --> 00:12:13,350
my training example X is

338
00:12:13,360 --> 00:12:15,350
far from l2 and l3 I

339
00:12:15,360 --> 00:12:17,580
have that, you know, f2 would be close to

340
00:12:17,590 --> 00:12:21,540
0 and f3 will be close to 0.

341
00:12:21,550 --> 00:12:22,870
So, if I look at

342
00:12:22,880 --> 00:12:24,220
that formula, I have theta

343
00:12:24,230 --> 00:12:26,590
0 plus theta 1

344
00:12:26,600 --> 00:12:30,500
times 1 plus theta 2 times some value.

345
00:12:30,510 --> 00:12:33,130
Not exactly 0, but let's say close to 0.

346
00:12:33,140 --> 00:12:37,470
Then plus theta 3 times something close to 0.

347
00:12:37,480 --> 00:12:41,040
And this is going to be equal to plugging in these values now.

348
00:12:41,050 --> 00:12:44,150
So, that gives minus 0.5

349
00:12:44,160 --> 00:12:46,950
plus 1 times 1 which is 1, and so on.

350
00:12:46,960 --> 00:12:47,990
Which is equal to 0.5 which is greater than or equal to 0.

351
00:12:48,000 --> 00:12:51,150
So, at this point,

352
00:12:51,160 --> 00:12:54,730
we're going to predict Y equals

353
00:12:54,740 --> 00:12:58,820
1, because that's greater than or equal to zero.

354
00:12:58,910 --> 00:13:00,790
Now let's take a different point.

355
00:13:00,800 --> 00:13:02,130
Now lets' say I take a

356
00:13:02,140 --> 00:13:03,250
different point, I'm going to

357
00:13:03,260 --> 00:13:04,760
draw this one in a different

358
00:13:04,770 --> 00:13:07,240
color, in cyan say, for

359
00:13:07,250 --> 00:13:08,700
a point out there, if that

360
00:13:08,710 --> 00:13:11,260
were my training example X, then

361
00:13:11,270 --> 00:13:12,940
if you make a similar computation,

362
00:13:12,950 --> 00:13:15,410
you find that f1, f2,

363
00:13:15,420 --> 00:13:18,150
Ff3 are all going to be close to 0.

364
00:13:18,160 --> 00:13:20,230
And so, we have theta

365
00:13:20,240 --> 00:13:24,220
0 plus theta 1, f1,

366
00:13:24,230 --> 00:13:26,190
plus so on and this

367
00:13:26,200 --> 00:13:28,010
will be about equal to

368
00:13:28,020 --> 00:13:31,160
minus 0.5, because theta

369
00:13:31,170 --> 00:13:32,180
0 is minus 0.5 and

370
00:13:32,190 --> 00:13:34,900
f1, f2, f3 are all zero.

371
00:13:34,910 --> 00:13:37,850
So this will be minus 0.5, this is less than zero.

372
00:13:37,860 --> 00:13:39,080
And so, at this

373
00:13:39,090 --> 00:13:40,460
point out there, we're going to

374
00:13:40,470 --> 00:13:43,510
predict Y equals zero.

375
00:13:44,190 --> 00:13:45,260
And if you do

376
00:13:45,270 --> 00:13:46,370
this yourself for a range

377
00:13:46,380 --> 00:13:47,660
of different points, be sure to

378
00:13:47,670 --> 00:13:48,720
convince yourself that if you

379
00:13:48,730 --> 00:13:50,880
have a training example that's

380
00:13:50,890 --> 00:13:52,960
close to L2, say,

381
00:13:52,970 --> 00:13:56,790
then at this point we'll also predict Y equals one.

382
00:13:56,800 --> 00:13:58,230
And in fact, what you end

383
00:13:58,240 --> 00:13:59,340
up doing is, you know,

384
00:13:59,350 --> 00:14:01,130
if you look around this boundary, this

385
00:14:01,140 --> 00:14:02,810
space, what we'll find is that

386
00:14:02,820 --> 00:14:04,080
for points near l1

387
00:14:04,090 --> 00:14:06,540
and l2 we end up predicting positive.

388
00:14:06,550 --> 00:14:08,040
And for points far away from

389
00:14:08,050 --> 00:14:09,460
l1 and l2, that's for

390
00:14:09,470 --> 00:14:12,470
points far away from these two

391
00:14:12,480 --> 00:14:14,380
landmarks, we end up predicting

392
00:14:14,390 --> 00:14:16,500
that the class is equal to 0.

393
00:14:16,510 --> 00:14:17,880
As so, what we end up doing,is

394
00:14:17,890 --> 00:14:20,390
that the decision boundary of

395
00:14:20,400 --> 00:14:22,270
this hypothesis would end

396
00:14:22,280 --> 00:14:24,360
up looking something like this where

397
00:14:24,370 --> 00:14:26,570
inside this red decision boundary

398
00:14:26,580 --> 00:14:28,620
would predict Y equals

399
00:14:28,630 --> 00:14:31,750
1 and outside we predict

400
00:14:32,570 --> 00:14:33,010
Y equals 0.

401
00:14:33,020 --> 00:14:34,840
And so this is

402
00:14:34,850 --> 00:14:36,860
how with this definition

403
00:14:36,870 --> 00:14:39,360
of the landmarks and of the kernel function.

404
00:14:39,370 --> 00:14:41,410
We can learn pretty complex non-linear

405
00:14:41,420 --> 00:14:42,920
decision boundary, like what I

406
00:14:42,930 --> 00:14:44,550
just drew where we predict

407
00:14:44,560 --> 00:14:47,560
positive when we're close to either one of the two landmarks.

408
00:14:47,570 --> 00:14:49,250
And we predict negative when we're

409
00:14:49,260 --> 00:14:50,940
very far away from any

410
00:14:50,950 --> 00:14:53,430
of the landmarks.

411
00:14:53,440 --> 00:14:55,040
And so this is part of

412
00:14:55,050 --> 00:14:57,590
the idea of kernels of and

413
00:14:57,600 --> 00:14:58,760
how we use them with the

414
00:14:58,770 --> 00:14:59,980
support vector machine, which is that

415
00:14:59,990 --> 00:15:02,030
we define these extra features using

416
00:15:02,040 --> 00:15:04,760
landmarks and similarity functions

417
00:15:04,770 --> 00:15:08,200
to learn more complex nonlinear classifiers.

418
00:15:08,210 --> 00:15:09,380
So hopefully that gives you

419
00:15:09,390 --> 00:15:10,580
a sense of the idea of

420
00:15:10,590 --> 00:15:11,880
kernels and how we could

421
00:15:11,890 --> 00:15:15,500
use it to define new features for the Support Vector Machine.

422
00:15:15,510 --> 00:15:18,000
But there are a couple of questions that we haven't answered yet.

423
00:15:18,010 --> 00:15:20,110
One is, how do we get these landmarks?

424
00:15:20,120 --> 00:15:21,040
How do we choose these landmarks?

425
00:15:21,050 --> 00:15:23,080
And another is, what

426
00:15:23,090 --> 00:15:24,740
other similarity functions, if any,

427
00:15:24,750 --> 00:15:25,770
can we use other than the

428
00:15:25,780 --> 00:15:29,180
one we talked about, which is called the Gaussian kernel.

429
00:15:29,190 --> 00:15:29,980
In the next video we give

430
00:15:29,990 --> 00:15:31,480
answers to these questions and put

431
00:15:31,490 --> 00:15:33,730
everything together to show how

432
00:15:33,740 --> 00:15:35,710
support vector machines with kernels

433
00:15:35,720 --> 00:15:37,190
can be a powerful way

434
00:15:37,200 --> 00:15:40,110
to learn complex nonlinear functions.

