1
00:00:00,090 --> 00:00:01,500
Suppose you like to decide

2
00:00:01,510 --> 00:00:03,480
what degree of polynomial to

3
00:00:03,490 --> 00:00:04,810
fit to a data set, sort of

4
00:00:04,820 --> 00:00:08,760
what features to include to give you a learning algorithm.

5
00:00:08,770 --> 00:00:10,220
Or suppose you'd like to choose

6
00:00:10,230 --> 00:00:14,470
the regularization parameter lambda for the learning algorithm.

7
00:00:14,480 --> 00:00:15,770
How do you do that?

8
00:00:15,780 --> 00:00:18,260
These are called model selection problems.

9
00:00:18,270 --> 00:00:19,590
And in our discussion of

10
00:00:19,600 --> 00:00:21,060
how to do this we'll talk

11
00:00:21,070 --> 00:00:22,140
about not just how to

12
00:00:22,150 --> 00:00:23,870
split your data into a train

13
00:00:23,880 --> 00:00:25,470
and test sets but how to

14
00:00:25,480 --> 00:00:27,130
split your data into what

15
00:00:27,140 --> 00:00:28,660
we'll discover is called the

16
00:00:28,670 --> 00:00:31,020
train validation and test sets.

17
00:00:31,030 --> 00:00:32,490
We'll see in this video

18
00:00:32,500 --> 00:00:33,430
just what these things are and

19
00:00:33,440 --> 00:00:36,830
how to use them to do model selection.

20
00:00:36,840 --> 00:00:38,050
We've already seen a lot

21
00:00:38,060 --> 00:00:40,110
of times the problem of overfitting,

22
00:00:40,120 --> 00:00:41,630
in which just because the

23
00:00:41,640 --> 00:00:43,550
learning algorithm fits a training

24
00:00:43,560 --> 00:00:47,060
set well, that doesn't mean there's a good hypothesis.

25
00:00:47,440 --> 00:00:48,960
More generally, this is why

26
00:00:48,970 --> 00:00:50,590
the training set error is

27
00:00:50,600 --> 00:00:52,480
not a good predictor for how

28
00:00:52,490 --> 00:00:55,930
well the hypothesis will do on new examples.

29
00:00:55,940 --> 00:00:57,660
Concretely, if you fit

30
00:00:57,670 --> 00:00:59,070
some set of parameters - theta

31
00:00:59,080 --> 00:01:00,300
0, theta 1, theta 2

32
00:01:00,310 --> 00:01:01,670
and so on - to your

33
00:01:01,680 --> 00:01:04,250
training set then, the fact

34
00:01:04,260 --> 00:01:05,860
that your hypothesis does well in

35
00:01:05,870 --> 00:01:07,560
the training set, well, this

36
00:01:07,570 --> 00:01:08,990
doesn't mean much in terms

37
00:01:09,000 --> 00:01:10,410
of predicting how well your

38
00:01:10,420 --> 00:01:12,730
hypothesis will generalize to new

39
00:01:12,740 --> 00:01:14,490
examples not seen in

40
00:01:14,500 --> 00:01:16,030
the training set.

41
00:01:16,040 --> 00:01:18,310
And the more general principal is that,

42
00:01:18,320 --> 00:01:19,930
once your parameters were fit

43
00:01:19,940 --> 00:01:21,500
to some set of data--maybe the

44
00:01:21,510 --> 00:01:23,550
training set, maybe something else--then

45
00:01:23,560 --> 00:01:25,610
the error of your hypothesis

46
00:01:25,620 --> 00:01:26,940
as measured on that same

47
00:01:26,950 --> 00:01:28,490
data set, such as the

48
00:01:28,500 --> 00:01:31,600
training error, that's unlikely

49
00:01:31,790 --> 00:01:33,680
to be a good estimate

50
00:01:33,690 --> 00:01:35,730
of your actual generalization error, that

51
00:01:35,740 --> 00:01:37,550
is, of how well the

52
00:01:37,560 --> 00:01:40,750
hypothesis will generalize to new examples.

53
00:01:43,020 --> 00:01:45,900
Now let's consider the model selection problem.

54
00:01:45,910 --> 00:01:47,460
Let's say you try to choose

55
00:01:47,470 --> 00:01:50,010
what degree polynomial to fit to data.

56
00:01:50,020 --> 00:01:51,550
So, you should you choose a linear function, a

57
00:01:51,560 --> 00:01:53,450
quadratic function, a cubic function,

58
00:01:53,460 --> 00:01:57,520
all the way up to a 10th power polynomial?

59
00:01:57,530 --> 00:01:59,630
So it's as if there's one extra parameter in

60
00:01:59,640 --> 00:02:00,880
this algorithm, which I'm going

61
00:02:00,890 --> 00:02:02,990
to denote d, which is

62
00:02:03,000 --> 00:02:07,040
what degree of polynomial do you want to pick?

63
00:02:07,050 --> 00:02:08,140
So it is as if

64
00:02:08,150 --> 00:02:11,070
does this, in addition

65
00:02:11,080 --> 00:02:13,260
to the theta parameters it's

66
00:02:13,270 --> 00:02:14,980
as if there's one more parameter d

67
00:02:14,990 --> 00:02:18,420
that your trying to determine using your data cells.

68
00:02:18,430 --> 00:02:19,980
the first option is d

69
00:02:19,990 --> 00:02:21,260
equals 1, which is for the linear

70
00:02:21,270 --> 00:02:23,500
function we can choose d

71
00:02:23,510 --> 00:02:24,880
equals 2, d equals 3,

72
00:02:24,890 --> 00:02:26,110
all the way up to d

73
00:02:26,120 --> 00:02:27,080
equals 10, so we would like

74
00:02:27,090 --> 00:02:29,800
to fit this extra sort of parameter,

75
00:02:29,810 --> 00:02:31,640
which I am denoting by d,

76
00:02:31,650 --> 00:02:33,520
and concretely, let's say

77
00:02:33,530 --> 00:02:35,020
that you want to choose a

78
00:02:35,030 --> 00:02:36,490
model, that is choose

79
00:02:36,500 --> 00:02:38,420
a degree of polynomial choose one off

80
00:02:38,430 --> 00:02:41,240
these ten models, and fit that model

81
00:02:41,250 --> 00:02:43,620
and also get some estimate

82
00:02:43,630 --> 00:02:46,120
of how well your fitted hypothesis

83
00:02:46,130 --> 00:02:48,380
will generalize to new

84
00:02:48,390 --> 00:02:49,990
examples.

Here's one thing

85
00:02:50,000 --> 00:02:52,300
you could do: you could

86
00:02:52,310 --> 00:02:54,500
take your first model and

87
00:02:54,510 --> 00:02:56,660
minimize the training error

88
00:02:56,670 --> 00:02:57,810
and this would give you some

89
00:02:57,820 --> 00:03:01,070
parameter vector theta, and

90
00:03:01,080 --> 00:03:02,750
you can then take your second model,

91
00:03:02,760 --> 00:03:05,450
the quadratic function and

92
00:03:05,460 --> 00:03:06,940
for that your training set and

93
00:03:06,950 --> 00:03:09,830
this will give you some other parameters vector theta.

94
00:03:09,840 --> 00:03:11,310
In order to distinguish between

95
00:03:11,320 --> 00:03:13,430
these different parameter vectors, I'm

96
00:03:13,440 --> 00:03:15,860
going to use a superscript 1, superscript 2

97
00:03:15,870 --> 00:03:18,080
there where theta superscript

98
00:03:18,090 --> 00:03:19,640
1 just means the parameters

99
00:03:19,650 --> 00:03:21,140
I get by fitting this

100
00:03:21,150 --> 00:03:23,010
model to my training data,

101
00:03:23,020 --> 00:03:25,190
and theta superscript 2 just means

102
00:03:25,200 --> 00:03:26,970
the parameters I get by fitting

103
00:03:26,980 --> 00:03:30,760
this quadratic function to my training ata and so on.

104
00:03:30,770 --> 00:03:34,170
And by fitting a cubic model I get parameters theta 3

105
00:03:34,180 --> 00:03:36,990
up to, you know, say theta 10.

106
00:03:37,000 --> 00:03:38,750
And one thing we could

107
00:03:38,760 --> 00:03:40,700
do is then take these

108
00:03:40,710 --> 00:03:42,940
parameters and look at the test set error.

109
00:03:42,950 --> 00:03:44,340
So I can compute on my

110
00:03:44,350 --> 00:03:46,540
test set, j test

111
00:03:46,550 --> 00:03:50,910
of 1, j test of

112
00:03:50,920 --> 00:03:52,890
theta 2 and so

113
00:03:52,900 --> 00:03:55,860
on, j test of

114
00:03:55,870 --> 00:03:58,200
theta 3 and so on.

115
00:03:58,210 --> 00:04:00,390
So I'm going to

116
00:04:00,400 --> 00:04:02,440
take each of my hypothesis

117
00:04:02,450 --> 00:04:05,180
with the corresponding and just measure

118
00:04:05,190 --> 00:04:07,970
the performance on the test set.

119
00:04:08,870 --> 00:04:10,340
Now one thing I could do

120
00:04:10,350 --> 00:04:12,570
then is, in order to select

121
00:04:12,580 --> 00:04:13,870
one of these models, I could

122
00:04:13,880 --> 00:04:15,940
then see which model

123
00:04:15,950 --> 00:04:17,340
has the lowest test sets

124
00:04:17,350 --> 00:04:18,420
error, and lets just say

125
00:04:18,430 --> 00:04:19,960
for this example, that I

126
00:04:19,970 --> 00:04:23,140
ended up choosing the fifth order polynomial.

127
00:04:23,150 --> 00:04:25,540
So this seems reasonable so far.

128
00:04:25,550 --> 00:04:27,300
By now, lets say, I want to

129
00:04:27,310 --> 00:04:30,030
take my fit hypothesis, this fifth

130
00:04:30,040 --> 00:04:32,170
order model and let's

131
00:04:32,180 --> 00:04:35,680
say I want to ask how well this model generalized.

132
00:04:35,750 --> 00:04:37,120
One thing I could do is

133
00:04:37,130 --> 00:04:38,850
look at how well my fifth

134
00:04:38,860 --> 00:04:43,130
order polynomial hypothesis, had done on my test set.

135
00:04:43,140 --> 00:04:45,040
But the problem is this

136
00:04:45,050 --> 00:04:46,420
will not to be a fair

137
00:04:46,430 --> 00:04:48,110
estimate of how well

138
00:04:48,120 --> 00:04:50,630
my hypothesis generalizes.

139
00:04:51,050 --> 00:04:53,570
And the reason is, what we've

140
00:04:53,580 --> 00:04:55,010
done is, we've fit this

141
00:04:55,020 --> 00:04:56,940
extra the parameter d, that

142
00:04:56,950 --> 00:04:59,100
is this degree of polynomial,

143
00:04:59,110 --> 00:05:01,030
and we'll fit that parameter

144
00:05:01,040 --> 00:05:02,890
d using the test set.

145
00:05:02,900 --> 00:05:04,270
Namely, we chose the value

146
00:05:04,280 --> 00:05:05,700
of d that gave us the

147
00:05:05,710 --> 00:05:07,340
best possible performance on the

148
00:05:07,350 --> 00:05:09,300
test set, and

149
00:05:09,310 --> 00:05:11,550
so, the performance of

150
00:05:11,560 --> 00:05:13,670
my parameter vector theta five

151
00:05:13,680 --> 00:05:15,500
on the test set, that's likely to

152
00:05:15,510 --> 00:05:17,490
be to be an overly optimistic

153
00:05:17,500 --> 00:05:19,950
estimate of generalization error.

154
00:05:19,960 --> 00:05:21,240
Right? So that because I have fit

155
00:05:21,250 --> 00:05:22,350
this parameter d to my

156
00:05:22,360 --> 00:05:23,870
test set, it is no

157
00:05:23,880 --> 00:05:25,500
longer fair to

158
00:05:25,510 --> 00:05:28,330
evaluate my hypothesis on this test set.

159
00:05:28,340 --> 00:05:30,690
That's because I've fit my parameters to the test set.

160
00:05:30,700 --> 00:05:32,400
I've chosen the degree d of

161
00:05:32,410 --> 00:05:34,720
polynomial using the test set. And

162
00:05:34,730 --> 00:05:36,520
so our hypothesis is

163
00:05:36,530 --> 00:05:38,060
likely to do better on

164
00:05:38,070 --> 00:05:39,440
this test set than it

165
00:05:39,450 --> 00:05:41,110
would on new examples that

166
00:05:41,120 --> 00:05:45,580
it hasn't seen before and that's which is what we hear about.

167
00:05:45,590 --> 00:05:46,250
So, just to reiterate

168
00:05:46,260 --> 00:05:47,830
on the previous slide we

169
00:05:47,840 --> 00:05:49,140
saw that if we fit

170
00:05:49,150 --> 00:05:51,340
some set of parameters, say theta

171
00:05:51,350 --> 00:05:52,550
0, theta 1, and so on, to

172
00:05:52,560 --> 00:05:54,090
some training set, then the

173
00:05:54,100 --> 00:05:56,000
performance of the fitted model on

174
00:05:56,010 --> 00:05:58,010
the training set is not

175
00:05:58,020 --> 00:05:59,880
predictive of how well

176
00:05:59,890 --> 00:06:01,530
the hypothesis we generalized the

177
00:06:01,540 --> 00:06:03,870
new examples; is because these

178
00:06:03,880 --> 00:06:05,420
parameters would fit to the training set.

179
00:06:05,430 --> 00:06:06,660
So they are likely to do

180
00:06:06,670 --> 00:06:08,020
well on the training set, even

181
00:06:08,030 --> 00:06:11,890
if the parameters don't do well on other examples.

182
00:06:12,390 --> 00:06:13,780
And in the procedure I've just described

183
00:06:13,790 --> 00:06:14,900
on this slide, we've just done

184
00:06:14,910 --> 00:06:16,960
the same thing and specifically

185
00:06:16,970 --> 00:06:18,750
what we did is we fit this

186
00:06:18,760 --> 00:06:21,730
parameter d to the test set.

187
00:06:21,740 --> 00:06:23,180
And by having fit the parameter

188
00:06:23,190 --> 00:06:25,400
to the test set, this means that

189
00:06:25,410 --> 00:06:27,070
the performance of the hypothesis

190
00:06:27,080 --> 00:06:28,730
on that test set may not

191
00:06:28,740 --> 00:06:30,620
be a fair estimate of how

192
00:06:30,630 --> 00:06:33,100
well the hypothesis is likely to do

193
00:06:33,110 --> 00:06:35,500
on examples we haven't seen before.

194
00:06:35,510 --> 00:06:37,310
To address this problem in a

195
00:06:37,320 --> 00:06:39,460
model selection setting, if

196
00:06:39,470 --> 00:06:41,790
we want to evaluate a hypothesis

197
00:06:41,800 --> 00:06:45,590
this is usually what we do instead.

198
00:06:45,600 --> 00:06:47,060
Given the data set, instead of just splitting it

199
00:06:47,070 --> 00:06:48,470
into a train and test

200
00:06:48,480 --> 00:06:49,420
set, what we are going to

201
00:06:49,430 --> 00:06:52,610
do is instead split it into three pieces.

202
00:06:52,620 --> 00:06:54,700
And the first piece is

203
00:06:54,710 --> 00:06:58,720
going to be called the training set as usual.

204
00:06:58,730 --> 00:07:01,010
So you call this first part,

205
00:07:01,020 --> 00:07:03,720
the training set, and then

206
00:07:03,730 --> 00:07:06,030
were going to coddle the second

207
00:07:06,040 --> 00:07:07,580
piece of data, which is

208
00:07:07,590 --> 00:07:13,680
called the cross-validation set, and

209
00:07:13,690 --> 00:07:17,010
I'm going to abbreviate cross-validation

210
00:07:17,040 --> 00:07:19,550
CV, and the second

211
00:07:19,560 --> 00:07:21,030
piece of this data, I'm going

212
00:07:21,040 --> 00:07:24,180
to call the cross-validation set

213
00:07:26,430 --> 00:07:28,260
cross-validation, and I

214
00:07:28,270 --> 00:07:32,140
am going to abbreviate cross-validation as CV.

215
00:07:32,150 --> 00:07:33,520
Sometimes it's also called the

216
00:07:33,530 --> 00:07:36,670
validation set, instead of cross-validation set.

217
00:07:36,680 --> 00:07:37,800
And then the last part I

218
00:07:37,810 --> 00:07:40,870
am going to call my usual test set.

219
00:07:40,880 --> 00:07:43,360
And the pretty typical ratio

220
00:07:43,370 --> 00:07:45,500
I wish to split these things; would be to

221
00:07:45,510 --> 00:07:47,000
send 60% of your data

222
00:07:47,010 --> 00:07:48,870
to your training set, maybe 20%

223
00:07:48,880 --> 00:07:52,610
to your cross-validation set, and 20% to your test set.

224
00:07:52,620 --> 00:07:53,960
And these numbers can vary a little

225
00:07:53,970 --> 00:07:56,710
bit but this sort of ratio will be pretty typical.

226
00:07:56,720 --> 00:07:58,550
And so our training set

227
00:07:58,560 --> 00:08:00,170
will now be only, maybe

228
00:08:00,180 --> 00:08:02,290
60% of the data,

229
00:08:02,300 --> 00:08:04,680
and our cross-validation set or

230
00:08:04,690 --> 00:08:08,160
our validation set will have some number of examples.

231
00:08:08,170 --> 00:08:09,640
I'm going to denote that M

232
00:08:09,650 --> 00:08:11,150
subscript cv, so that's the

233
00:08:11,160 --> 00:08:15,280
number of cross-validation examples.

234
00:08:15,580 --> 00:08:17,410
And as following our earlier

235
00:08:17,420 --> 00:08:19,930
notational convention, I'm going

236
00:08:19,940 --> 00:08:23,680
to use XiCV, YiCV.

237
00:08:23,690 --> 00:08:25,500
Following our earlier notational convention

238
00:08:25,510 --> 00:08:26,720
I'm going to use

239
00:08:26,730 --> 00:08:30,870
XiCV, YiCV to

240
00:08:30,880 --> 00:08:34,400
denote the i cross-validation example.

241
00:08:34,410 --> 00:08:36,390
And finally we also

242
00:08:36,400 --> 00:08:38,000
have a test set over here;

243
00:08:38,010 --> 00:08:40,690
with M subscript test,

244
00:08:40,700 --> 00:08:43,080
being the number of test examples.

245
00:08:43,090 --> 00:08:44,150
So, now that we have

246
00:08:44,160 --> 00:08:46,160
defined the training validation or

247
00:08:46,170 --> 00:08:48,150
cross validation and test sets,

248
00:08:48,160 --> 00:08:49,880
we can also define the training

249
00:08:49,890 --> 00:08:53,060
error, cross validation error, and test error.

250
00:08:53,070 --> 00:08:55,240
So here's my training error,

251
00:08:55,250 --> 00:08:56,560
and I'm just writing this as

252
00:08:56,570 --> 00:08:58,490
J j subscript train of theta.

253
00:08:58,500 --> 00:09:00,660
This is pretty much the same thing.

254
00:09:00,670 --> 00:09:02,160
It's usually the same thing as

255
00:09:02,170 --> 00:09:04,150
the j of theta that we'll be writing so far,

256
00:09:04,160 --> 00:09:05,760
this is just a training set

257
00:09:05,770 --> 00:09:08,430
error you guys measure on your training set.

258
00:09:08,440 --> 00:09:09,940
And then j subscript cv

259
00:09:09,950 --> 00:09:13,090
is my cause validation error is pretty much what you'd expect.

260
00:09:13,100 --> 00:09:14,840
Just select the training error, except

261
00:09:14,850 --> 00:09:16,070
measure it on the

262
00:09:16,080 --> 00:09:18,210
cross-validation data set, and here's

263
00:09:18,220 --> 00:09:21,160
my test set error, same as before.

264
00:09:21,170 --> 00:09:22,800
So when theta with the model

265
00:09:22,810 --> 00:09:24,990
selection problem like this

266
00:09:25,000 --> 00:09:27,180
is, instead of using

267
00:09:27,190 --> 00:09:29,210
the test set to select

268
00:09:29,220 --> 00:09:30,690
the model, we're instead going

269
00:09:30,700 --> 00:09:32,480
to use validation set or

270
00:09:32,490 --> 00:09:36,220
the cross-validation set to select the model.

271
00:09:36,230 --> 00:09:37,880
Concretely, we're going to

272
00:09:37,890 --> 00:09:40,850
first take our first hypothesis, take

273
00:09:40,860 --> 00:09:43,180
this first model and say,

274
00:09:43,190 --> 00:09:45,110
minimize the cos function,

275
00:09:45,120 --> 00:09:46,130
and this would give me some

276
00:09:46,140 --> 00:09:49,240
parameter vector theta for the linear model

277
00:09:49,250 --> 00:09:50,860
and as before I'm going to put the

278
00:09:50,870 --> 00:09:52,400
superscript 1 just to

279
00:09:52,410 --> 00:09:53,710
denote that this is a parameter

280
00:09:53,720 --> 00:09:55,440
for the linear model. We do

281
00:09:55,450 --> 00:09:58,240
the same thing for the

282
00:09:58,250 --> 00:10:00,670
quadratic model, get some

283
00:10:00,680 --> 00:10:01,840
parameter vector theta 2, get

284
00:10:01,850 --> 00:10:03,710
some parameter vectors there 3, and so on down

285
00:10:03,720 --> 00:10:05,460
to, say, the tenth by the

286
00:10:05,470 --> 00:10:07,460
polynomial, and what

287
00:10:07,470 --> 00:10:09,340
we I'm going to do is, instead of

288
00:10:09,350 --> 00:10:11,190
testing these hypothesis on the

289
00:10:11,200 --> 00:10:12,580
test set, instead I'm

290
00:10:12,590 --> 00:10:13,660
going to test them on the

291
00:10:13,670 --> 00:10:15,390
cross-validation set. I'm going to

292
00:10:15,400 --> 00:10:18,260
measure j subscript cv,

293
00:10:18,270 --> 00:10:19,870
to see how well each of

294
00:10:19,880 --> 00:10:22,620
these hypothesis do on my

295
00:10:22,630 --> 00:10:29,100
cross validation set and then

296
00:10:29,110 --> 00:10:30,680
I'm going to pick the hypothesis

297
00:10:30,690 --> 00:10:32,770
with the lowest cross-validation error.

298
00:10:32,780 --> 00:10:34,480
So for this example, let's say

299
00:10:34,490 --> 00:10:36,440
for the sake of argument that

300
00:10:36,450 --> 00:10:39,280
it was my fourth order polynomial

301
00:10:39,290 --> 00:10:42,080
that had the lowest cross-validation error.

302
00:10:42,090 --> 00:10:43,210
So in that case, I'm going

303
00:10:43,220 --> 00:10:45,280
to pick this fourth order polynomial

304
00:10:45,290 --> 00:10:48,090
model and finally what

305
00:10:48,100 --> 00:10:50,130
this means is that that parameter

306
00:10:50,140 --> 00:10:51,620
d, remember d was the

307
00:10:51,630 --> 00:10:54,890
degree of polynomial, right d equals 2, d equals 3,

308
00:10:54,900 --> 00:10:57,060
up to d equals 10. What we've

309
00:10:57,070 --> 00:10:58,450
done is we fit that parameter

310
00:10:58,460 --> 00:10:59,890
d, and we'll set d equals 4, and

311
00:10:59,900 --> 00:11:01,590
we did so using

312
00:11:01,600 --> 00:11:02,910
the cross-validation set.

313
00:11:02,920 --> 00:11:04,530
And so this degree of

314
00:11:04,540 --> 00:11:06,520
polynomial, so the parameter

315
00:11:06,530 --> 00:11:08,820
is no longer fit to the test set.

316
00:11:08,830 --> 00:11:10,510
And so we've now saved

317
00:11:10,520 --> 00:11:13,290
a way the test set

318
00:11:13,300 --> 00:11:14,320
and we can use the test

319
00:11:14,330 --> 00:11:16,090
set to measure or to

320
00:11:16,100 --> 00:11:18,040
estimate the generalization error of

321
00:11:18,050 --> 00:11:20,360
the model that was selected

322
00:11:20,370 --> 00:11:23,670
by this algorithm.

So, that

323
00:11:23,680 --> 00:11:25,540
was model selection and how

324
00:11:25,550 --> 00:11:26,870
you can take your data and split

325
00:11:26,880 --> 00:11:28,930
it into a train validation

326
00:11:28,940 --> 00:11:30,290
and test set, and use your

327
00:11:30,300 --> 00:11:32,100
cross validation data to select

328
00:11:32,110 --> 00:11:35,070
model and evaluate it on the test set.

329
00:11:35,080 --> 00:11:36,800
One final note: I should

330
00:11:36,810 --> 00:11:39,080
say that in the machine

331
00:11:39,090 --> 00:11:40,480
learning as of this practice

332
00:11:40,490 --> 00:11:41,720
today, there are many

333
00:11:41,730 --> 00:11:43,430
people that will do

334
00:11:43,440 --> 00:11:44,560
that early thing that I

335
00:11:44,570 --> 00:11:45,990
talked about, and said that

336
00:11:46,000 --> 00:11:48,100
isn't such a good idea of

337
00:11:48,110 --> 00:11:49,740
selecting your model using the

338
00:11:49,750 --> 00:11:51,570
test set and they're using

339
00:11:51,580 --> 00:11:53,280
the same test set to report

340
00:11:53,290 --> 00:11:56,010
the error, as though selecting

341
00:11:56,020 --> 00:11:57,500
your degree of polynomial on the

342
00:11:57,510 --> 00:11:59,210
test set, and then reporting

343
00:11:59,220 --> 00:12:00,630
the error on the test set as

344
00:12:00,640 --> 00:12:03,430
though that were good estimate of generalization error.

345
00:12:03,440 --> 00:12:05,720
That sort of practice is unfortunately many

346
00:12:05,730 --> 00:12:06,880
people do do it; and

347
00:12:06,890 --> 00:12:08,450
if you have a massive massive

348
00:12:08,460 --> 00:12:09,690
test set is maybe not

349
00:12:09,700 --> 00:12:12,810
a terrible thing to do, but

350
00:12:13,060 --> 00:12:15,430
most practitioners of machine

351
00:12:15,440 --> 00:12:18,050
learning tend to advise

352
00:12:19,330 --> 00:12:20,320
against that

353
00:12:20,330 --> 00:12:22,550
and is considered better practice to have separate training validations of test sets. I'll just warn you that just

354
00:12:22,560 --> 00:12:24,120
sometimes people do

355
00:12:24,130 --> 00:12:25,540
you know, use the same data

356
00:12:25,550 --> 00:12:28,250
for the purpose of the validation

357
00:12:28,260 --> 00:12:29,480
set and for the purpose

358
00:12:29,490 --> 00:12:30,820
of the test sets. You only have a training set

359
00:12:30,830 --> 00:12:32,510
and the test set and that's because

360
00:12:32,520 --> 00:12:33,800
that's good practice. So, you

361
00:12:33,810 --> 00:12:35,530
will see some people do it

362
00:12:35,540 --> 00:12:37,100
but if possible I will

363
00:12:37,110 --> 00:12:40,210
recommend against doing.

