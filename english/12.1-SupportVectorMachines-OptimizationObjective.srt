1
00:00:00,570 --> 00:00:02,080
By now, you see the range

2
00:00:02,090 --> 00:00:05,270
of different learning algorithms. Within supervised learning,

3
00:00:05,280 --> 00:00:07,290
the performance of many supervised learning algorithms

4
00:00:07,300 --> 00:00:09,640
will be pretty similar

5
00:00:09,650 --> 00:00:11,030
and when that is less more often be

6
00:00:11,040 --> 00:00:12,430
whether you use

7
00:00:12,440 --> 00:00:13,650
learning algorithm A or learning algorithm

8
00:00:13,660 --> 00:00:15,180
B but when that

9
00:00:15,190 --> 00:00:16,350
is small there will often be

10
00:00:16,360 --> 00:00:17,320
things like the amount of data

11
00:00:17,330 --> 00:00:19,270
you are creating these algorithms on.

12
00:00:19,280 --> 00:00:20,590
That's always your skill in

13
00:00:20,600 --> 00:00:23,140
applying this algorithms.

Seems like

14
00:00:23,150 --> 00:00:24,650
your choice of the features that you

15
00:00:24,660 --> 00:00:26,000
designed to give the learning

16
00:00:26,010 --> 00:00:27,190
algorithms and how you

17
00:00:27,200 --> 00:00:29,180
choose the regularization parameter

18
00:00:29,190 --> 00:00:31,920
and things like that. But there's

19
00:00:31,930 --> 00:00:34,370
one more algorithm that is very

20
00:00:34,380 --> 00:00:35,570
powerful and its very

21
00:00:35,580 --> 00:00:38,040
widely used both within industry

22
00:00:38,050 --> 00:00:39,840
and in Academia. And that's called the

23
00:00:39,850 --> 00:00:41,190
support vector machine, and compared to

24
00:00:41,200 --> 00:00:44,100
both the logistic regression and neural networks, the

25
00:00:46,770 --> 00:00:48,430
support vector machine or the SVM

26
00:00:48,440 --> 00:00:50,880
sometimes gives a cleaner

27
00:00:50,890 --> 00:00:52,470
and sometimes more powerful way

28
00:00:52,480 --> 00:00:53,320
of learning complex nonlinear functions.

29
00:00:53,330 --> 00:00:54,960


30
00:00:54,970 --> 00:00:56,470
And so I'd like to take the next

31
00:00:56,480 --> 00:00:57,880
videos to

32
00:00:57,890 --> 00:01:00,390
talk about that.

33
00:01:00,400 --> 00:01:01,530
Later in this course, I will do

34
00:01:01,540 --> 00:01:03,090
a quick survey of the range

35
00:01:03,100 --> 00:01:05,190
of different supervised learning algorithms just

36
00:01:05,200 --> 00:01:07,420
to very briefly describe them

37
00:01:07,430 --> 00:01:09,360
but the support vector machine, given

38
00:01:09,370 --> 00:01:10,970
its popularity and how popular

39
00:01:10,980 --> 00:01:12,050
it is, this will be

40
00:01:12,060 --> 00:01:14,430
the last of the supervised learning algorithms

41
00:01:14,440 --> 00:01:18,210
that I'll spend a significant amount of time on in this course.

42
00:01:19,260 --> 00:01:20,660
As with our development of ever

43
00:01:20,670 --> 00:01:22,640
learning algorithms, we are going to start by talking

44
00:01:22,650 --> 00:01:24,740
about the optimization objective,

45
00:01:24,750 --> 00:01:26,610
so let's get started on

46
00:01:26,620 --> 00:01:29,410
this algorithm.

47
00:01:29,420 --> 00:01:31,260
In order to describe the support

48
00:01:31,270 --> 00:01:32,600
vector machine, I'm actually going

49
00:01:32,610 --> 00:01:34,980
to start with logistic regression

50
00:01:34,990 --> 00:01:36,810
and show how we can modify

51
00:01:36,820 --> 00:01:38,230
it a bit and get what

52
00:01:38,240 --> 00:01:40,280
is essentially the support vector machine.

53
00:01:40,290 --> 00:01:41,940
So, in logistic regression we have

54
00:01:41,950 --> 00:01:43,730
our familiar form of

55
00:01:43,740 --> 00:01:46,440
the hypotheses there and the

56
00:01:46,450 --> 00:01:50,090
sigmoid activation function shown on the right.

57
00:01:50,390 --> 00:01:51,790
And in order to explain

58
00:01:51,800 --> 00:01:52,840
some of the math, I'm going

59
00:01:52,850 --> 00:01:57,460
to use z to denote failure of transpose x here.

60
00:01:57,620 --> 00:01:58,890
Now let's think about what

61
00:01:58,900 --> 00:02:01,260
we will like the logistic regression to do.

62
00:02:01,270 --> 00:02:03,060
If we have an example with

63
00:02:03,070 --> 00:02:04,530
y equals 1, and by

64
00:02:04,540 --> 00:02:06,090
this I mean an example

65
00:02:06,100 --> 00:02:07,430
in either a training set

66
00:02:07,440 --> 00:02:12,020
or the test set, you know, order cross valuation set where y is equal to 1 then

67
00:02:12,030 --> 00:02:14,370
we are sort of hoping that h of x will be close to 1.

68
00:02:14,380 --> 00:02:16,130
So, right, we are hoping to

69
00:02:16,140 --> 00:02:18,510
correctly classify that example

70
00:02:18,520 --> 00:02:19,500
and what, having h of x

71
00:02:19,510 --> 00:02:20,840
close to 1, what that means

72
00:02:20,850 --> 00:02:22,350
is that theta transpose x

73
00:02:22,360 --> 00:02:23,760
must be much larger

74
00:02:23,770 --> 00:02:25,320
than 0, so there's

75
00:02:25,330 --> 00:02:26,890
greater than, greater than sign, that

76
00:02:26,900 --> 00:02:28,520
means much, much greater

77
00:02:28,530 --> 00:02:31,110
than 0 and that's

78
00:02:31,120 --> 00:02:32,950
because it is z, that is theta transpose

79
00:02:32,960 --> 00:02:34,930
x

80
00:02:34,940 --> 00:02:36,000
is when z is much bigger than

81
00:02:36,010 --> 00:02:37,300
0, is far to the

82
00:02:37,310 --> 00:02:39,350
right of this figure that, you know, the

83
00:02:39,360 --> 00:02:43,930
output of logistic regression becomes close to 1.

84
00:02:44,510 --> 00:02:45,620
Conversely, if we have

85
00:02:45,630 --> 00:02:46,990
an example where y is

86
00:02:47,000 --> 00:02:48,740
equal to 0 then what

87
00:02:48,750 --> 00:02:50,410
were hoping for is that the hypothesis

88
00:02:50,420 --> 00:02:52,000
will output the value to

89
00:02:52,010 --> 00:02:54,640
close to 0 and that corresponds to theta transpose x

90
00:02:54,650 --> 00:02:56,240
or z pretty much

91
00:02:56,250 --> 00:02:57,430
less than 0 because

92
00:02:57,440 --> 00:02:59,150
that corresponds to

93
00:02:59,160 --> 00:03:02,170
hypothesis of outputting a value close to 0. If

94
00:03:02,180 --> 00:03:03,750
you look at the

95
00:03:03,760 --> 00:03:06,430
cost function of logistic regression, what

96
00:03:06,440 --> 00:03:07,700
you find is that each

97
00:03:07,710 --> 00:03:10,180
example x, y,

98
00:03:10,190 --> 00:03:11,690
contributes a term like

99
00:03:11,700 --> 00:03:15,440
this to the overall cost function.

100
00:03:15,450 --> 00:03:17,380
All right. So, for the overall cost function, we usually, we will

101
00:03:17,390 --> 00:03:18,880
also have a sum over

102
00:03:18,890 --> 00:03:22,440
all the training examples and 1 over m term.

103
00:03:22,450 --> 00:03:23,230
But this

104
00:03:23,240 --> 00:03:24,460
expression here. That's

105
00:03:24,470 --> 00:03:26,210
the term that a single

106
00:03:26,220 --> 00:03:28,770
training example contributes to

107
00:03:28,780 --> 00:03:33,050
the overall objective function for logistic regression.

108
00:03:33,250 --> 00:03:35,180
Now, if I take the definition

109
00:03:35,190 --> 00:03:37,020
for the full of my hypothesis

110
00:03:37,030 --> 00:03:39,780
and plug it in, over here,

111
00:03:39,790 --> 00:03:40,910
the one I get is that

112
00:03:40,920 --> 00:03:44,260
each training example contributes this term, right?

113
00:03:44,270 --> 00:03:45,710
Ignoring the 1 over

114
00:03:45,720 --> 00:03:47,460
m but it contributes that term

115
00:03:47,470 --> 00:03:49,670
to be my overall cost function for

116
00:03:49,680 --> 00:03:52,810
logistic regression. Now let's

117
00:03:52,820 --> 00:03:54,690
consider the 2 cases

118
00:03:54,700 --> 00:03:56,030
of when y is equal to 1

119
00:03:56,040 --> 00:03:57,810
and when y is equal to 0.

120
00:03:57,820 --> 00:03:59,160
In the first case, let's

121
00:03:59,170 --> 00:04:00,510
suppose that y is equal

122
00:04:00,520 --> 00:04:02,430
to 1. In that case,

123
00:04:02,440 --> 00:04:04,970
only this first row in

124
00:04:04,980 --> 00:04:07,120
the objective matters because this

125
00:04:07,130 --> 00:04:09,200
1 minus y term will be equal

126
00:04:09,210 --> 00:04:12,010
to 0 if y is equal to 1.

127
00:04:13,640 --> 00:04:15,390
So, when y is equal to

128
00:04:15,400 --> 00:04:17,300
1 when in an example, x,

129
00:04:17,310 --> 00:04:18,410
y when y is equal to

130
00:04:18,420 --> 00:04:20,000
1, what we get is this

131
00:04:20,010 --> 00:04:21,550
term minus log 1

132
00:04:21,560 --> 00:04:22,850
over 1 plus e to the negative

133
00:04:22,860 --> 00:04:25,320
z. Where, similar to the last slide,

134
00:04:25,330 --> 00:04:27,480
I'm using z to denote

135
00:04:27,490 --> 00:04:29,630
data transpose x.  And

136
00:04:29,640 --> 00:04:31,030
of course, in the cost we

137
00:04:31,040 --> 00:04:32,370
actually had this minus y

138
00:04:32,380 --> 00:04:33,530
but we just said that you know, if y is

139
00:04:33,540 --> 00:04:35,010
equal to 1. So that's equal

140
00:04:35,020 --> 00:04:36,570
to 1. I just simplified it

141
00:04:36,580 --> 00:04:38,290
a way in the expression that

142
00:04:38,300 --> 00:04:41,320
I have written down here.

143
00:04:41,950 --> 00:04:43,570
And if we plot this function,

144
00:04:43,580 --> 00:04:45,220
as a function of z, what

145
00:04:45,230 --> 00:04:47,150
you find is that you get

146
00:04:47,160 --> 00:04:49,210
this curve shown on the

147
00:04:49,220 --> 00:04:51,110
lower left of this line

148
00:04:51,120 --> 00:04:52,630
and thus we also see

149
00:04:52,640 --> 00:04:53,850
that when z is equal

150
00:04:53,860 --> 00:04:55,430
to large that is to when

151
00:04:55,440 --> 00:04:57,790
theta transpose x is large

152
00:04:57,800 --> 00:04:58,880
that corresponds to a

153
00:04:58,890 --> 00:05:00,090
value of z that gives

154
00:05:00,100 --> 00:05:02,990
us a very small value, a very

155
00:05:03,000 --> 00:05:04,730
small contribution to the

156
00:05:04,740 --> 00:05:06,260
cost function and this

157
00:05:06,270 --> 00:05:08,250
kind of explains why when

158
00:05:08,260 --> 00:05:10,630
logistic regression sees a positive example

159
00:05:10,640 --> 00:05:12,850
with y equals 1 it tries

160
00:05:12,860 --> 00:05:14,640
to set theta transpose x

161
00:05:14,650 --> 00:05:15,970
to be very large because that

162
00:05:15,980 --> 00:05:18,290
corresponds to this term

163
00:05:18,300 --> 00:05:21,750
in a cost function being small. Now, to build

164
00:05:21,760 --> 00:05:23,730
the Support Vector Machine, here is what we are going to do.

165
00:05:23,740 --> 00:05:25,730
We are going to take this cost function, this

166
00:05:25,740 --> 00:05:30,920
minus log 1 over 1 plus e to the negative z and modify it a little bit.

167
00:05:31,270 --> 00:05:33,580
Let me take this point

168
00:05:33,590 --> 00:05:36,140
1 over here and let

169
00:05:36,150 --> 00:05:37,270
me draw the course function that I'm going to

170
00:05:37,280 --> 00:05:38,860
use, the new cost function is gonna

171
00:05:38,870 --> 00:05:41,820
be flat from here on out

172
00:05:42,000 --> 00:05:43,160
and then I'm going to draw something

173
00:05:43,170 --> 00:05:46,270
that grows as a straight

174
00:05:46,280 --> 00:05:49,520
line similar to logistic

175
00:05:49,530 --> 00:05:50,940
regression but this is going to be the

176
00:05:50,950 --> 00:05:52,860
straight line in this posh inning.

177
00:05:52,870 --> 00:05:55,180
So the curve that

178
00:05:55,190 --> 00:05:58,080
I just drew in magenta. The curve that I just drew purple and magenta.

179
00:05:58,090 --> 00:05:59,720
So, it's a pretty

180
00:05:59,730 --> 00:06:02,300
close approximation to the

181
00:06:02,310 --> 00:06:03,890
cost function used by logistic

182
00:06:03,900 --> 00:06:05,120
regression except that it is

183
00:06:05,130 --> 00:06:07,480
now made out of two line segments. This

184
00:06:07,490 --> 00:06:09,420
is flat potion on the right

185
00:06:09,430 --> 00:06:11,850
and then this is a straight

186
00:06:11,860 --> 00:06:14,620
line portion on the

187
00:06:14,630 --> 00:06:16,920
left. And don't worry too much about the slope of the straight line portion.

188
00:06:16,930 --> 00:06:19,170
It doesn't matter that

189
00:06:19,180 --> 00:06:21,720
much but that's the

190
00:06:21,730 --> 00:06:24,090
new cost function we're going to use where y is equal to 1 and

191
00:06:24,100 --> 00:06:25,330
you can imagine you

192
00:06:25,340 --> 00:06:29,180
should do something pretty similar to logistic regression

193
00:06:29,190 --> 00:06:30,740
but it turns out that this will give the

194
00:06:30,750 --> 00:06:33,680
support vector machine computational advantage

195
00:06:33,690 --> 00:06:34,880
that will give us later on

196
00:06:34,890 --> 00:06:37,560
an easier optimization problem, that

197
00:06:37,570 --> 00:06:41,040
will be easier for stock trades and so on.

198
00:06:41,050 --> 00:06:42,110
We just talked about the case

199
00:06:42,120 --> 00:06:43,360
of y equals to 1. The other

200
00:06:43,370 --> 00:06:44,650
case is if y is equal

201
00:06:44,660 --> 00:06:47,080
to 0. In that case,

202
00:06:47,090 --> 00:06:48,500
if you look at the cost

203
00:06:48,510 --> 00:06:50,210
then only this second term

204
00:06:50,220 --> 00:06:51,600
will apply because the first

205
00:06:51,610 --> 00:06:53,320
term goes a way

206
00:06:53,330 --> 00:06:54,630
where if y is equal to 0 then nearly

207
00:06:54,640 --> 00:06:55,790
it was 0 here. So

208
00:06:55,800 --> 00:06:57,030
your left only with the second

209
00:06:57,040 --> 00:06:59,140
term of the expression above

210
00:06:59,150 --> 00:07:00,700
and so the cost of an

211
00:07:00,710 --> 00:07:01,970
example or the contribution

212
00:07:01,980 --> 00:07:03,830
of the cost function is going

213
00:07:03,840 --> 00:07:05,170
to be given by this term

214
00:07:05,180 --> 00:07:06,700
over here and if you

215
00:07:06,710 --> 00:07:08,550
plug that as a function

216
00:07:08,560 --> 00:07:09,980
z. So, I have here z on the

217
00:07:09,990 --> 00:07:11,390
horizontal axis, you end up

218
00:07:11,400 --> 00:07:13,460
with this group and for

219
00:07:13,470 --> 00:07:14,780
the support vector machine, once

220
00:07:14,790 --> 00:07:16,240
again we're going to replace

221
00:07:16,250 --> 00:07:18,370
this blue line with something similar

222
00:07:18,380 --> 00:07:20,660
and see if we can

223
00:07:20,670 --> 00:07:23,470
replace it with a new cost, there

224
00:07:23,480 --> 00:07:25,010
is flat out here. There's 0 out here and then

225
00:07:25,020 --> 00:07:27,730
it grows as a straight

226
00:07:27,900 --> 00:07:29,060
line like so.

227
00:07:29,070 --> 00:07:29,850
So, let me give

228
00:07:29,860 --> 00:07:32,820
these two functions names.

229
00:07:32,830 --> 00:07:34,070
This function on the left, I'm

230
00:07:34,080 --> 00:07:37,130
going to call

231
00:07:37,140 --> 00:07:38,790
cost subscript 1 of z.

232
00:07:38,800 --> 00:07:39,860
And this function on the right, I'm going to call

233
00:07:39,870 --> 00:07:42,970
cost subscript 0

234
00:07:42,980 --> 00:07:44,850
of z. And the subscript just refers

235
00:07:44,860 --> 00:07:47,060
to the cost corresponding to

236
00:07:47,070 --> 00:07:49,920
y is equal to 1 versus y is equal to 0.

237
00:07:49,930 --> 00:07:51,570
Armed with these definitions, we are

238
00:07:51,580 --> 00:07:54,990
now ready to build the support vector machine.

239
00:07:55,000 --> 00:07:56,290
Here is the cost function

240
00:07:56,300 --> 00:07:57,330
j of theta that we have for

241
00:07:57,340 --> 00:07:58,760
logistic regression. In case

242
00:07:58,770 --> 00:07:59,850
this the equation looks a

243
00:07:59,860 --> 00:08:02,350
bit unfamiliar is because previously we

244
00:08:02,360 --> 00:08:04,790
had a minor sign outside, but

245
00:08:04,800 --> 00:08:05,920
here what I did was I

246
00:08:05,930 --> 00:08:07,600
instead moved the minor signs

247
00:08:07,610 --> 00:08:08,940
inside this expression. So it

248
00:08:08,950 --> 00:08:10,070
just, you know, makes it look a

249
00:08:10,080 --> 00:08:13,330
little more different. For the support

250
00:08:13,340 --> 00:08:14,720
vector machine what we are

251
00:08:14,730 --> 00:08:16,810
going to do is essentially take

252
00:08:16,820 --> 00:08:19,070
this, and replace this with

253
00:08:19,080 --> 00:08:21,730
cost 1 of z,

254
00:08:21,740 --> 00:08:23,310
that is cost 1 of theta transpose x.

255
00:08:23,320 --> 00:08:25,290
I'm going

256
00:08:25,300 --> 00:08:28,630
to take this and replace it with cost

257
00:08:28,640 --> 00:08:32,050
0 of z. This is cost 0 of

258
00:08:32,060 --> 00:08:35,020
theta transpose x

259
00:08:35,030 --> 00:08:36,990
where the cost 1 function is

260
00:08:37,000 --> 00:08:38,160
what we had on the previous

261
00:08:38,170 --> 00:08:40,880
line that looks like this and

262
00:08:40,890 --> 00:08:42,670
the cost 0 function, again what

263
00:08:42,680 --> 00:08:44,900
we have on the previous line that

264
00:08:44,910 --> 00:08:46,850
looks like this.

265
00:08:46,860 --> 00:08:48,410
So, what we have for the support

266
00:08:48,420 --> 00:08:49,900
vector machine is an minimizationminimalization

267
00:08:49,910 --> 00:08:52,330
problem of one of

268
00:08:52,340 --> 00:08:55,390
1 over m, sum over

269
00:08:55,400 --> 00:08:59,080
my training examples of y(i) times cost

270
00:08:59,090 --> 00:09:01,290
1 of theta transpose

271
00:09:01,300 --> 00:09:04,640
x(i) plus 1 minus

272
00:09:04,650 --> 00:09:07,210
y(i) times cost zero of theta transpose x(i).

273
00:09:07,220 --> 00:09:10,980
And then

274
00:09:10,990 --> 00:09:14,970
plus my usual regularization

275
00:09:17,120 --> 00:09:24,120
parameter like so. Now

276
00:09:24,130 --> 00:09:25,560
by convention for the Support

277
00:09:25,570 --> 00:09:27,780
Vector Machine, we actually write

278
00:09:27,790 --> 00:09:30,560
things slightly differently. We parametrize

279
00:09:30,570 --> 00:09:31,840
this just very slightly differently.

280
00:09:31,850 --> 00:09:34,120
First, we're going

281
00:09:34,130 --> 00:09:35,660
to get rid of the 1

282
00:09:35,670 --> 00:09:37,120
over m terms and this just,

283
00:09:37,130 --> 00:09:38,760
this just happens

284
00:09:38,770 --> 00:09:40,630
to be a slightly different convention that people

285
00:09:40,640 --> 00:09:42,130
use for support vector machines

286
00:09:42,140 --> 00:09:44,150
compared to for logistic regression. But here's what

287
00:09:44,160 --> 00:09:46,660
I mean, you know, what I'm going

288
00:09:46,670 --> 00:09:48,200
to do is I am just gonna get

289
00:09:48,210 --> 00:09:50,060
rid  of this 1 over m

290
00:09:50,070 --> 00:09:51,060
terms and this should give

291
00:09:51,070 --> 00:09:53,610
me the same optimal value for theta, right.

292
00:09:53,620 --> 00:09:56,410
Because 1 over m is just a constant.

293
00:09:56,420 --> 00:09:57,920
So, you know, whether I solve

294
00:09:57,930 --> 00:09:59,570
this minimization problem with 1

295
00:09:59,580 --> 00:10:01,090
over m in front or not,

296
00:10:01,100 --> 00:10:02,480
I should end up with the same

297
00:10:02,490 --> 00:10:04,580
optimal value of theta.

298
00:10:04,590 --> 00:10:05,580
Here is what I mean, to

299
00:10:05,590 --> 00:10:08,000
give you a concrete example,

300
00:10:08,010 --> 00:10:09,360
suppose I had a minimization

301
00:10:09,370 --> 00:10:11,450
problem that you know minimize over

302
00:10:11,460 --> 00:10:16,200
a real number u of u minus 5 squared,

303
00:10:17,080 --> 00:10:18,610
plus 1, right. Well, the

304
00:10:18,620 --> 00:10:20,430
minimum of this happens, happens

305
00:10:20,440 --> 00:10:23,080
to know the minimum of this is u equals 5.

306
00:10:23,090 --> 00:10:24,110
Now if I want to take

307
00:10:24,120 --> 00:10:26,420
this objective function and multiply

308
00:10:26,430 --> 00:10:28,760
it by 10, so

309
00:10:28,770 --> 00:10:30,560
here my minimization problem is

310
00:10:30,570 --> 00:10:33,950
minimum of u of 10, u minus

311
00:10:33,960 --> 00:10:35,910
5 squared plus 10.

312
00:10:35,920 --> 00:10:37,660
Well the value of u

313
00:10:37,670 --> 00:10:40,930
that minimizes this is still u equals 5, right.

314
00:10:40,940 --> 00:10:42,630
So, multiplying something that

315
00:10:42,640 --> 00:10:44,350
you are minimizing over by some

316
00:10:44,360 --> 00:10:46,000
constant, 10 in this case,

317
00:10:46,010 --> 00:10:48,280
it does not change the value

318
00:10:48,290 --> 00:10:52,640
of u that gives us, that minimizes this function.

319
00:10:52,650 --> 00:10:53,820
So the same way what I've

320
00:10:53,830 --> 00:10:55,420
done by crossing out this

321
00:10:55,430 --> 00:10:56,980
m is, all I

322
00:10:56,990 --> 00:10:59,230
am doing is multiplying my objective

323
00:10:59,240 --> 00:11:00,930
function by some constant m

324
00:11:00,940 --> 00:11:02,350
and it doesn't change the value

325
00:11:02,360 --> 00:11:05,470
of theta that achieves the minimum.

326
00:11:05,480 --> 00:11:07,460
The second bit of notational change,

327
00:11:07,470 --> 00:11:08,730
we're just designating the most

328
00:11:08,740 --> 00:11:11,160
standard convention, when using as

329
00:11:11,170 --> 00:11:14,200
the SVM, instead of logistic regression as a following.

330
00:11:14,210 --> 00:11:16,510
So, for logistic regression, we had

331
00:11:16,520 --> 00:11:19,330
two terms to our objective function.

332
00:11:19,340 --> 00:11:20,910
The first is this term

333
00:11:20,920 --> 00:11:22,440
which is the cost that comes

334
00:11:22,450 --> 00:11:23,980
from the training set and the

335
00:11:23,990 --> 00:11:26,130
second is this term, which

336
00:11:26,140 --> 00:11:28,370
is the regularization term

337
00:11:28,380 --> 00:11:29,860
and what we had, we had to

338
00:11:29,870 --> 00:11:31,260
control the trade off between

339
00:11:31,270 --> 00:11:32,800
these by saying, you know, that we

340
00:11:32,810 --> 00:11:35,750
wanted to minimize A plus

341
00:11:35,760 --> 00:11:39,360
and then my regularization parameter lambda,

342
00:11:39,370 --> 00:11:42,420
and then times some other

343
00:11:42,430 --> 00:11:43,500
term B, right? Where as I

344
00:11:43,510 --> 00:11:45,070
am using A to denote

345
00:11:45,080 --> 00:11:46,380
this first term, and I am

346
00:11:46,390 --> 00:11:48,480
using B to denote that

347
00:11:48,490 --> 00:11:49,640
second term, may be without the

348
00:11:49,650 --> 00:11:53,130
lambda, and instead of

349
00:11:53,140 --> 00:11:56,260
prioritizing this as A plus lambda B,

350
00:11:56,270 --> 00:11:58,190
we could, and so what we

351
00:11:58,200 --> 00:12:00,000
did was by setting different

352
00:12:00,010 --> 00:12:03,050
values for this regularization parameter lambda.

353
00:12:03,060 --> 00:12:04,660
We could trade off the relative

354
00:12:04,670 --> 00:12:05,890
way between how much we

355
00:12:05,900 --> 00:12:07,550
want to fit the training set well,

356
00:12:07,560 --> 00:12:09,500
as minimizing A, versus how

357
00:12:09,510 --> 00:12:13,460
much we care about keeping the values of the parameters small.

358
00:12:13,470 --> 00:12:14,630
So that would be

359
00:12:14,640 --> 00:12:16,370
for the parameters B. For the Support

360
00:12:16,380 --> 00:12:18,240
Vector Machine, just by convention

361
00:12:18,250 --> 00:12:19,560
we're going to use a different

362
00:12:19,570 --> 00:12:22,170
parameter. So instead of using lambda here

363
00:12:22,180 --> 00:12:23,630
to control the relative

364
00:12:23,640 --> 00:12:24,800
waiting between you know, the first and second terms,

365
00:12:24,810 --> 00:12:26,290
we are

366
00:12:26,300 --> 00:12:27,700
still going to use a different

367
00:12:27,710 --> 00:12:29,280
parameter which by convention

368
00:12:29,290 --> 00:12:31,720
is called C and

369
00:12:31,730 --> 00:12:34,420
we instead are going to minimize C times

370
00:12:34,430 --> 00:12:39,370
A plus B. So

371
00:12:39,380 --> 00:12:41,330
for logistic regression if we

372
00:12:41,340 --> 00:12:42,980
send a very large value of

373
00:12:42,990 --> 00:12:44,250
lambda, that means to give

374
00:12:44,260 --> 00:12:46,580
B a very high weight. Here

375
00:12:46,590 --> 00:12:47,950
is that if we set C

376
00:12:47,960 --> 00:12:50,060
to be a very small value. That

377
00:12:50,070 --> 00:12:51,790
corresponds to giving B

378
00:12:51,800 --> 00:12:54,600
much larger weight than C than A.

379
00:12:54,610 --> 00:12:55,880
So this is just a different

380
00:12:55,890 --> 00:12:57,620
way of controlling the trade off

381
00:12:57,630 --> 00:12:59,050
or just a different way of

382
00:12:59,060 --> 00:13:01,690
parametrizing how much we care about optimizing the first term versus how much we care about optimizing the second term.

383
00:13:01,700 --> 00:13:03,030


384
00:13:03,040 --> 00:13:05,280


385
00:13:05,290 --> 00:13:06,370
And if you want you can

386
00:13:06,380 --> 00:13:08,170
think of this as the parameter

387
00:13:08,180 --> 00:13:09,790
C playing a role

388
00:13:09,800 --> 00:13:11,880
similar to 1 over

389
00:13:11,890 --> 00:13:14,070
lambda and it's

390
00:13:14,080 --> 00:13:16,710
not that it's two equations

391
00:13:16,720 --> 00:13:17,990
or these two expressions will be

392
00:13:18,000 --> 00:13:19,640
equal, it's equals 1 over

393
00:13:19,650 --> 00:13:22,250
lambda and it's not that these two equations or these two expressions will be equal. It's equals t 1 over lambda. That's not the case where it bothers that if C is equal to 1 over lambda then

394
00:13:22,260 --> 00:13:24,700
these

395
00:13:24,710 --> 00:13:26,930
two optimization objectives should

396
00:13:26,940 --> 00:13:28,490
give you the same value, same

397
00:13:28,500 --> 00:13:30,340
optimal value of theta.

So

398
00:13:30,350 --> 00:13:31,390
just filling that

399
00:13:31,400 --> 00:13:33,720
in. I'm gonna cross out lambda here

400
00:13:33,730 --> 00:13:35,020
and write in the constant C there.

401
00:13:35,030 --> 00:13:38,160
So,that's gives

402
00:13:38,170 --> 00:13:41,270
us our overall optimization objective

403
00:13:41,280 --> 00:13:42,890
function for the Support Vector

404
00:13:42,900 --> 00:13:44,070
Machine and where you

405
00:13:44,080 --> 00:13:46,330
minimize that function then what

406
00:13:46,340 --> 00:13:48,220
you have is the parameters

407
00:13:48,230 --> 00:13:52,930
learned by SVM.

Finally on

408
00:13:52,940 --> 00:13:54,830
light of logistic regression, the Support

409
00:13:54,840 --> 00:13:56,210
Vector Machine doesn't output the

410
00:13:56,220 --> 00:13:57,960
probability. Instead what we

411
00:13:57,970 --> 00:13:59,180
have is, we have this cost

412
00:13:59,190 --> 00:14:00,720
function which we minimize to

413
00:14:00,730 --> 00:14:02,900
get the parameters theta and what

414
00:14:02,910 --> 00:14:05,120
the Support Vector Machine does,

415
00:14:05,130 --> 00:14:07,040
is it just makes the prediction

416
00:14:07,050 --> 00:14:08,680
of y being equal 1

417
00:14:08,690 --> 00:14:11,300
or 0 directly. So the hypothesis,

418
00:14:11,310 --> 00:14:14,140
where I predict, 1, if

419
00:14:14,150 --> 00:14:15,880
theta transpose x is

420
00:14:15,890 --> 00:14:18,220
greater than or equal to

421
00:14:18,230 --> 00:14:20,310
0 and I'll predict 0 otherwise.

422
00:14:20,320 --> 00:14:21,600
And so, having learned the

423
00:14:21,610 --> 00:14:23,350
parameters theta, this is

424
00:14:23,360 --> 00:14:26,840
the form of the hypothesis for the support vector machine.

425
00:14:26,850 --> 00:14:27,970
So, that was a

426
00:14:27,980 --> 00:14:29,830
mathematical definition of what

427
00:14:29,840 --> 00:14:31,740
a support vector machine does.

428
00:14:31,750 --> 00:14:33,090
In the next few videos, let's

429
00:14:33,100 --> 00:14:34,250
try to get back to

430
00:14:34,260 --> 00:14:36,470
intuition about what this

431
00:14:36,480 --> 00:14:37,810
optimization objective leads to and

432
00:14:37,820 --> 00:14:39,710
whether the source of the hypothesis

433
00:14:39,720 --> 00:14:41,690
a SVM will learn and also

434
00:14:41,700 --> 00:14:43,590
talk about how to modify

435
00:14:43,600 --> 00:14:44,910
this just a little bit to

436
00:14:44,920 --> 00:14:47,780
learn complex, nonlinear functions.

