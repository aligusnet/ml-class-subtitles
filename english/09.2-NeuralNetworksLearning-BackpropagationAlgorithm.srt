1
00:00:00,090 --> 00:00:01,570
In the previous video, we talked

2
00:00:01,580 --> 00:00:04,360
about a cost function for the neural network.

3
00:00:04,370 --> 00:00:05,990
In this video, let's start to

4
00:00:06,000 --> 00:00:09,230
talk about an algorithm, for trying to minimize the cost function.

5
00:00:09,240 --> 00:00:13,220
In particular, we'll talk about the back propagation algorithm.

6
00:00:14,080 --> 00:00:15,510
Here's the cost function that we

7
00:00:15,520 --> 00:00:17,500
wrote down in the previous video.

8
00:00:17,510 --> 00:00:19,120
What we'd like to do

9
00:00:19,130 --> 00:00:19,990
is try to find parameters theta

10
00:00:20,000 --> 00:00:23,520
to try to minimize j of theta.

11
00:00:23,530 --> 00:00:25,160
In order to use either gradient

12
00:00:25,170 --> 00:00:28,330
descent or one of the advance optimization algorithms.

13
00:00:28,340 --> 00:00:29,420
What we need to do

14
00:00:29,430 --> 00:00:31,280
therefore is to write code

15
00:00:31,290 --> 00:00:33,530
that takes this input the parameters theta

16
00:00:33,540 --> 00:00:35,200
and computes j of theta

17
00:00:35,210 --> 00:00:36,400
and these partial derivative terms.

18
00:00:36,410 --> 00:00:38,580
Remember, that the parameters

19
00:00:38,590 --> 00:00:40,150
in the the neural network of these

20
00:00:40,160 --> 00:00:42,820
things, theta superscript l subscript

21
00:00:42,830 --> 00:00:44,040
ij, that's the real

22
00:00:44,050 --> 00:00:45,870
number and so, these

23
00:00:45,880 --> 00:00:48,910
are the partial derivative terms we need to compute.

24
00:00:48,920 --> 00:00:50,320
In order to compute the cost

25
00:00:50,330 --> 00:00:51,950
function j of theta, we

26
00:00:51,960 --> 00:00:53,340
just use this formula up

27
00:00:53,350 --> 00:00:55,030
here and so, what

28
00:00:55,040 --> 00:00:56,010
I want to do for the most

29
00:00:56,020 --> 00:00:57,980
of this video is focus on

30
00:00:57,990 --> 00:00:59,210
talking about how we can

31
00:00:59,220 --> 00:01:02,190
compute these partial derivative terms.

32
00:01:02,200 --> 00:01:04,310
Let's start by talking about the case

33
00:01:04,320 --> 00:01:05,700
of when we have only one

34
00:01:05,710 --> 00:01:07,570
training example, so imagine

35
00:01:07,580 --> 00:01:09,350
if you will that our entire

36
00:01:09,360 --> 00:01:11,310
training set comprises only one

37
00:01:11,320 --> 00:01:12,870
training example which is

38
00:01:12,880 --> 00:01:14,450
a pair xy. I'm not going to

39
00:01:14,460 --> 00:01:16,400
write x1y1 just write this.

40
00:01:16,410 --> 00:01:17,800
Write a one training example as

41
00:01:17,810 --> 00:01:19,590
xy and let's tap

42
00:01:19,600 --> 00:01:21,360
through the sequence of calculations

43
00:01:21,370 --> 00:01:24,540
we would do with this one training example.

44
00:01:25,800 --> 00:01:27,080
The first thing we do is

45
00:01:27,090 --> 00:01:29,140
we apply four propagation in

46
00:01:29,150 --> 00:01:31,280
order to compute whether a

47
00:01:31,290 --> 00:01:34,400
hypotheses actually outputs given the input.

48
00:01:34,410 --> 00:01:36,590
Concretely, the called the

49
00:01:36,600 --> 00:01:38,830
a1 is the activation values

50
00:01:38,840 --> 00:01:41,590
of this first layer that was the input there.

51
00:01:41,600 --> 00:01:43,510
So, I'm going to set that to x

52
00:01:43,520 --> 00:01:45,610
and then we're going to compute

53
00:01:45,620 --> 00:01:48,250
z2 equals theta 1

54
00:01:48,260 --> 00:01:49,970
a1 and a2 equals g, the sigmoid

55
00:01:49,980 --> 00:01:52,300
activation function applied to z2

56
00:01:52,310 --> 00:01:53,590
and this would give us our

57
00:01:53,600 --> 00:01:56,090
activations for the first middle layer.

58
00:01:56,100 --> 00:01:57,200
That is for layer two of

59
00:01:57,210 --> 00:02:01,180
the network and we also add those bias terms.

60
00:02:01,330 --> 00:02:03,120
Next we apply 2 more steps

61
00:02:03,130 --> 00:02:05,080
of this four and propagation to

62
00:02:05,090 --> 00:02:08,550
compute a3 and a4

63
00:02:08,560 --> 00:02:10,880
which is also the

64
00:02:10,890 --> 00:02:12,670
upwards of a hypotheses h

65
00:02:12,680 --> 00:02:15,350
of x. So this

66
00:02:15,360 --> 00:02:18,020
is our vectorized implementation of

67
00:02:18,030 --> 00:02:19,790
forward propagation and it allows

68
00:02:19,800 --> 00:02:21,720
us to compute the activation

69
00:02:21,730 --> 00:02:23,500
values for all of the

70
00:02:23,510 --> 00:02:26,170
neurons in our neural network.

71
00:02:27,980 --> 00:02:29,440
Next, in order to compute

72
00:02:29,450 --> 00:02:30,570
the derivatives, we're going to

73
00:02:30,580 --> 00:02:33,920
use an algorithm called back propagation.

74
00:02:34,950 --> 00:02:36,520
The intuition of the back

75
00:02:36,530 --> 00:02:38,230
propagation algorithm is that

76
00:02:38,240 --> 00:02:39,670
for each note we're going

77
00:02:39,680 --> 00:02:42,220
to compute the term delta superscript

78
00:02:42,230 --> 00:02:44,330
l subscript j that's going

79
00:02:44,340 --> 00:02:46,320
to somehow represent the error

80
00:02:46,330 --> 00:02:47,450
of note j in the

81
00:02:47,460 --> 00:02:49,660
layer l. So, recall that

82
00:02:49,670 --> 00:02:52,560
a superscript l subscript j

83
00:02:52,570 --> 00:02:54,120
that does the activation of

84
00:02:54,130 --> 00:02:55,660
the j of unit in layer

85
00:02:55,670 --> 00:02:57,250
l and so, this

86
00:02:57,260 --> 00:02:58,580
delta term is in some

87
00:02:58,590 --> 00:03:00,940
sense going to capture our error

88
00:03:00,950 --> 00:03:03,640
in the activation of that neural duo.

89
00:03:03,650 --> 00:03:05,690
So, how we might wish the activation

90
00:03:05,700 --> 00:03:08,160
of that note is slightly different.

91
00:03:08,170 --> 00:03:10,260
Concretely, taking the example

92
00:03:10,270 --> 00:03:11,350
neural network that we have

93
00:03:11,360 --> 00:03:13,430
on the right which has four layers.

94
00:03:13,440 --> 00:03:16,050
And so capital L is equal to 4.

95
00:03:16,060 --> 00:03:17,390
For each output unit, we're going

96
00:03:17,400 --> 00:03:20,630
to compute this delta term. So, delta for the j of unit in the fourth layer is equal to

97
00:03:23,380 --> 00:03:24,710
just the activation of that

98
00:03:24,720 --> 00:03:26,480
unit minus what was

99
00:03:26,490 --> 00:03:29,890
the actual value of 0 in our training example.

100
00:03:29,900 --> 00:03:32,570
So, this term here can

101
00:03:32,580 --> 00:03:34,700
also be written h of

102
00:03:34,710 --> 00:03:38,320
x subscript j, right.

103
00:03:38,330 --> 00:03:39,920
So this delta term is just

104
00:03:39,930 --> 00:03:41,280
the difference between when a

105
00:03:41,290 --> 00:03:43,360
hypotheses output and what

106
00:03:43,370 --> 00:03:45,560
was the value of y

107
00:03:45,570 --> 00:03:47,050
in our training set whereas

108
00:03:47,060 --> 00:03:48,740
y subscript j is

109
00:03:48,750 --> 00:03:50,080
the j of element of the

110
00:03:50,090 --> 00:03:54,840
vector value y in our labeled training set.

111
00:03:56,200 --> 00:03:57,960
And by the way, if you

112
00:03:57,970 --> 00:04:00,990
think of delta a and

113
00:04:01,000 --> 00:04:02,510
y as vectors then you can

114
00:04:02,520 --> 00:04:04,020
also take those and come

115
00:04:04,030 --> 00:04:06,000
up with a vectorized implementation of

116
00:04:06,010 --> 00:04:07,680
it, which is just

117
00:04:07,690 --> 00:04:10,690
delta 4 gets set as

118
00:04:10,700 --> 00:04:14,550
a4 minus y. Where

119
00:04:14,560 --> 00:04:16,530
here, each of these delta

120
00:04:16,540 --> 00:04:18,170
4 a4 and y, each of

121
00:04:18,180 --> 00:04:20,630
these is a vector whose

122
00:04:20,640 --> 00:04:22,240
dimension is equal to

123
00:04:22,250 --> 00:04:25,200
the number of output units in our network.

124
00:04:25,210 --> 00:04:27,310
So we've now computed the

125
00:04:27,320 --> 00:04:29,010
era term's delta

126
00:04:29,020 --> 00:04:31,430
4 for our network.

127
00:04:31,440 --> 00:04:33,610
What we do next is compute

128
00:04:33,620 --> 00:04:37,200
the delta terms for the earlier layers in our network.

129
00:04:37,210 --> 00:04:39,000
Here's a formula for computing delta

130
00:04:39,010 --> 00:04:40,300
3 is delta 3 is equal

131
00:04:40,310 --> 00:04:42,550
to theta 3 transpose times delta 4.

132
00:04:42,560 --> 00:04:44,380
And this dot times, this

133
00:04:44,390 --> 00:04:47,570
is the element y's multiplication operation

134
00:04:47,580 --> 00:04:49,150
that we know from MATLAB.

135
00:04:49,160 --> 00:04:51,010
So delta 3 transpose delta

136
00:04:51,020 --> 00:04:53,470
4, that's a vector; g prime

137
00:04:53,480 --> 00:04:55,790
z3 that's also a vector

138
00:04:55,800 --> 00:04:57,520
and so dot times is

139
00:04:57,530 --> 00:05:01,170
in element y's multiplication between these two vectors.

140
00:05:01,460 --> 00:05:02,730
This term g prime of

141
00:05:02,740 --> 00:05:04,940
z3, that formally is actually

142
00:05:04,950 --> 00:05:06,710
the derivative of the activation

143
00:05:06,720 --> 00:05:08,880
function g evaluated at

144
00:05:08,890 --> 00:05:10,750
the input values given by z3.

145
00:05:10,760 --> 00:05:12,700
If you know calculus, you

146
00:05:12,710 --> 00:05:13,840
can try to work it out yourself

147
00:05:13,850 --> 00:05:16,850
and see that you can simplify it to the same answer that I get.

148
00:05:16,860 --> 00:05:19,990
But I'll just tell you pragmatically what that means.

149
00:05:20,000 --> 00:05:21,450
What you do to compute this g

150
00:05:21,460 --> 00:05:23,500
prime, these derivative terms is

151
00:05:23,510 --> 00:05:26,000
just a3 dot times1

152
00:05:26,010 --> 00:05:28,150
minus A3 where A3

153
00:05:28,160 --> 00:05:30,140
is the vector of activations.

154
00:05:30,150 --> 00:05:31,590
1 is the vector of

155
00:05:31,600 --> 00:05:34,010
ones and A3 is

156
00:05:34,020 --> 00:05:36,280
again the activation

157
00:05:36,290 --> 00:05:39,160
the vector of activation values for that layer.

158
00:05:39,170 --> 00:05:40,530
Next you apply a similar

159
00:05:40,540 --> 00:05:43,210
formula to compute delta 2

160
00:05:43,220 --> 00:05:45,660
where again that can be

161
00:05:45,670 --> 00:05:48,440
computed using a similar formula.

162
00:05:48,450 --> 00:05:50,110
Only now it is a2

163
00:05:50,120 --> 00:05:53,950
like so and I

164
00:05:53,960 --> 00:05:55,100
then prove it here but you

165
00:05:55,110 --> 00:05:56,480
can actually, it's possible to

166
00:05:56,490 --> 00:05:58,230
prove it if you know calculus

167
00:05:58,240 --> 00:05:59,850
that this expression is equal

168
00:05:59,860 --> 00:06:02,180
to mathematically, the derivative of

169
00:06:02,190 --> 00:06:04,030
the g function of the activation

170
00:06:04,040 --> 00:06:05,900
function, which I'm denoting

171
00:06:05,910 --> 00:06:09,260
by g prime. And finally,

172
00:06:09,270 --> 00:06:10,850
that's it and there is

173
00:06:10,860 --> 00:06:13,710
no delta1 term, because the

174
00:06:13,720 --> 00:06:15,620
first layer corresponds to the

175
00:06:15,630 --> 00:06:16,990
input layer and that's just the

176
00:06:17,000 --> 00:06:18,290
feature we observed in our

177
00:06:18,300 --> 00:06:20,590
training sets, so that doesn't have any error associated with that.

178
00:06:20,600 --> 00:06:22,110
It's not like, you know,

179
00:06:22,120 --> 00:06:24,310
we don't really want to try to change those values.

180
00:06:24,320 --> 00:06:25,500
And so we have delta

181
00:06:25,510 --> 00:06:29,590
terms only for layers 2, 3 and for this example.

182
00:06:30,170 --> 00:06:32,160
The name back propagation comes from

183
00:06:32,170 --> 00:06:33,340
the fact that we start by

184
00:06:33,350 --> 00:06:34,730
computing the delta term for

185
00:06:34,740 --> 00:06:36,360
the output layer and then

186
00:06:36,370 --> 00:06:37,870
we go back a layer and

187
00:06:37,880 --> 00:06:39,840
compute the delta terms for the

188
00:06:39,850 --> 00:06:41,170
third hidden layer and then we

189
00:06:41,180 --> 00:06:42,760
go back another step to compute

190
00:06:42,770 --> 00:06:44,650
delta 2 and so, we're sort of

191
00:06:44,660 --> 00:06:46,270
back propagating the errors from

192
00:06:46,280 --> 00:06:47,640
the output layer to layer 3

193
00:06:47,650 --> 00:06:51,260
to their to hence the name back complication.

194
00:06:51,270 --> 00:06:53,330
Finally, the derivation is

195
00:06:53,340 --> 00:06:56,810
surprisingly complicated, surprisingly involved but

196
00:06:56,820 --> 00:06:58,270
if you just do this few steps

197
00:06:58,280 --> 00:07:00,670
steps of computation it is possible

198
00:07:00,680 --> 00:07:02,800
to prove viral frankly some

199
00:07:02,810 --> 00:07:05,190
what complicated mathematical proof.

200
00:07:05,200 --> 00:07:07,550
It's possible to prove that if

201
00:07:07,560 --> 00:07:09,790
you ignore authorization then the

202
00:07:09,800 --> 00:07:12,210
partial derivative terms you want

203
00:07:12,220 --> 00:07:14,770
are exactly given by the

204
00:07:14,780 --> 00:07:17,860
activations and these delta terms.

205
00:07:17,870 --> 00:07:20,770
This is ignoring lambda or

206
00:07:20,780 --> 00:07:23,760
alternatively the regularization

207
00:07:23,770 --> 00:07:24,990
term lambda will

208
00:07:25,000 --> 00:07:25,670
equal to 0.

209
00:07:25,680 --> 00:07:27,460
We'll fix this detail later

210
00:07:27,470 --> 00:07:29,610
about the regularization term, but

211
00:07:29,620 --> 00:07:31,600
so by performing back propagation

212
00:07:31,610 --> 00:07:33,170
and computing these delta terms,

213
00:07:33,180 --> 00:07:34,520
you can, you know, pretty

214
00:07:34,530 --> 00:07:36,370
quickly compute these partial

215
00:07:36,380 --> 00:07:38,910
derivative terms for all of your parameters.

216
00:07:38,920 --> 00:07:40,560
So this is a lot of detail.

217
00:07:40,570 --> 00:07:42,310
Let's take everything and put

218
00:07:42,320 --> 00:07:44,110
it all together to talk about

219
00:07:44,120 --> 00:07:46,550
how to implement back propagation

220
00:07:46,560 --> 00:07:49,780
to compute derivatives with respect to your parameters.

221
00:07:49,790 --> 00:07:50,990
And for the case of when

222
00:07:51,000 --> 00:07:52,820
we have a large training

223
00:07:52,830 --> 00:07:54,090
set, not just a training

224
00:07:54,100 --> 00:07:57,280
set of one example, here's what we do.

225
00:07:57,290 --> 00:07:58,260
Suppose we have a training

226
00:07:58,270 --> 00:07:59,890
set of m examples like

227
00:07:59,900 --> 00:08:01,840
that shown here.

228
00:08:01,850 --> 00:08:03,210
The first thing we're going to do is

229
00:08:03,220 --> 00:08:05,090
we're going to set these delta

230
00:08:05,100 --> 00:08:08,080
l subscript i j. So this triangular symbol?

231
00:08:08,090 --> 00:08:10,300
That's actually the capital Greek

232
00:08:10,310 --> 00:08:12,040
alphabet delta .

233
00:08:12,050 --> 00:08:14,380
The symbol we had on the previous slide was the lower case delta.

234
00:08:14,390 --> 00:08:17,420
So the triangle is capital delta.

235
00:08:17,430 --> 00:08:18,670
We're gonna set this equal to zero

236
00:08:18,680 --> 00:08:22,100
for all values of l i j.

237
00:08:22,110 --> 00:08:24,520
Eventually, this capital delta

238
00:08:24,530 --> 00:08:26,850
l i j will be used

239
00:08:26,860 --> 00:08:30,280
to compute the partial

240
00:08:30,290 --> 00:08:32,370
derivative term, partial derivative

241
00:08:32,380 --> 00:08:35,420
respect to theta l i j of

242
00:08:35,430 --> 00:08:38,690
J of theta.

243
00:08:39,040 --> 00:08:40,470
So as we'll see in

244
00:08:40,480 --> 00:08:41,660
a second, these deltas are going

245
00:08:41,670 --> 00:08:43,940
to be used as accumulators that

246
00:08:43,950 --> 00:08:45,690
will slowly add things in

247
00:08:45,700 --> 00:08:48,630
order to compute these partial derivatives.

248
00:08:49,570 --> 00:08:52,140
Next, we're going to loop through our training set.

249
00:08:52,150 --> 00:08:53,600
So, we'll say for i equals

250
00:08:53,610 --> 00:08:55,610
1 through m and so

251
00:08:55,620 --> 00:08:57,400
for the i iteration, we're

252
00:08:57,410 --> 00:09:00,470
going to working with the training example xi, yi.

253
00:09:00,480 --> 00:09:03,710
So

254
00:09:03,720 --> 00:09:04,680
the first thing we're going to do

255
00:09:04,690 --> 00:09:06,560
is set a1 which is the

256
00:09:06,570 --> 00:09:08,180
activations of the input layer,

257
00:09:08,190 --> 00:09:09,940
set that to be equal to

258
00:09:09,950 --> 00:09:12,660
xi is the inputs for our

259
00:09:12,670 --> 00:09:15,330
i training example, and then

260
00:09:15,340 --> 00:09:17,720
we're going to perform forward propagation to

261
00:09:17,730 --> 00:09:19,780
compute the activations for

262
00:09:19,790 --> 00:09:21,160
layer two, layer three and so

263
00:09:21,170 --> 00:09:22,490
on up to the final

264
00:09:22,500 --> 00:09:25,560
layer, layer capital L. Next,

265
00:09:25,570 --> 00:09:27,270
we're going to use the output

266
00:09:27,280 --> 00:09:28,670
label yi from this

267
00:09:28,680 --> 00:09:30,330
specific example we're looking

268
00:09:30,340 --> 00:09:31,940
at to compute the error

269
00:09:31,950 --> 00:09:34,470
term for delta L for the output there.

270
00:09:34,480 --> 00:09:35,870
So delta L is what

271
00:09:35,880 --> 00:09:38,650
a hypotheses output minus what

272
00:09:38,660 --> 00:09:41,370
the target label was?

273
00:09:41,840 --> 00:09:42,840
And then we're going to use

274
00:09:42,850 --> 00:09:44,730
the back propagation algorithm to

275
00:09:44,740 --> 00:09:46,210
compute delta L minus 1,

276
00:09:46,220 --> 00:09:47,340
delta L minus 2, and

277
00:09:47,350 --> 00:09:50,260
so on down to delta 2 and once again

278
00:09:50,270 --> 00:09:51,450
there is now delta 1 because

279
00:09:51,460 --> 00:09:55,880
we don't associate an error term with the input layer.

280
00:09:57,000 --> 00:09:58,330
And finally, we're going to

281
00:09:58,340 --> 00:10:01,180
use these capital delta terms

282
00:10:01,190 --> 00:10:03,390
to accumulate these partial derivative

283
00:10:03,400 --> 00:10:06,860
terms that we wrote down on the previous line.

284
00:10:06,870 --> 00:10:07,950
And by the way, if you

285
00:10:07,960 --> 00:10:12,010
look at this expression, it's possible to vectorize this too.

286
00:10:12,020 --> 00:10:13,300
Concretely, if you think

287
00:10:13,310 --> 00:10:14,990
of delta ij as

288
00:10:15,000 --> 00:10:19,210
a matrix, indexed by subscript ij.

289
00:10:19,220 --> 00:10:20,770
Then, if delta L is

290
00:10:20,780 --> 00:10:22,120
a matrix we can rewrite

291
00:10:22,130 --> 00:10:24,340
this as delta L, gets

292
00:10:24,350 --> 00:10:27,820
updated as delta L plus

293
00:10:27,830 --> 00:10:29,630
lower case delta L plus

294
00:10:29,640 --> 00:10:33,560
one times aL transpose.

295
00:10:33,570 --> 00:10:35,510
So that's a vectorized implementation of

296
00:10:35,520 --> 00:10:37,580
this that automatically does

297
00:10:37,590 --> 00:10:39,000
an update for all values of

298
00:10:39,010 --> 00:10:41,490
i and j.

Finally, after

299
00:10:41,500 --> 00:10:43,570
executing the body of

300
00:10:43,580 --> 00:10:46,320
the four-loop we then go outside the four-loop

301
00:10:46,330 --> 00:10:47,430
and we compute the following.

302
00:10:47,440 --> 00:10:50,010
We compute capital D as

303
00:10:50,020 --> 00:10:51,500
follows and we have

304
00:10:51,510 --> 00:10:52,970
two separate cases for j

305
00:10:52,980 --> 00:10:56,070
equals zero and j not equals zero.

306
00:10:56,080 --> 00:10:57,670
The case of j equals zero

307
00:10:57,680 --> 00:10:59,140
corresponds to the bias

308
00:10:59,150 --> 00:11:00,380
term so when j equals

309
00:11:00,390 --> 00:11:01,790
zero that's why we're missing

310
00:11:01,800 --> 00:11:04,820
is an extra regularization term.

311
00:11:05,470 --> 00:11:07,170
Finally, while the formal proof

312
00:11:07,180 --> 00:11:09,020
is pretty complicated what you

313
00:11:09,030 --> 00:11:10,630
can show is that once

314
00:11:10,640 --> 00:11:13,500
you've computed these D terms,

315
00:11:13,510 --> 00:11:15,630
that is exactly the partial

316
00:11:15,640 --> 00:11:17,910
derivative of the cost

317
00:11:17,920 --> 00:11:19,460
function with respect to each

318
00:11:19,470 --> 00:11:21,030
of your perimeters and so you

319
00:11:21,040 --> 00:11:22,600
can use those in either gradient

320
00:11:22,610 --> 00:11:25,030
descent or in one of the advanced authorization

321
00:11:25,450 --> 00:11:26,950
algorithms.

322
00:11:28,310 --> 00:11:29,980
So that's the back propagation

323
00:11:29,990 --> 00:11:31,460
algorithm and how you compute

324
00:11:31,470 --> 00:11:33,330
derivatives of your cost

325
00:11:33,340 --> 00:11:35,460
function for a neural network.

326
00:11:35,470 --> 00:11:36,460
I know this looks like this

327
00:11:36,470 --> 00:11:39,450
was a lot of details and this was a lot of steps strung together.

328
00:11:39,460 --> 00:11:41,090
But both in the programming

329
00:11:41,100 --> 00:11:43,100
assignments write out and later

330
00:11:43,110 --> 00:11:44,710
in this video, we'll give

331
00:11:44,720 --> 00:11:46,040
you a summary of this so

332
00:11:46,050 --> 00:11:47,250
we can have all the pieces

333
00:11:47,260 --> 00:11:48,910
of the algorithm together so that

334
00:11:48,920 --> 00:11:50,600
you know exactly what you need

335
00:11:50,610 --> 00:11:51,930
to implement if you want

336
00:11:51,940 --> 00:11:53,880
to implement back propagation to compute

337
00:11:53,890 --> 00:11:56,410
the derivatives of your neural network's

338
00:11:56,420 --> 00:11:59,710
cost function with respect to those parameters.

