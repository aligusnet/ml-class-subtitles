1
00:00:00,530 --> 00:00:01,940
In the last video, we started

2
00:00:01,950 --> 00:00:03,700
to talk about the kernels idea

3
00:00:03,710 --> 00:00:04,850
and how it can be used to

4
00:00:04,860 --> 00:00:08,090
define new features for the support vector machine.

5
00:00:08,100 --> 00:00:09,220
In this video, I'd like to throw

6
00:00:09,230 --> 00:00:11,010
in some of the missing details and,

7
00:00:11,020 --> 00:00:12,260
also, say a few words about

8
00:00:12,270 --> 00:00:14,640
how to use these ideas in practice.

9
00:00:14,650 --> 00:00:16,330
Such as, how they pertain

10
00:00:16,340 --> 00:00:21,620
to, for example, the bias variance trade-off in support vector machines.

11
00:00:22,690 --> 00:00:23,990
In the last video, I talked

12
00:00:24,000 --> 00:00:26,650
about the process of picking a few landmarks.

13
00:00:26,660 --> 00:00:29,140
You know, l1, l2, l3 and that

14
00:00:29,150 --> 00:00:30,290
allowed us to define the

15
00:00:30,300 --> 00:00:32,190
similarity function also called

16
00:00:32,200 --> 00:00:33,680
the kernel or in this

17
00:00:33,690 --> 00:00:35,060
example if you have

18
00:00:35,070 --> 00:00:38,600
this similarity function this is a Gaussian kernel.

19
00:00:38,610 --> 00:00:40,650
And that allowed us to build

20
00:00:40,660 --> 00:00:43,170
this form of a hypothesis function.

21
00:00:43,180 --> 00:00:45,140
But where do we get these landmarks from?

22
00:00:45,150 --> 00:00:45,680
Where do we get l1, l2, l3 from?

23
00:00:45,690 --> 00:00:49,600
And it seems, also, that for complex learning

24
00:00:49,610 --> 00:00:50,910
problems, maybe we want a

25
00:00:50,920 --> 00:00:54,560
lot more landmarks than just three of them that we might choose by hand.

26
00:00:55,160 --> 00:00:56,570
So in practice this is

27
00:00:56,580 --> 00:00:57,820
how the landmarks are chosen

28
00:00:57,830 --> 00:01:00,140
which is that given the

29
00:01:00,150 --> 00:01:01,360
machine learning problem. We have some

30
00:01:01,370 --> 00:01:02,700
data set of some some positive

31
00:01:02,710 --> 00:01:05,300
and negative examples. So, this is the idea here

32
00:01:05,310 --> 00:01:06,620
which is that we're gonna take the

33
00:01:06,630 --> 00:01:08,460
examples and for every

34
00:01:08,470 --> 00:01:10,480
training example that we have,

35
00:01:10,490 --> 00:01:11,970
we are just going to call

36
00:01:11,980 --> 00:01:13,430
it. We're just going

37
00:01:13,440 --> 00:01:15,480
to put landmarks as exactly

38
00:01:15,490 --> 00:01:18,920
the same locations as the training examples.

39
00:01:18,930 --> 00:01:20,670
So if I have one training

40
00:01:20,680 --> 00:01:22,110
example if that is x1,

41
00:01:22,120 --> 00:01:23,660
well then I'm going

42
00:01:23,670 --> 00:01:25,090
to choose this is my first landmark

43
00:01:25,100 --> 00:01:27,240
to be at xactly the same location

44
00:01:27,250 --> 00:01:29,250
as my first training example.

45
00:01:29,260 --> 00:01:30,460
And if I have a different training

46
00:01:30,470 --> 00:01:32,490
example x2. Well we're

47
00:01:32,500 --> 00:01:35,050
going to set the second landmark

48
00:01:35,060 --> 00:01:38,470
to be the location of my second training example.

49
00:01:38,480 --> 00:01:39,470
On the figure on the right, I

50
00:01:39,480 --> 00:01:40,810
used red and blue dots

51
00:01:40,820 --> 00:01:42,410
just as illustration, the color

52
00:01:42,420 --> 00:01:44,360
of this figure, the color of

53
00:01:44,370 --> 00:01:47,110
the dots on the figure on the right is not significant.

54
00:01:47,120 --> 00:01:48,100
But what I'm going to end up

55
00:01:48,110 --> 00:01:49,780
with using this method is I'm

56
00:01:49,790 --> 00:01:52,150
going to end up with m

57
00:01:52,160 --> 00:01:54,940
landmarks of l1, l2

58
00:01:54,950 --> 00:01:56,370
down to l(m) if I

59
00:01:56,380 --> 00:01:58,410
have m training examples with

60
00:01:58,420 --> 00:02:00,800
one landmark per location of

61
00:02:00,810 --> 00:02:02,850
my per location of each

62
00:02:02,860 --> 00:02:04,940
of my training examples. And this is

63
00:02:04,950 --> 00:02:06,110
nice because it is saying that

64
00:02:06,120 --> 00:02:07,690
my features are basically going

65
00:02:07,700 --> 00:02:09,370
to measure how close an

66
00:02:09,380 --> 00:02:10,960
example is to one

67
00:02:10,970 --> 00:02:13,430
of the things I saw in my training set.

68
00:02:13,440 --> 00:02:14,340
So, just to write this outline a

69
00:02:14,350 --> 00:02:16,460
little more concretely, given m

70
00:02:16,470 --> 00:02:18,040
training examples, I'm going

71
00:02:18,050 --> 00:02:19,300
to choose the the location

72
00:02:19,310 --> 00:02:21,180
of my landmarks to be exactly

73
00:02:21,190 --> 00:02:25,420
near the locations of my m training examples.

74
00:02:25,430 --> 00:02:26,910
When you are given example x,

75
00:02:26,920 --> 00:02:28,220
and in this example x can be

76
00:02:28,230 --> 00:02:29,560
something in the training set,

77
00:02:29,570 --> 00:02:31,480
it can be something in the cross validation

78
00:02:31,490 --> 00:02:33,310
set, or it can be something in the test set.

79
00:02:33,320 --> 00:02:34,310
Given an example x we are

80
00:02:34,320 --> 00:02:35,740
going to compute, you know,

81
00:02:35,750 --> 00:02:37,550
these features as so f1,

82
00:02:37,560 --> 00:02:39,570
f2, and so on.

83
00:02:39,580 --> 00:02:41,480
Where l1 is actually equal

84
00:02:41,490 --> 00:02:43,560
to x1 and so on.

85
00:02:43,570 --> 00:02:46,830
And these then give me a feature vector.

86
00:02:46,840 --> 00:02:50,260
So let me write f as the feature vector.

87
00:02:50,270 --> 00:02:52,280
I'm going to take these f1, f2 and

88
00:02:52,290 --> 00:02:53,570
so on, and just group

89
00:02:53,580 --> 00:02:56,320
them into feature vector.

90
00:02:56,330 --> 00:02:59,310
Take those down to fm.

91
00:02:59,320 --> 00:03:01,600
And, you know, just by convention.

92
00:03:01,610 --> 00:03:02,980
If we want, we can add an

93
00:03:02,990 --> 00:03:06,440
extra feature f0, which is always equal to 1.

94
00:03:06,450 --> 00:03:09,470
So this plays a role similar to what we had previously.

95
00:03:09,480 --> 00:03:12,700
For x0, which was our interceptor.

96
00:03:13,200 --> 00:03:14,570
So, for example, if we

97
00:03:14,580 --> 00:03:18,050
have a training example x(i), y(i),

98
00:03:18,270 --> 00:03:20,070
the features we would compute for

99
00:03:20,080 --> 00:03:21,430
this training example will be

100
00:03:21,440 --> 00:03:23,630
as follows: given x(i), we

101
00:03:23,640 --> 00:03:27,970
will then map it to, you know, f1(i).

102
00:03:27,980 --> 00:03:29,950
Which is the similarity. I'm going to

103
00:03:29,960 --> 00:03:32,080
abbreviate as SIM instead of writing out the whole

104
00:03:32,090 --> 00:03:34,880
word

105
00:03:35,540 --> 00:03:37,040
similarity, right?

106
00:03:37,050 --> 00:03:40,080
And f2(i) equals the similarity

107
00:03:40,090 --> 00:03:43,130
between x(i) and l2,

108
00:03:43,140 --> 00:03:45,220
and so on,

109
00:03:45,230 --> 00:03:49,590
down to fm(i) equals

110
00:03:49,600 --> 00:03:55,690
the similarity between x(i) and l(m).

111
00:03:55,700 --> 00:03:59,150
And somewhere in the middle.

112
00:03:59,160 --> 00:04:01,470
Somewhere in this list, you know, at

113
00:04:01,480 --> 00:04:04,220
the i-th component, I will

114
00:04:04,230 --> 00:04:06,140
actually have one feature

115
00:04:06,150 --> 00:04:08,160
component which is f subscript

116
00:04:08,170 --> 00:04:10,040
i(i), which is

117
00:04:10,050 --> 00:04:12,680
going to be the similarity

118
00:04:13,080 --> 00:04:15,670
between x and l(i).

119
00:04:15,680 --> 00:04:17,180
Where l(i) is equal to

120
00:04:17,190 --> 00:04:19,130
x(i), and so you know

121
00:04:19,140 --> 00:04:20,400
fi(i) is just going to

122
00:04:20,410 --> 00:04:23,750
be the similarity between x and itself.

123
00:04:23,960 --> 00:04:25,610
And if you're using the Gaussian kernel this is

124
00:04:25,620 --> 00:04:27,160
actually e to the minus 0

125
00:04:27,170 --> 00:04:29,780
over 2 sigma squared and so, this will be equal to 1 and that's okay.

126
00:04:29,790 --> 00:04:31,360
So one of my features for this

127
00:04:31,370 --> 00:04:34,280
training example is going to be equal to 1.

128
00:04:34,290 --> 00:04:35,980
And then similar to what I have above.

129
00:04:35,990 --> 00:04:37,860
I can take all of these

130
00:04:37,870 --> 00:04:40,330
m features and group them into a feature vector.

131
00:04:40,340 --> 00:04:42,700
So instead of representing my example,

132
00:04:42,710 --> 00:04:44,420
using, you know, x(i) which is this what

133
00:04:44,430 --> 00:04:48,280
R(n) plus R(n) one dimensional vector.

134
00:04:48,290 --> 00:04:49,980
Depending on whether you can

135
00:04:49,990 --> 00:04:52,060
set terms, is either R(n)

136
00:04:52,070 --> 00:04:53,430
or R(n) plus 1.

137
00:04:53,440 --> 00:04:55,290
We can now instead represent my

138
00:04:55,300 --> 00:04:56,970
training example using this feature

139
00:04:56,980 --> 00:04:58,910
vector f. I am

140
00:04:58,920 --> 00:05:01,390
going to write this f superscript i.  Which

141
00:05:01,400 --> 00:05:03,290
is going to be taking all

142
00:05:03,300 --> 00:05:06,530
of these things and stacking them into a vector.

143
00:05:06,540 --> 00:05:09,420
So, f1(i) down

144
00:05:09,430 --> 00:05:13,020
to fm(i) and if you want and

145
00:05:13,030 --> 00:05:15,410
well, usually we'll also add this

146
00:05:15,420 --> 00:05:17,120
f0(i), where

147
00:05:17,130 --> 00:05:19,360
f0(i) is equal to 1.

148
00:05:19,370 --> 00:05:21,290
And so this vector

149
00:05:21,300 --> 00:05:23,420
here gives me my

150
00:05:23,430 --> 00:05:25,470
new feature vector with which

151
00:05:25,480 --> 00:05:29,030
to represent my training example.

152
00:05:29,040 --> 00:05:31,520
So given these kernels

153
00:05:31,530 --> 00:05:33,390
and similarity functions, here's how

154
00:05:33,400 --> 00:05:35,710
we use a simple vector machine.

155
00:05:35,720 --> 00:05:37,290
If you already have a learning

156
00:05:37,300 --> 00:05:39,840
set of parameters theta, then if you given a value of x and you want to make a prediction.

157
00:05:39,850 --> 00:05:41,670


158
00:05:41,680 --> 00:05:43,050
What we do is we compute the

159
00:05:43,060 --> 00:05:44,440
features f, which is now

160
00:05:44,450 --> 00:05:48,420
an R(m) plus 1 dimensional feature vector.

161
00:05:49,040 --> 00:05:51,600
And we have m here because we have

162
00:05:51,610 --> 00:05:53,560
m training examples and thus

163
00:05:53,570 --> 00:05:57,320
m landmarks and what

164
00:05:57,330 --> 00:05:58,590
we do is we predict

165
00:05:58,600 --> 00:06:00,770
1 if theta transpose f

166
00:06:00,780 --> 00:06:02,220
is greater than or equal to 0.

167
00:06:02,230 --> 00:06:02,630
Right.

168
00:06:02,640 --> 00:06:04,080
So, if theta transpose f, of course,

169
00:06:04,090 --> 00:06:07,890
that's just equal to theta 0, f0 plus theta 1,

170
00:06:07,900 --> 00:06:09,110
f1 plus dot dot

171
00:06:09,120 --> 00:06:12,160
dot, plus theta m

172
00:06:12,170 --> 00:06:14,040
f(m). And so my

173
00:06:14,050 --> 00:06:16,160
parameter vector theta is also now

174
00:06:16,170 --> 00:06:17,980
going to be an m

175
00:06:17,990 --> 00:06:21,770
plus 1 dimensional vector.

176
00:06:21,780 --> 00:06:23,250
And we have m here because where

177
00:06:23,260 --> 00:06:25,440
the number of landmarks is equal

178
00:06:25,450 --> 00:06:26,900
to the training set size.

179
00:06:26,910 --> 00:06:29,090
So m was the training set size and now, the

180
00:06:29,100 --> 00:06:32,980
parameter vector theta is going to be m plus one dimensional.

181
00:06:32,990 --> 00:06:34,350
So that's how you make a prediction

182
00:06:34,360 --> 00:06:37,830
if you already have a setting for the parameter's theta.

183
00:06:37,840 --> 00:06:39,670
How do you get the parameter's theta?

184
00:06:39,680 --> 00:06:40,910
Well you do that using the

185
00:06:40,920 --> 00:06:43,840
SVM learning algorithm, and specifically

186
00:06:43,850 --> 00:06:46,680
what you do is you would solve this minimization problem.

187
00:06:46,690 --> 00:06:48,530
You've minimized the parameter's

188
00:06:48,540 --> 00:06:52,420
theta of C times this cost function which we had before.

189
00:06:52,430 --> 00:06:55,030
Only now, instead of looking

190
00:06:55,040 --> 00:06:56,960
there instead of making

191
00:06:56,970 --> 00:07:00,010
predictions using theta transpose

192
00:07:00,020 --> 00:07:01,710
x(i) using our original

193
00:07:01,720 --> 00:07:03,510
features, x(i). Instead we've

194
00:07:03,520 --> 00:07:05,080
taken the features x(i)

195
00:07:05,090 --> 00:07:07,260
and replace them with a new features

196
00:07:07,270 --> 00:07:09,370
so we are using theta transpose

197
00:07:09,380 --> 00:07:11,120
f(i) to make a

198
00:07:11,130 --> 00:07:12,850
prediction on the i'f training

199
00:07:12,860 --> 00:07:14,220
examples and we see that, you know,

200
00:07:14,230 --> 00:07:16,690
in both places here and

201
00:07:16,700 --> 00:07:18,750
it's by solving this minimization problem

202
00:07:18,760 --> 00:07:23,230
that you get the parameters for your Support Vector Machine.

203
00:07:23,240 --> 00:07:24,860
And one last detail is

204
00:07:24,870 --> 00:07:27,500
because this optimization

205
00:07:27,510 --> 00:07:30,560
problem we really have

206
00:07:30,570 --> 00:07:32,850
n equals m features.

207
00:07:32,860 --> 00:07:34,510
That is here.

208
00:07:34,520 --> 00:07:37,090
The number of features we have.

209
00:07:37,100 --> 00:07:38,400
Really, the effective number of

210
00:07:38,410 --> 00:07:39,660
features we have is dimension

211
00:07:39,670 --> 00:07:41,720
of f. So that n

212
00:07:41,730 --> 00:07:42,890
is actually going to be equal

213
00:07:42,900 --> 00:07:44,600
to m. So, if you want to, you can

214
00:07:44,610 --> 00:07:46,330
think of this as a sum,

215
00:07:46,340 --> 00:07:47,580
this really is a sum

216
00:07:47,590 --> 00:07:49,480
from j equals 1 through

217
00:07:49,490 --> 00:07:50,460
m. And then one way to think

218
00:07:50,470 --> 00:07:51,610
about this, is you can

219
00:07:51,620 --> 00:07:53,540
think of it as n being

220
00:07:53,550 --> 00:07:55,560
equal to m, because if

221
00:07:55,570 --> 00:07:57,960
f isn't a new feature, then

222
00:07:57,970 --> 00:08:00,110
we have m plus 1

223
00:08:00,120 --> 00:08:04,420
features, with the plus 1 coming from the interceptor.

224
00:08:05,090 --> 00:08:06,980
And here, we still do sum

225
00:08:06,990 --> 00:08:08,430
from j equal 1 through n,

226
00:08:08,440 --> 00:08:10,370
because similar to our

227
00:08:10,380 --> 00:08:12,570
earlier videos on regularization,

228
00:08:12,580 --> 00:08:14,170
we still do not regularize the

229
00:08:14,180 --> 00:08:15,770
parameter theta zero, which is

230
00:08:15,780 --> 00:08:16,730
why this is a sum for

231
00:08:16,740 --> 00:08:18,870
j equals 1 through m

232
00:08:18,880 --> 00:08:19,990
instead of j equals zero though

233
00:08:20,000 --> 00:08:22,570
m.  So that's

234
00:08:22,580 --> 00:08:24,650
the support vector machine learning algorithm.

235
00:08:24,660 --> 00:08:27,150
That's one sort of, mathematical

236
00:08:27,160 --> 00:08:28,430
detail aside that I

237
00:08:28,440 --> 00:08:29,920
should mention, which is

238
00:08:29,930 --> 00:08:31,300
that in the way the support

239
00:08:31,310 --> 00:08:33,310
vector machine is implemented, this last

240
00:08:33,320 --> 00:08:35,670
term is actually done a little bit differently.

241
00:08:35,680 --> 00:08:36,760
So you don't really need to

242
00:08:36,770 --> 00:08:38,180
know about this last detail in

243
00:08:38,190 --> 00:08:39,690
order to use support vector machines,

244
00:08:39,700 --> 00:08:41,440
and in fact the equations that

245
00:08:41,450 --> 00:08:42,610
are written down here should give

246
00:08:42,620 --> 00:08:45,300
you all the intuitions that should need.

247
00:08:45,310 --> 00:08:46,440
But in the way the support vector machine

248
00:08:46,450 --> 00:08:48,560
is implemented, you know, that term, the

249
00:08:48,570 --> 00:08:52,460
sum of j of theta j squared right?

250
00:08:53,110 --> 00:08:55,570
Another way to write this is this can

251
00:08:55,580 --> 00:08:58,490
be written as theta transpose

252
00:08:58,500 --> 00:09:00,110
theta if we ignore

253
00:09:00,120 --> 00:09:03,560
the parameter theta 0.

254
00:09:03,570 --> 00:09:05,790
So theta 1 down to

255
00:09:05,800 --> 00:09:11,120
theta m.  Ignoring theta 0.

256
00:09:11,130 --> 00:09:14,500
Then this sum of

257
00:09:14,510 --> 00:09:16,030
j of theta j squared that this

258
00:09:16,040 --> 00:09:19,920
can also be written theta transpose theta.

259
00:09:19,930 --> 00:09:21,720
And what most support vector

260
00:09:21,730 --> 00:09:23,710
machine implementations do is actually

261
00:09:23,720 --> 00:09:26,270
replace this theta transpose theta,

262
00:09:26,280 --> 00:09:28,580
will instead, theta transpose times

263
00:09:28,590 --> 00:09:30,810
some matrix inside, that depends

264
00:09:30,820 --> 00:09:34,150
on the kernel you use, times theta.

265
00:09:34,160 --> 00:09:36,130
And so this gives us a slightly different distance metric.

266
00:09:36,140 --> 00:09:38,060
We'll use a slightly different

267
00:09:38,070 --> 00:09:41,310
measure instead of minimizing exactly

268
00:09:41,320 --> 00:09:43,780
the norm of theta squared means

269
00:09:43,790 --> 00:09:46,130
that minimize something slightly similar to it.

270
00:09:46,140 --> 00:09:47,760
That's like a rescale version of

271
00:09:47,770 --> 00:09:50,940
the parameter vector theta that depends on the kernel.

272
00:09:50,950 --> 00:09:53,200
But this is kind of a mathematical detail.

273
00:09:53,210 --> 00:09:54,640
That allows the support vector

274
00:09:54,650 --> 00:09:57,850
machine software to run much more efficiently.

275
00:09:58,300 --> 00:09:59,690
And the reason the support vector machine

276
00:09:59,700 --> 00:10:02,010
does this is with this modification.

277
00:10:02,020 --> 00:10:03,290
It allows it to

278
00:10:03,300 --> 00:10:06,360
scale to much bigger training sets.

279
00:10:06,370 --> 00:10:07,960
Because for example, if you have

280
00:10:07,970 --> 00:10:12,580
a training set with 10,000 training examples.

281
00:10:12,590 --> 00:10:13,940
Then, you know, the way we define

282
00:10:13,950 --> 00:10:16,770
landmarks, we end up with 10,000 landmarks.

283
00:10:16,780 --> 00:10:18,480
And so theta becomes 10,000 dimensional.

284
00:10:18,490 --> 00:10:20,440
And maybe that works, but when m

285
00:10:20,450 --> 00:10:22,460
becomes really, really big

286
00:10:22,470 --> 00:10:24,140
then solving for all

287
00:10:24,150 --> 00:10:25,580
of these parameters, you know, if m were

288
00:10:25,590 --> 00:10:26,870
50,000 or a 100,000

289
00:10:26,880 --> 00:10:28,330
then solving for

290
00:10:28,340 --> 00:10:29,880
all of these parameters can become

291
00:10:29,890 --> 00:10:31,410
expensive for the support

292
00:10:31,420 --> 00:10:33,860
vector machine optimization software, thus

293
00:10:33,870 --> 00:10:36,480
solving the minimization problem that I drew here.

294
00:10:36,490 --> 00:10:37,850
So kind of as mathematical

295
00:10:37,860 --> 00:10:40,990
detail, which again you really don't need to know about.

296
00:10:41,000 --> 00:10:43,340
It actually modifies that last

297
00:10:43,350 --> 00:10:44,490
term a little bit to

298
00:10:44,500 --> 00:10:46,070
optimize something slightly different than

299
00:10:46,080 --> 00:10:49,360
just minimizing the norm squared of theta squared, of theta.

300
00:10:49,370 --> 00:10:51,070
But if you want,

301
00:10:51,080 --> 00:10:52,700
you can feel free to think

302
00:10:52,710 --> 00:10:55,330
of this as an kind of a n implementational detail

303
00:10:55,340 --> 00:10:56,870
that does change the objective a

304
00:10:56,880 --> 00:10:58,920
bit, but is done primarily

305
00:10:58,930 --> 00:11:02,250
for reasons of computational efficiency,

306
00:11:02,260 --> 00:11:05,890
so usually you don't really have to worry about this.

307
00:11:07,640 --> 00:11:09,550
And by the way, in case your

308
00:11:09,560 --> 00:11:11,090
wondering why we don't apply

309
00:11:11,100 --> 00:11:12,560
the kernel's idea to other

310
00:11:12,570 --> 00:11:14,030
algorithms as well like logistic

311
00:11:14,040 --> 00:11:15,660
regression, it turns out

312
00:11:15,670 --> 00:11:16,890
that if you want, you

313
00:11:16,900 --> 00:11:18,540
can actually apply the kernel's

314
00:11:18,550 --> 00:11:19,980
idea and define the source

315
00:11:19,990 --> 00:11:23,870
of features using landmarks and so on for logistic regression.

316
00:11:23,880 --> 00:11:26,430
But the computational tricks that apply

317
00:11:26,440 --> 00:11:28,420
for support vector machines don't

318
00:11:28,430 --> 00:11:31,300
generalize well to other algorithms like logistic regression.

319
00:11:31,310 --> 00:11:33,250
And so, using kernels with

320
00:11:33,260 --> 00:11:34,570
logistic regression is going too

321
00:11:34,580 --> 00:11:36,430
very slow, whereas, because of

322
00:11:36,440 --> 00:11:38,140
computational tricks, like that

323
00:11:38,150 --> 00:11:39,890
embodied and how it modifies

324
00:11:39,900 --> 00:11:41,310
this and the details of how

325
00:11:41,320 --> 00:11:43,230
the support vector machine software is

326
00:11:43,240 --> 00:11:45,290
implemented, support vector machines and

327
00:11:45,300 --> 00:11:47,920
kernels tend go particularly well together.

328
00:11:47,930 --> 00:11:50,240
Whereas, logistic regression and kernels,

329
00:11:50,250 --> 00:11:52,880
you know, you can do it, but this would run very slowly.

330
00:11:52,890 --> 00:11:53,740
And it won't be able to

331
00:11:53,750 --> 00:11:56,030
take advantage of advanced optimization

332
00:11:56,040 --> 00:11:57,520
techniques that people have figured

333
00:11:57,530 --> 00:11:59,130
out for the particular case

334
00:11:59,140 --> 00:12:01,530
of running a support vector machine with a kernel.

335
00:12:01,540 --> 00:12:03,700
But all this pertains only

336
00:12:03,710 --> 00:12:05,220
to how you actually implement

337
00:12:05,230 --> 00:12:07,860
software to minimize the cost function.

338
00:12:07,870 --> 00:12:09,030
I will say more about that in

339
00:12:09,040 --> 00:12:10,140
the next video, but you really don't

340
00:12:10,150 --> 00:12:12,190
need to know about

341
00:12:12,200 --> 00:12:13,660
how to write software to

342
00:12:13,670 --> 00:12:15,160
minimize this  cost function because

343
00:12:15,170 --> 00:12:18,660
you can find very good off the shelf software for doing so.

344
00:12:18,670 --> 00:12:20,130
And just as, you know, I wouldn't

345
00:12:20,140 --> 00:12:21,840
recommend writing code to invert

346
00:12:21,850 --> 00:12:23,140
a matrix or to compute a

347
00:12:23,150 --> 00:12:24,650
square root, I actually do

348
00:12:24,660 --> 00:12:26,550
not recommend writing software to

349
00:12:26,560 --> 00:12:28,230
minimize this cost function yourself,

350
00:12:28,240 --> 00:12:29,770
but instead to use off

351
00:12:29,780 --> 00:12:31,730
the shelf software packages that people

352
00:12:31,740 --> 00:12:33,530
have developed and so

353
00:12:33,540 --> 00:12:35,780
those software packages already embody

354
00:12:35,790 --> 00:12:39,220
these numerical optimization tricks,

355
00:12:39,540 --> 00:12:41,940
so you don't really have to worry about them.

356
00:12:41,950 --> 00:12:43,170
But one other thing that is

357
00:12:43,180 --> 00:12:45,340
worth knowing about is when

358
00:12:45,350 --> 00:12:46,630
you're applying a support vector

359
00:12:46,640 --> 00:12:47,810
machine, how do you

360
00:12:47,820 --> 00:12:51,510
choose the parameters of the support vector machine?

361
00:12:51,520 --> 00:12:52,390
And the last thing I want to

362
00:12:52,400 --> 00:12:53,440
do in this video is say a

363
00:12:53,450 --> 00:12:54,830
little word about the bias and

364
00:12:54,840 --> 00:12:57,890
variance trade offs when using a support vector machine.

365
00:12:57,900 --> 00:12:59,380
When using an SVM, one of

366
00:12:59,390 --> 00:13:00,950
the things you need to choose is

367
00:13:00,960 --> 00:13:04,080
the parameter C which

368
00:13:04,090 --> 00:13:05,970
was in the optimization objective, and

369
00:13:05,980 --> 00:13:07,760
you recall that C played a

370
00:13:07,770 --> 00:13:10,040
role similar to 1 over

371
00:13:10,050 --> 00:13:12,510
lambda, where lambda was the regularization

372
00:13:12,520 --> 00:13:15,350
parameter we had for logistic regression.

373
00:13:15,360 --> 00:13:16,920
So, if you have a

374
00:13:16,930 --> 00:13:19,510
large value of C, this corresponds

375
00:13:19,520 --> 00:13:21,260
to what we have back in logistic

376
00:13:21,270 --> 00:13:22,660
regression, of a small

377
00:13:22,670 --> 00:13:25,970
value of lambda meaning of not using much regularization.

378
00:13:25,980 --> 00:13:27,040
And if you do that, you

379
00:13:27,050 --> 00:13:30,560
tend to have a hypothesis with lower bias and higher variance.

380
00:13:30,570 --> 00:13:31,620
Whereas if you use a smaller

381
00:13:31,630 --> 00:13:33,230
value of C then this

382
00:13:33,240 --> 00:13:34,650
corresponds to when we

383
00:13:34,660 --> 00:13:36,610
are using logistic regression with a

384
00:13:36,620 --> 00:13:38,680
large value of lambda and that corresponds

385
00:13:38,690 --> 00:13:40,460
to a hypothesis with higher

386
00:13:40,470 --> 00:13:42,570
bias and lower variance.

387
00:13:42,580 --> 00:13:44,990
And so, hypothesis with large

388
00:13:45,000 --> 00:13:47,440
C has a higher

389
00:13:47,450 --> 00:13:48,570
variance, and is more prone

390
00:13:48,580 --> 00:13:50,440
to overfitting, whereas hypothesis with

391
00:13:50,450 --> 00:13:52,900
small C has higher bias

392
00:13:52,910 --> 00:13:56,400
and is thus more prone to underfitting.

393
00:13:56,710 --> 00:14:00,200
So this parameter C is one of the parameters we need to choose.

394
00:14:00,210 --> 00:14:02,270
The other one is the parameter

395
00:14:02,280 --> 00:14:05,750
sigma squared, which appeared in the Gaussian kernel.

396
00:14:05,760 --> 00:14:07,740
So if the Gaussian kernel

397
00:14:07,750 --> 00:14:09,630
sigma squared is large, then

398
00:14:09,640 --> 00:14:11,520
in the similarity function, which

399
00:14:11,530 --> 00:14:13,380
was this you know E to the

400
00:14:13,390 --> 00:14:16,210
minus x minus landmark

401
00:14:16,280 --> 00:14:19,450
varies squared over 2 sigma squared.

402
00:14:20,130 --> 00:14:21,470
In this one of the example; If I

403
00:14:21,480 --> 00:14:23,560
have only one feature, x1, if

404
00:14:23,570 --> 00:14:25,480
I have a landmark there at

405
00:14:25,490 --> 00:14:27,950
that location, if sigma

406
00:14:27,960 --> 00:14:29,470
squared is large, then, you know, the

407
00:14:29,480 --> 00:14:30,680
Gaussian kernel would tend to

408
00:14:30,690 --> 00:14:33,950
fall off relatively slowly

409
00:14:33,960 --> 00:14:35,200
and so this would be my feature

410
00:14:35,210 --> 00:14:36,870
f(i), and so this

411
00:14:36,880 --> 00:14:39,050
would be smoother function that varies

412
00:14:39,060 --> 00:14:40,750
more smoothly, and so this will

413
00:14:40,760 --> 00:14:43,020
give you a hypothesis with higher

414
00:14:43,030 --> 00:14:44,540
bias and lower variance, because

415
00:14:44,550 --> 00:14:46,830
the Gaussian kernel that falls off smoothly,

416
00:14:46,840 --> 00:14:48,510
you tend to get a hypothesis that

417
00:14:48,520 --> 00:14:50,120
varies slowly, or varies smoothly

418
00:14:50,130 --> 00:14:52,040
as you change the

419
00:14:52,050 --> 00:14:54,020
input x. Whereas in contrast,

420
00:14:54,030 --> 00:14:55,650
if sigma squared was

421
00:14:55,660 --> 00:14:57,530
small and if that's my

422
00:14:57,540 --> 00:14:58,950
landmark given my 1

423
00:14:58,960 --> 00:15:01,810
feature x1, you know, my Gaussian

424
00:15:01,820 --> 00:15:05,300
kernel, my similarity function, will vary more abruptly.

425
00:15:05,310 --> 00:15:07,570
And in both cases I'd pick

426
00:15:07,580 --> 00:15:08,860
out 1, and so if sigma squared

427
00:15:08,870 --> 00:15:12,180
is small, then my features vary less smoothly.

428
00:15:12,190 --> 00:15:14,240
So if it's just higher slopes

429
00:15:14,250 --> 00:15:16,010
or higher derivatives here.

430
00:15:16,020 --> 00:15:17,320
And using this, you end

431
00:15:17,330 --> 00:15:19,830
up fitting hypotheses of lower

432
00:15:19,840 --> 00:15:23,020
bias and you can have higher variance.

433
00:15:23,030 --> 00:15:24,670
And if you look at this

434
00:15:24,680 --> 00:15:26,440
week's points exercise, you actually get

435
00:15:26,450 --> 00:15:27,320
to play around with some

436
00:15:27,330 --> 00:15:30,980
of these ideas yourself and see these effects yourself.

437
00:15:31,590 --> 00:15:35,310
So, that was the support vector machine with kernels algorithm.

438
00:15:35,320 --> 00:15:37,080
And hopefully this discussion of

439
00:15:37,090 --> 00:15:39,300
bias and variance will give

440
00:15:39,310 --> 00:15:40,450
you some sense of how you

441
00:15:40,460 --> 00:15:44,100
can expect this algorithm to behave as well.

