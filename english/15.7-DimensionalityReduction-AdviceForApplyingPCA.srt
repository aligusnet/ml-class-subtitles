1
00:00:00,090 --> 00:00:01,600
In an earlier video, I had

2
00:00:01,610 --> 00:00:02,830
said that PCA can be

3
00:00:02,840 --> 00:00:06,910
sometimes used to speed up the running time of a learning algorithm.

4
00:00:07,070 --> 00:00:08,360
In this video, I'd like

5
00:00:08,370 --> 00:00:09,810
to explain how to actually

6
00:00:09,820 --> 00:00:11,450
do that, and also say

7
00:00:11,460 --> 00:00:12,980
some, just try to give

8
00:00:12,990 --> 00:00:16,050
some advice about how to apply PCA.

9
00:00:17,110 --> 00:00:20,260
Here's how you can use PCA to speed up a learning algorithm,

10
00:00:20,270 --> 00:00:22,260
and this supervised learning algorithm

11
00:00:22,270 --> 00:00:23,860
speed up is actually the most

12
00:00:23,870 --> 00:00:26,520
common use that I

13
00:00:26,530 --> 00:00:28,700
personally make of PCA.

14
00:00:28,710 --> 00:00:30,290
Let's say you have a supervised

15
00:00:30,300 --> 00:00:31,800
learning problem, note this is

16
00:00:31,810 --> 00:00:33,680
a supervised learning problem with inputs

17
00:00:33,690 --> 00:00:35,800
X and labels Y, and

18
00:00:35,810 --> 00:00:37,820
let's say that your examples

19
00:00:37,830 --> 00:00:39,830
xi are very high dimensional.

20
00:00:39,840 --> 00:00:41,790
So, lets say that your examples, xi are

21
00:00:41,800 --> 00:00:45,500
10,000 dimensional feature vectors.

22
00:00:45,510 --> 00:00:46,690
One example of that, would

23
00:00:46,700 --> 00:00:48,530
be, if you were doing some computer

24
00:00:48,540 --> 00:00:50,640
vision problem, where you have

25
00:00:50,650 --> 00:00:52,770
a 100x100 images, and so

26
00:00:52,780 --> 00:00:55,840
if you have 100x100, that's 10000

27
00:00:55,850 --> 00:00:57,770
pixels, and so if xi are,

28
00:00:57,780 --> 00:00:59,750
you know, feature vectors

29
00:00:59,760 --> 00:01:02,460
that contain your 10000 pixel

30
00:01:02,470 --> 00:01:04,400
intensity values, then

31
00:01:04,410 --> 00:01:06,870
you have 10000 dimensional feature vectors.

32
00:01:06,880 --> 00:01:09,290
So with very high-dimensional

33
00:01:09,300 --> 00:01:11,310
feature vectors like this, running a

34
00:01:11,320 --> 00:01:13,020
learning algorithm can be slow, right?

35
00:01:13,030 --> 00:01:14,780
Just, if you feed 10,000 dimensional

36
00:01:14,790 --> 00:01:17,560
feature vectors into logistic regression,

37
00:01:17,570 --> 00:01:20,440
or a new network, or support vector machine or what have you,

38
00:01:20,450 --> 00:01:22,190
just because that's a lot of data,

39
00:01:22,200 --> 00:01:24,120
that's 10,000 numbers,

40
00:01:24,130 --> 00:01:27,160
it can make your learning algorithm run more slowly.

41
00:01:27,170 --> 00:01:28,670
Fortunately with PCA we'll be

42
00:01:28,680 --> 00:01:30,050
able to reduce the dimension of

43
00:01:30,060 --> 00:01:31,170
this data and so make

44
00:01:31,180 --> 00:01:32,910
our algorithms run more

45
00:01:32,920 --> 00:01:34,580
efficiently. Here's how you

46
00:01:34,590 --> 00:01:35,970
do that. We are going

47
00:01:35,980 --> 00:01:37,390
first check our labeled

48
00:01:37,400 --> 00:01:39,790
training set and extract just

49
00:01:39,800 --> 00:01:42,720
the inputs, we're just going to extract the X's

50
00:01:42,730 --> 00:01:45,850
and temporarily put aside the Y's.

51
00:01:45,860 --> 00:01:47,080
So this will now give us

52
00:01:47,090 --> 00:01:49,390
an unlabelled training set x1

53
00:01:49,400 --> 00:01:51,230
through xm which are maybe

54
00:01:51,240 --> 00:01:53,930
there's a ten thousand dimensional data,

55
00:01:53,940 --> 00:01:55,860
ten thousand dimensional examples we have.

56
00:01:55,870 --> 00:01:58,360
So just extract the input vectors

57
00:01:58,370 --> 00:02:00,430
x1 through xm.

58
00:02:00,650 --> 00:02:02,690
Then we're going to apply PCA

59
00:02:02,700 --> 00:02:03,970
and this will give me a

60
00:02:03,980 --> 00:02:06,400
reduced dimension representation of the

61
00:02:06,410 --> 00:02:08,250
data, so instead of

62
00:02:08,260 --> 00:02:09,770
10,000 dimensional feature vectors I now

63
00:02:09,780 --> 00:02:12,320
have maybe one thousand dimensional feature vectors.

64
00:02:12,330 --> 00:02:15,000
So that's like a 10x savings.

65
00:02:15,110 --> 00:02:17,900
So this gives me, if you will, a new training set.

66
00:02:17,910 --> 00:02:19,610
So whereas previously I might

67
00:02:19,620 --> 00:02:21,480
have had an example x1, y1,

68
00:02:21,490 --> 00:02:24,570
my first training input, is now represented by z1.

69
00:02:24,580 --> 00:02:26,040
And so we'll have a

70
00:02:26,050 --> 00:02:28,200
new sort of training example,

71
00:02:28,210 --> 00:02:30,690
which is Z1 paired with y1.

72
00:02:30,700 --> 00:02:33,760
And similarly Z2, Y2, and so on, up to ZM, YM.

73
00:02:33,770 --> 00:02:35,450
Because my training examples are

74
00:02:35,460 --> 00:02:37,470
now represented with this much

75
00:02:37,480 --> 00:02:41,300
lower dimensional representation Z1, Z2, up to ZM.

76
00:02:41,310 --> 00:02:43,640
Finally, I can take this

77
00:02:43,650 --> 00:02:45,230
reduced dimension training set and

78
00:02:45,240 --> 00:02:46,630
feed it to a learning algorithm maybe

79
00:02:46,640 --> 00:02:48,270
a neural network, maybe logistic

80
00:02:48,280 --> 00:02:49,740
regression, and I can

81
00:02:49,750 --> 00:02:52,220
learn the hypothesis H, that

82
00:02:52,230 --> 00:02:54,320
takes this input, these low-dimensional

83
00:02:54,330 --> 00:02:57,730
representations Z and tries to make predictions.

84
00:02:57,890 --> 00:02:59,450
So if I were using logistic

85
00:02:59,460 --> 00:03:01,050
regression for example, I would

86
00:03:01,060 --> 00:03:03,070
train a hypothesis that outputs, you know,

87
00:03:03,080 --> 00:03:04,170
one over one plus E to

88
00:03:04,180 --> 00:03:07,520
the negative-theta transpose

89
00:03:07,620 --> 00:03:10,600
Z, that

90
00:03:10,610 --> 00:03:11,950
takes this input to one these

91
00:03:11,960 --> 00:03:15,160
z vectors, and tries to make a prediction.

92
00:03:15,260 --> 00:03:16,620
And finally, if you have

93
00:03:16,630 --> 00:03:17,910
a new example, maybe a new

94
00:03:17,920 --> 00:03:20,210
test example X. What

95
00:03:20,220 --> 00:03:22,120
you do is you would

96
00:03:22,130 --> 00:03:24,950
take your test example x,

97
00:03:24,960 --> 00:03:26,980
map it through the same mapping

98
00:03:26,990 --> 00:03:28,210
that was found by PCA

99
00:03:28,220 --> 00:03:30,380
to get you your corresponding z.

100
00:03:30,390 --> 00:03:31,940
And that z then gets

101
00:03:31,950 --> 00:03:33,900
fed to this hypothesis, and this

102
00:03:33,910 --> 00:03:35,740
hypothesis then makes a

103
00:03:35,750 --> 00:03:38,100
prediction on your input x.

104
00:03:38,110 --> 00:03:40,500
One final note, what PCA does

105
00:03:40,510 --> 00:03:42,700
is it defines a mapping from

106
00:03:42,710 --> 00:03:45,950
x to z and

107
00:03:45,960 --> 00:03:47,040
this mapping from x to

108
00:03:47,050 --> 00:03:48,570
z should be defined by running

109
00:03:48,580 --> 00:03:51,640
PCA only on the training sets.

110
00:03:51,650 --> 00:03:53,520
And in particular, this mapping that

111
00:03:53,530 --> 00:03:54,940
PCA is learning, right, this

112
00:03:54,950 --> 00:03:58,200
mapping, what that does is it computes the set of parameters.

113
00:03:58,210 --> 00:04:01,230
That's the feature scaling and mean normalization.

114
00:04:01,240 --> 00:04:04,670
And there's also computing this matrix U reduced.

115
00:04:04,680 --> 00:04:05,660
But all of these things that

116
00:04:05,670 --> 00:04:07,110
U reduce, that's like a

117
00:04:07,120 --> 00:04:08,660
parameter that is learned

118
00:04:08,670 --> 00:04:10,140
by PCA and we should

119
00:04:10,150 --> 00:04:12,470
be fitting our parameters only to

120
00:04:12,480 --> 00:04:14,030
our training sets and not

121
00:04:14,040 --> 00:04:16,360
to our cross validation or test sets and

122
00:04:16,370 --> 00:04:18,170
so these things the U reduced

123
00:04:18,180 --> 00:04:19,810
so on, that should be

124
00:04:19,820 --> 00:04:23,290
obtained by running PCA only on your training set.

125
00:04:23,300 --> 00:04:27,340
And then having found U reduced, or having found the parameters for feature

126
00:04:27,350 --> 00:04:29,310
scaling where the mean normalization

127
00:04:29,320 --> 00:04:32,170
and scaling the scale

128
00:04:32,180 --> 00:04:34,750
that you divide the features by to get them on to comparable scales.

129
00:04:34,760 --> 00:04:36,560
Having found all those parameters

130
00:04:36,570 --> 00:04:38,210
on the training set, you can

131
00:04:38,220 --> 00:04:41,810
then apply the same mapping to other examples that may be

132
00:04:41,820 --> 00:04:45,170
In your cross-validation sets or

133
00:04:45,180 --> 00:04:47,140
in your test sets, OK?

134
00:04:47,150 --> 00:04:48,440
Just to summarize, when you're

135
00:04:48,450 --> 00:04:49,890
running PCA, run your

136
00:04:49,900 --> 00:04:51,210
PCA only on the

137
00:04:51,220 --> 00:04:52,480
training set portion of the

138
00:04:52,490 --> 00:04:56,400
data not the cross-validation set or the test set portion of your data.

139
00:04:56,410 --> 00:04:57,860
And that defines the mapping from

140
00:04:57,870 --> 00:04:58,940
x to z and you can

141
00:04:58,950 --> 00:05:00,550
then apply that mapping to

142
00:05:00,560 --> 00:05:02,280
your cross-validation set and your

143
00:05:02,290 --> 00:05:03,440
test set and by the

144
00:05:03,450 --> 00:05:04,990
way in this example I talked

145
00:05:05,000 --> 00:05:06,940
about reducing the data from

146
00:05:06,950 --> 00:05:08,730
ten thousand dimensional to one

147
00:05:08,740 --> 00:05:10,650
thousand dimensional, this is actually

148
00:05:10,660 --> 00:05:12,270
not that unrealistic. For many

149
00:05:12,280 --> 00:05:16,220
problems we actually reduce the dimensional data. You

150
00:05:17,600 --> 00:05:18,770
know by 5x maybe by 10x

151
00:05:18,780 --> 00:05:21,260
and still retain most of the variance and we can do this

152
00:05:21,270 --> 00:05:23,890
barely effecting the performance,

153
00:05:23,900 --> 00:05:26,230
in terms of classification accuracy, let's say,

154
00:05:26,240 --> 00:05:28,760
barely affecting the classification

155
00:05:28,770 --> 00:05:31,080
accuracy of the learning algorithm.

156
00:05:31,090 --> 00:05:32,580
And by working with lower dimensional

157
00:05:32,590 --> 00:05:34,050
data our learning algorithm

158
00:05:34,060 --> 00:05:36,900
can often run much much faster.

159
00:05:36,910 --> 00:05:38,400
To summarize, we've so far talked

160
00:05:38,410 --> 00:05:41,960
about the following applications of PCA.

161
00:05:41,970 --> 00:05:44,010
First is the compression application where

162
00:05:44,020 --> 00:05:45,490
we might do so to reduce

163
00:05:45,500 --> 00:05:46,580
the memory or the disk space

164
00:05:46,590 --> 00:05:48,230
needed to store data and we

165
00:05:48,240 --> 00:05:49,450
just talked about how to

166
00:05:49,460 --> 00:05:52,090
use this to speed up a learning algorithm.

167
00:05:52,100 --> 00:05:54,120
In these applications, in order

168
00:05:54,130 --> 00:05:56,410
to choose K, often we'll

169
00:05:56,420 --> 00:05:59,150
do so according to, figuring

170
00:05:59,160 --> 00:06:00,800
out what is the percentage of

171
00:06:00,810 --> 00:06:04,770
variance retained, and so

172
00:06:04,780 --> 00:06:06,560
for this learning algorithm, speed

173
00:06:06,570 --> 00:06:10,520
up application often will retain 99%  of the variance.

174
00:06:10,530 --> 00:06:12,090
That would be a very typical choice

175
00:06:12,100 --> 00:06:14,720
for how to choose k. So

176
00:06:14,730 --> 00:06:17,840
that's how you choose k for these compression applications.

177
00:06:17,850 --> 00:06:20,750
Whereas for visualization applications

178
00:06:20,760 --> 00:06:22,220
while usually we know

179
00:06:22,230 --> 00:06:24,010
how to plot only two dimensional

180
00:06:24,020 --> 00:06:26,530
data or three dimensional data,

181
00:06:26,540 --> 00:06:28,820
and so for visualization applications, we'll

182
00:06:28,830 --> 00:06:29,700
usually choose k equals 2

183
00:06:29,710 --> 00:06:32,730
or k equals 3, because we can plot

184
00:06:32,740 --> 00:06:34,500
only 2D and 3D data sets.

185
00:06:34,510 --> 00:06:36,010
So that summarizes the main

186
00:06:36,020 --> 00:06:37,860
applications of PCA, as well

187
00:06:37,870 --> 00:06:39,660
as how to choose the

188
00:06:39,670 --> 00:06:42,880
value of k for these different applications.

189
00:06:42,890 --> 00:06:46,390
I should mention that there is often one frequent misuse

190
00:06:46,400 --> 00:06:48,790
of PCA and

191
00:06:48,800 --> 00:06:50,570
you sometimes hear about others

192
00:06:50,580 --> 00:06:52,220
doing this hopefully not too often.

193
00:06:52,230 --> 00:06:55,470
I just want to mention this so that you know not to do it.

194
00:06:55,480 --> 00:06:56,530
And there is one bad use of

195
00:06:56,540 --> 00:07:00,370
PCA, which iss to try to use it to prevent over-fitting.

196
00:07:00,380 --> 00:07:01,900
Here's the reasoning.

197
00:07:01,910 --> 00:07:03,720
This is not a great

198
00:07:03,730 --> 00:07:04,660
way to use PCA,

199
00:07:04,670 --> 00:07:05,680
but here's the reasoning behind

200
00:07:05,690 --> 00:07:07,340
this method, which is,you know

201
00:07:07,350 --> 00:07:09,290
if we have Xi, then

202
00:07:09,300 --> 00:07:10,820
maybe we'll have n features, but

203
00:07:10,830 --> 00:07:12,740
if we compress the data, and

204
00:07:12,750 --> 00:07:14,260
use Zi instead

205
00:07:14,270 --> 00:07:15,550
and that reduces the number

206
00:07:15,560 --> 00:07:17,280
of features to k, which

207
00:07:17,290 --> 00:07:19,400
could be much lower dimensional. And

208
00:07:19,410 --> 00:07:21,480
so if we have a much smaller

209
00:07:21,490 --> 00:07:22,760
number of features, if k

210
00:07:22,770 --> 00:07:26,080
is 1,000 and n is

211
00:07:26,090 --> 00:07:27,770
10,000, then if we have

212
00:07:27,780 --> 00:07:29,660
only 1,000 dimensional data, maybe

213
00:07:29,670 --> 00:07:31,250
we're less likely to over-fit

214
00:07:31,260 --> 00:07:33,270
than if we were using 10,000-dimensional

215
00:07:33,280 --> 00:07:35,940
data with like a thousand features.

216
00:07:35,950 --> 00:07:37,350
So some people think

217
00:07:37,360 --> 00:07:39,940
of PCA as a way to prevent over-fitting.

218
00:07:39,950 --> 00:07:42,100
But just to emphasize this

219
00:07:42,110 --> 00:07:44,250
is a bad application of PCA

220
00:07:44,260 --> 00:07:46,510
and I do not recommend doing this.

221
00:07:46,520 --> 00:07:48,990
And it's not that this method works badly.

222
00:07:49,000 --> 00:07:50,320
If you want to use

223
00:07:50,330 --> 00:07:51,880
this method to reduce the dimensional

224
00:07:51,890 --> 00:07:53,680
data, to try to prevent over-fitting,

225
00:07:53,690 --> 00:07:55,550
it might actually work OK.

226
00:07:55,560 --> 00:07:57,030
But this just is not

227
00:07:57,040 --> 00:07:58,670
a good way to address

228
00:07:58,680 --> 00:08:00,500
over-fitting and instead, if you're

229
00:08:00,510 --> 00:08:02,020
worried about over-fitting, there is

230
00:08:02,030 --> 00:08:03,790
a much better way to address

231
00:08:03,800 --> 00:08:05,890
it, to use regularization instead of

232
00:08:05,900 --> 00:08:08,660
using PCA to reduce the dimension of the data.

233
00:08:08,670 --> 00:08:11,000
And the reason is, if

234
00:08:11,010 --> 00:08:12,890
you think about how PCA works,

235
00:08:12,900 --> 00:08:14,520
it does not use the labels

236
00:08:14,530 --> 00:08:16,040
y. You are just looking

237
00:08:16,050 --> 00:08:17,330
at your inputs xi, and you're

238
00:08:17,340 --> 00:08:19,120
using that to find a

239
00:08:19,130 --> 00:08:21,380
lower-dimensional approximation to your data.

240
00:08:21,390 --> 00:08:23,180
So what PCA does,

241
00:08:23,190 --> 00:08:26,450
is it throws away some information.

242
00:08:26,460 --> 00:08:28,170
It throws away or reduces the

243
00:08:28,180 --> 00:08:30,100
dimension of your data without

244
00:08:30,110 --> 00:08:32,370
knowing what the values of y

245
00:08:32,380 --> 00:08:34,240
is, so this is probably

246
00:08:34,250 --> 00:08:35,910
okay using PCA this way

247
00:08:35,920 --> 00:08:37,980
is probably okay if, say

248
00:08:37,990 --> 00:08:39,400
99 percent of the

249
00:08:39,410 --> 00:08:40,820
variance is retained, if you're keeping most

250
00:08:40,830 --> 00:08:42,090
of the variance, but

251
00:08:42,100 --> 00:08:45,000
it might also throw away some valuable information.

252
00:08:45,010 --> 00:08:46,300
And it turns out that

253
00:08:46,310 --> 00:08:47,810
if you're retaining 99% of

254
00:08:47,820 --> 00:08:49,350
the variance or 95%

255
00:08:49,360 --> 00:08:51,010
of the variance or whatever, it

256
00:08:51,020 --> 00:08:52,710
turns out that just using

257
00:08:52,720 --> 00:08:54,780
regularization will often give

258
00:08:54,790 --> 00:08:56,210
you at least as good

259
00:08:56,220 --> 00:08:58,890
a method for preventing over-fitting

260
00:08:58,900 --> 00:09:00,580
and regularization will often just

261
00:09:00,590 --> 00:09:02,340
work better, because when you

262
00:09:02,350 --> 00:09:04,240
are applying linear regression or logistic

263
00:09:04,250 --> 00:09:05,590
regression or some other method

264
00:09:05,600 --> 00:09:08,000
with regularization, well, this minimization

265
00:09:08,010 --> 00:09:09,470
problem actually knows what the

266
00:09:09,480 --> 00:09:10,950
values of y are, and

267
00:09:10,960 --> 00:09:12,870
so is less likely to throw

268
00:09:12,880 --> 00:09:14,720
away some valuable information, whereas

269
00:09:14,730 --> 00:09:16,050
PCA doesn't make use

270
00:09:16,060 --> 00:09:17,840
of the labels and is more

271
00:09:17,850 --> 00:09:20,220
likely to throw away valuable information.

272
00:09:20,230 --> 00:09:21,610
So, to summarize, it is

273
00:09:21,620 --> 00:09:23,000
a good use of PCA, if your

274
00:09:23,010 --> 00:09:24,520
main motivation to speed up

275
00:09:24,530 --> 00:09:26,780
your learning algorithm, but using

276
00:09:26,790 --> 00:09:28,640
PCA to prevent over-fitting, that

277
00:09:28,650 --> 00:09:30,020
is not a good use of

278
00:09:30,030 --> 00:09:32,890
PCA, and using regularization instead

279
00:09:32,900 --> 00:09:36,430
is really what many people

280
00:09:36,440 --> 00:09:41,300
would recommend doing instead. Finally,

281
00:09:41,310 --> 00:09:43,740
one last misuse of PCA.

282
00:09:43,750 --> 00:09:46,260
And so I should say PCA is a very useful algorithm,

283
00:09:46,270 --> 00:09:50,220
I often use it for the compression on the visualization purposes.

284
00:09:50,230 --> 00:09:51,560
But, what I sometimes

285
00:09:51,570 --> 00:09:53,700
see, is also people sometimes

286
00:09:53,710 --> 00:09:56,210
use PCA where it shouldn't be.

287
00:09:56,220 --> 00:09:58,020
So, here's a pretty common thing that

288
00:09:58,030 --> 00:09:59,320
I see, which is if someone

289
00:09:59,330 --> 00:10:01,000
is designing a machine-learning system,

290
00:10:01,010 --> 00:10:02,190
they may write down the

291
00:10:02,200 --> 00:10:05,050
plan like this: let's design a learning system.

292
00:10:05,060 --> 00:10:06,560
Get a training set and then,

293
00:10:06,570 --> 00:10:07,390
you know, what I'm going to

294
00:10:07,400 --> 00:10:08,850
do is run PCA, then train

295
00:10:08,860 --> 00:10:11,670
logistic regression and then test on my test data.

296
00:10:11,680 --> 00:10:13,080
So often at the very

297
00:10:13,090 --> 00:10:14,590
start of a project,

298
00:10:14,600 --> 00:10:15,710
someone will just write out a

299
00:10:15,720 --> 00:10:17,300
project plan than says lets

300
00:10:17,310 --> 00:10:20,110
do these four steps with PCA inside.

301
00:10:20,210 --> 00:10:21,520
Before writing down a project

302
00:10:21,530 --> 00:10:23,550
plan the incorporates PCA like

303
00:10:23,560 --> 00:10:25,020
this, one very good

304
00:10:25,030 --> 00:10:27,620
question to ask is, well, what if we

305
00:10:27,630 --> 00:10:29,530
were to just do the whole

306
00:10:29,540 --> 00:10:32,160
without using PCA.

307
00:10:32,170 --> 00:10:33,790
And often people do not

308
00:10:33,800 --> 00:10:35,430
consider this step before

309
00:10:35,440 --> 00:10:37,910
coming up with a complicated project plan and

310
00:10:37,920 --> 00:10:40,800
implementing PCA and so on.

311
00:10:40,810 --> 00:10:43,040
And sometime, and so specifically,

312
00:10:43,050 --> 00:10:44,660
what I often advise people

313
00:10:44,670 --> 00:10:46,440
is, before you implement

314
00:10:46,450 --> 00:10:48,210
PCA, I would first

315
00:10:48,220 --> 00:10:49,590
suggest that, you know, do

316
00:10:49,600 --> 00:10:50,840
whatever it is, take whatever it

317
00:10:50,850 --> 00:10:52,440
is you want to do and first

318
00:10:52,450 --> 00:10:53,970
consider doing it with your

319
00:10:53,980 --> 00:10:56,590
original raw data xi, and

320
00:10:56,600 --> 00:10:57,950
only if that doesn't do

321
00:10:57,960 --> 00:11:01,000
what you want, then implement PCA before using Zi.

322
00:11:01,010 --> 00:11:03,020
So, before using PCA you know,

323
00:11:03,030 --> 00:11:04,350
instead of reducing the dimension

324
00:11:04,360 --> 00:11:06,630
of the data, I would consider

325
00:11:06,640 --> 00:11:08,410
well, let's ditch this PCA step,

326
00:11:08,420 --> 00:11:10,030
and I would consider, let's

327
00:11:10,040 --> 00:11:12,430
just train my learning algorithm

328
00:11:12,440 --> 00:11:14,400
on my original data.

329
00:11:14,410 --> 00:11:16,290
Let's just use my original raw

330
00:11:16,300 --> 00:11:18,170
inputs xi, and I would

331
00:11:18,180 --> 00:11:19,710
recommend, instead of putting

332
00:11:19,720 --> 00:11:21,020
PCA into the algorithm, just

333
00:11:21,030 --> 00:11:24,080
try doing whatever it is you're doing with the xi first.

334
00:11:24,090 --> 00:11:25,140
And only if you have

335
00:11:25,150 --> 00:11:26,470
a reason to believe that doesn't

336
00:11:26,480 --> 00:11:27,780
work, so that only if your

337
00:11:27,790 --> 00:11:29,500
learning algorithm ends up

338
00:11:29,510 --> 00:11:31,270
running too slowly, or only if

339
00:11:31,280 --> 00:11:32,900
the memory requirement or the

340
00:11:32,910 --> 00:11:34,420
disk space requirement is too large,

341
00:11:34,430 --> 00:11:36,180
so you want to compress your

342
00:11:36,190 --> 00:11:37,990
representation, but if only

343
00:11:38,000 --> 00:11:39,350
using the xi doesn't work,

344
00:11:39,360 --> 00:11:40,940
only if you have evidence or strong

345
00:11:40,950 --> 00:11:42,370
reason to believe that using

346
00:11:42,380 --> 00:11:44,370
the xi won't work, then implement

347
00:11:44,380 --> 00:11:47,980
PCA and consider using the compressed representation.

348
00:11:47,990 --> 00:11:49,090
Because what I do see, is

349
00:11:49,100 --> 00:11:50,520
sometimes people start off with

350
00:11:50,530 --> 00:11:52,090
a project plan that incorporates PCA

351
00:11:52,100 --> 00:11:54,640
inside, and sometimes they,

352
00:11:54,650 --> 00:11:55,810
whatever they're

353
00:11:55,820 --> 00:11:57,650
doing will work just

354
00:11:57,660 --> 00:11:59,830
fine, even with out using PCA instead.

355
00:11:59,840 --> 00:12:01,850
So, just consider that

356
00:12:01,860 --> 00:12:03,310
as an alternative as well, before you

357
00:12:03,320 --> 00:12:04,290
go to spend a lot of

358
00:12:04,300 --> 00:12:05,760
time to get PCA in, figure

359
00:12:05,770 --> 00:12:08,240
out what k is and so on.

360
00:12:08,250 --> 00:12:09,790
So, that's it for PCA.

361
00:12:09,800 --> 00:12:11,070
Despite these last sets of

362
00:12:11,080 --> 00:12:12,680
comments, PCA is an

363
00:12:12,690 --> 00:12:14,140
incredibly useful algorithm, when you

364
00:12:14,150 --> 00:12:16,060
use it for the appropriate applications

365
00:12:16,070 --> 00:12:17,760
and I've actually used PCA pretty

366
00:12:17,770 --> 00:12:19,570
often and for me,

367
00:12:19,580 --> 00:12:20,840
I use it mostly to speed

368
00:12:20,850 --> 00:12:22,870
up the running time of my learning algorithms.

369
00:12:22,880 --> 00:12:24,390
But I think, just as

370
00:12:24,400 --> 00:12:26,010
common an application of PCA,

371
00:12:26,020 --> 00:12:27,400
is to use it to

372
00:12:27,410 --> 00:12:29,610
compress data, to reduce

373
00:12:29,620 --> 00:12:30,980
the memory or disk space

374
00:12:30,990 --> 00:12:34,260
requirements, or to use it to visualize data.

375
00:12:34,270 --> 00:12:35,740
And PCA is one of

376
00:12:35,750 --> 00:12:36,980
the most commonly used and one

377
00:12:36,990 --> 00:12:40,050
of the most powerful unsupervised learning algorithms.

378
00:12:40,060 --> 00:12:41,410
And with what you've learned

379
00:12:41,420 --> 00:12:43,490
in these videos, I think hopefully

380
00:12:43,500 --> 00:12:45,140
you'll be able to implement

381
00:12:45,150 --> 00:12:46,490
PCA and use them

382
00:12:46,500 --> 00:12:49,430
through all of these purposes as well.

