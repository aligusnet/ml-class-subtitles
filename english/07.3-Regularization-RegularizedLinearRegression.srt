1
00:00:00,260 --> 00:00:01,670
For linear regression, we had

2
00:00:01,680 --> 00:00:03,480
previously worked out two learning

3
00:00:03,490 --> 00:00:05,170
algorithms, one based on

4
00:00:05,180 --> 00:00:08,740
gradient descent and one based on the normal equation.

5
00:00:08,750 --> 00:00:09,880
In this video we will take

6
00:00:09,890 --> 00:00:12,280
those two algorithms and generalize

7
00:00:12,290 --> 00:00:14,320
them to the case of regularized

8
00:00:14,330 --> 00:00:18,090
linear regression. Here's the

9
00:00:18,100 --> 00:00:20,190
optimization objective, that we

10
00:00:20,200 --> 00:00:23,350
came up with last time for regularized linear regression.

11
00:00:23,360 --> 00:00:24,970
This first part is our

12
00:00:24,980 --> 00:00:28,160
usual, objective for linear regression,

13
00:00:28,170 --> 00:00:30,190
and we now have this additional

14
00:00:30,200 --> 00:00:32,440
regularization term, where londer

15
00:00:32,450 --> 00:00:35,210
is our regularization parameter, and

16
00:00:35,220 --> 00:00:37,150
we like to find parameters theta,

17
00:00:37,160 --> 00:00:39,020
that minimizes this cost function,

18
00:00:39,030 --> 00:00:41,830
this regularized cost function, J of theta.

19
00:00:41,840 --> 00:00:43,430
Previously, we were using

20
00:00:43,440 --> 00:00:46,610
gradient descent for the original

21
00:00:46,620 --> 00:00:48,760
cost function, without the regularization

22
00:00:48,770 --> 00:00:50,050
term, and we had

23
00:00:50,060 --> 00:00:52,360
the following algorithm for regular

24
00:00:52,370 --> 00:00:54,650
linear regression, without regularization.

25
00:00:54,660 --> 00:00:56,320
We will repeatedly update the

26
00:00:56,330 --> 00:00:58,260
parameters theta J as follows

27
00:00:58,270 --> 00:01:00,390
for J equals 1,2 up

28
00:01:00,400 --> 00:01:02,520
through n. Let me

29
00:01:02,530 --> 00:01:04,230
take this and just write

30
00:01:04,240 --> 00:01:07,200
the case for theta zero separately.

31
00:01:07,210 --> 00:01:08,710
So, you know, I'm just gonna

32
00:01:08,720 --> 00:01:10,150
write the update for theta

33
00:01:10,160 --> 00:01:12,670
zero separately, then for

34
00:01:12,680 --> 00:01:14,770
the update for the parameters

35
00:01:14,780 --> 00:01:17,360
1, 2, 3, and so on up

36
00:01:17,370 --> 00:01:19,960
to n. So, I haven't changed anything yet, right?

37
00:01:19,970 --> 00:01:21,290
This is just writing the update

38
00:01:21,300 --> 00:01:23,540
for theta zero separately from the

39
00:01:23,550 --> 00:01:25,500
updates from theta 1, theta

40
00:01:25,510 --> 00:01:27,030
2, theta 3, up to theta n. And

41
00:01:27,040 --> 00:01:28,220
the reason I want to do this

42
00:01:28,230 --> 00:01:29,870
is you may remember

43
00:01:29,880 --> 00:01:32,610
that for our regularized linear regression,

44
00:01:32,620 --> 00:01:34,430
we penalize the parameters theta

45
00:01:34,440 --> 00:01:35,850
1, theta 2, and so

46
00:01:35,860 --> 00:01:38,810
on up to theta n, but we don't penalize theta zero.

47
00:01:38,820 --> 00:01:40,400
So when we modify this

48
00:01:40,410 --> 00:01:42,740
algorithm for regularized

49
00:01:42,750 --> 00:01:44,700
linear regression, we're going to

50
00:01:44,710 --> 00:01:48,370
end up treating theta zero slightly differently.

51
00:01:48,560 --> 00:01:50,490
Concretely, if we

52
00:01:50,500 --> 00:01:52,290
want to take this algorithm and

53
00:01:52,300 --> 00:01:53,860
modify it to use the

54
00:01:53,870 --> 00:01:55,730
regularized objective, all we

55
00:01:55,740 --> 00:01:57,340
need to do is take this

56
00:01:57,350 --> 00:02:00,450
term at the bottom and modify as follows.

57
00:02:00,460 --> 00:02:02,660
We're gonna take this term and add

58
00:02:02,670 --> 00:02:06,320
minus londer M,

59
00:02:06,330 --> 00:02:09,090
times theta J. And

60
00:02:09,100 --> 00:02:10,990
if you implement this, then you

61
00:02:11,000 --> 00:02:13,950
have gradient descent for trying

62
00:02:13,960 --> 00:02:16,150
to minimize the regularized cost

63
00:02:16,160 --> 00:02:19,510
function J of F theta, and concretely,

64
00:02:19,520 --> 00:02:20,670
I'm not gonna do the

65
00:02:20,680 --> 00:02:22,380
calculus to prove it, but

66
00:02:22,390 --> 00:02:23,680
concretely if you look

67
00:02:23,690 --> 00:02:27,720
at this term, this term that's written is square brackets.

68
00:02:27,730 --> 00:02:29,370
If you know calculus, it's possible

69
00:02:29,380 --> 00:02:31,360
to prove that that term is

70
00:02:31,370 --> 00:02:33,970
the partial derivative, with respect of

71
00:02:33,980 --> 00:02:35,650
J of theta, using the new

72
00:02:35,660 --> 00:02:38,130
definition of J of theta

73
00:02:38,140 --> 00:02:39,500
with the regularization term.

74
00:02:39,510 --> 00:02:42,750
And somebody on this

75
00:02:42,760 --> 00:02:44,740
term up on top,

76
00:02:44,750 --> 00:02:45,670
which I guess I am

77
00:02:45,680 --> 00:02:47,990
drawing the salient box

78
00:02:48,000 --> 00:02:49,930
that's still the partial derivative

79
00:02:49,940 --> 00:02:53,670
respect of theta zero for J of theta.

80
00:02:53,680 --> 00:02:55,590
If you look at the update for

81
00:02:55,600 --> 00:02:56,900
theta J, it's possible to

82
00:02:56,910 --> 00:02:59,850
show's something pretty interesting, concretely

83
00:02:59,860 --> 00:03:01,270
theta J gets updated as

84
00:03:01,280 --> 00:03:04,080
theta J, minus alpha times,

85
00:03:04,090 --> 00:03:05,370
and then you have this other term

86
00:03:05,380 --> 00:03:06,900
here that depends on theta J

87
00:03:06,910 --> 00:03:08,410
. So if you

88
00:03:08,420 --> 00:03:10,020
group all the terms together

89
00:03:10,030 --> 00:03:11,770
that depending on theta J. We

90
00:03:11,780 --> 00:03:13,660
can show that this update can

91
00:03:13,670 --> 00:03:15,190
be written equivalently as

92
00:03:15,200 --> 00:03:16,460
follows and all I did

93
00:03:16,470 --> 00:03:18,300
was have, you know, theta J

94
00:03:18,310 --> 00:03:20,440
here is theta J times

95
00:03:20,450 --> 00:03:22,900
1 and this term is

96
00:03:22,910 --> 00:03:25,130
londer over M. There's also an alpha

97
00:03:25,140 --> 00:03:26,170
here, so you end up

98
00:03:26,180 --> 00:03:27,960
with alpha londer over

99
00:03:27,970 --> 00:03:31,810
m, multiply them to

100
00:03:31,820 --> 00:03:34,220
theta J and this term here, one minus

101
00:03:34,230 --> 00:03:36,590
alpha times londer M, is

102
00:03:36,600 --> 00:03:40,970
a pretty interesting term, it has a pretty interesting effect.

103
00:03:42,310 --> 00:03:43,880
Concretely, this term one

104
00:03:43,890 --> 00:03:45,720
minus alpha times londer over

105
00:03:45,730 --> 00:03:46,860
M, is going to be

106
00:03:46,870 --> 00:03:48,790
a number that's, you know, usually a number

107
00:03:48,800 --> 00:03:50,600
that's a loop and less than 1,

108
00:03:50,610 --> 00:03:51,910
right? Because of

109
00:03:51,920 --> 00:03:54,060
alpha times londer over M is

110
00:03:54,070 --> 00:03:56,120
going to be positive and usually, if you're learning rate is small and M is large.

111
00:03:56,130 --> 00:03:58,640


112
00:03:58,650 --> 00:03:59,640
That's usually pretty small.

113
00:03:59,650 --> 00:04:00,730
So this term here, it's going

114
00:04:00,740 --> 00:04:03,330
to be a number, it's usually, you know, a little bit less than one.

115
00:04:03,340 --> 00:04:04,320
So think of it as

116
00:04:04,330 --> 00:04:07,360
a number like 0.99, let's say

117
00:04:07,380 --> 00:04:09,110
and so, the effect of our

118
00:04:09,120 --> 00:04:10,680
updates of theta J is we're

119
00:04:10,690 --> 00:04:12,400
going to say that theta J

120
00:04:12,410 --> 00:04:15,760
gets replaced by thetata J times 0.99.

121
00:04:15,770 --> 00:04:18,480
Alright so theta J

122
00:04:18,490 --> 00:04:21,270
times 0.99 has the effect of

123
00:04:21,280 --> 00:04:23,660
shrinking theta J a little bit towards 0.

124
00:04:23,670 --> 00:04:26,210
So this makes theta J a bit smaller.

125
00:04:26,220 --> 00:04:28,410
More formally, this you know, this

126
00:04:28,420 --> 00:04:29,860
square norm of theta J

127
00:04:29,870 --> 00:04:31,710
is smaller and then

128
00:04:31,720 --> 00:04:33,900
after that the second

129
00:04:33,910 --> 00:04:35,970
term here, that's actually

130
00:04:35,980 --> 00:04:38,040
exactly the same as the

131
00:04:38,050 --> 00:04:40,740
original gradient descent updated that we had.

132
00:04:40,750 --> 00:04:44,260
Before we added all this regularization stuff.

133
00:04:44,270 --> 00:04:47,370
So, hopefully this gradient

134
00:04:47,380 --> 00:04:48,870
descent, hopefully this update makes

135
00:04:48,880 --> 00:04:51,540
sense, when we're using regularized linear

136
00:04:51,550 --> 00:04:53,310
regression what we're doing is on

137
00:04:53,320 --> 00:04:55,410
every regularization were multiplying data

138
00:04:55,420 --> 00:04:56,390
J by a number that

139
00:04:56,400 --> 00:04:57,390
is a little bit less than one, so

140
00:04:57,400 --> 00:04:59,220
we're shrinking the parameter a

141
00:04:59,230 --> 00:05:00,490
little bit, and then we're

142
00:05:00,500 --> 00:05:04,160
performing a, you know, similar update as before.

143
00:05:04,170 --> 00:05:05,600
Of course that's just the

144
00:05:05,610 --> 00:05:08,900
intuition behind what this particular update is doing.

145
00:05:08,910 --> 00:05:10,570
Mathematically, what it's doing

146
00:05:10,580 --> 00:05:13,120
is exactly gradient descent on

147
00:05:13,130 --> 00:05:15,140
the cost function J of theta

148
00:05:15,150 --> 00:05:16,470
that we defined on the previous

149
00:05:16,480 --> 00:05:19,770
slide that uses the regularization term.

150
00:05:19,780 --> 00:05:21,460
Gradient descent was just

151
00:05:21,470 --> 00:05:24,460
one our two algorithms for

152
00:05:24,470 --> 00:05:26,620
fitting a linear regression model.

153
00:05:26,630 --> 00:05:28,150
The second algorithm was the

154
00:05:28,160 --> 00:05:29,670
one based on the normal

155
00:05:29,680 --> 00:05:31,730
equation where, what we

156
00:05:31,740 --> 00:05:33,050
did was we created the

157
00:05:33,060 --> 00:05:35,070
design matrix "x" where each

158
00:05:35,080 --> 00:05:38,510
row corresponded to a separate training example.

159
00:05:38,520 --> 00:05:40,160
And we created a vector

160
00:05:40,170 --> 00:05:41,930
Y, so this is

161
00:05:41,940 --> 00:05:43,580
a vector that is an

162
00:05:43,590 --> 00:05:46,000
M dimensional vector and that

163
00:05:46,010 --> 00:05:48,460
contain the labels from a training set.

164
00:05:48,470 --> 00:05:49,820
So whereas X is an

165
00:05:49,830 --> 00:05:53,580
M by N plus 1 dimensional matrix.

166
00:05:53,590 --> 00:05:55,770
Y is an M dimensional

167
00:05:55,780 --> 00:05:58,020
vector and in order

168
00:05:58,030 --> 00:05:59,460
to minimize the cost

169
00:05:59,470 --> 00:06:01,460
function change we found

170
00:06:01,470 --> 00:06:03,220
that of one way

171
00:06:03,230 --> 00:06:04,660
to do is to set

172
00:06:04,670 --> 00:06:07,530
theta to be equal to this.

173
00:06:07,540 --> 00:06:10,540
We have X transpose X,

174
00:06:10,860 --> 00:06:13,010
inverse X transpose Y.

175
00:06:13,020 --> 00:06:14,110
I am leaving room here, to fill

176
00:06:14,120 --> 00:06:17,640
in stuff of course. And what this

177
00:06:17,650 --> 00:06:19,170
value for theta does, is

178
00:06:19,180 --> 00:06:21,240
this minimizes the cost

179
00:06:21,250 --> 00:06:22,830
function J of theta when

180
00:06:22,840 --> 00:06:26,450
we were not using regularization. Now

181
00:06:26,460 --> 00:06:28,770
that we are using regularization, if

182
00:06:28,780 --> 00:06:30,510
you were to derive what the

183
00:06:30,520 --> 00:06:31,900
minimum is, and just to

184
00:06:31,910 --> 00:06:32,970
give you a sense of how to

185
00:06:32,980 --> 00:06:34,210
derive the minimum, the way

186
00:06:34,220 --> 00:06:35,920
you derive it is you know,

187
00:06:35,930 --> 00:06:38,330
take partial derivatives in respect

188
00:06:38,340 --> 00:06:40,820
to each parameter, set this

189
00:06:40,830 --> 00:06:42,050
to zero, and then do

190
00:06:42,060 --> 00:06:43,090
a bunch of math, and you can

191
00:06:43,100 --> 00:06:45,540
then show that is a formula

192
00:06:45,550 --> 00:06:48,580
like this that minimizes the cost function.

193
00:06:48,590 --> 00:06:52,230
And concretely, if you

194
00:06:52,240 --> 00:06:54,240
are using regularization then this

195
00:06:54,250 --> 00:06:56,470
formula changes as follows. Inside this

196
00:06:56,480 --> 00:06:59,450
parenthesis, you end up with a matrix like this.

197
00:06:59,460 --> 00:07:01,790
Zero, one, one, one

198
00:07:01,800 --> 00:07:04,500
and so on, one until the bottom.

199
00:07:04,510 --> 00:07:05,620
So this thing over here is

200
00:07:05,630 --> 00:07:08,550
a matrix, who's upper leftmost entry is zero.

201
00:07:08,560 --> 00:07:10,180
There's ones on the diagonals and

202
00:07:10,190 --> 00:07:13,040
then the zeros everywhere else on this matrix.

203
00:07:13,050 --> 00:07:15,170
Because I am drawing this a little bit sloppy.

204
00:07:15,180 --> 00:07:17,050
But as a concrete

205
00:07:17,060 --> 00:07:19,080
example if N equals 2,

206
00:07:19,090 --> 00:07:21,830
then this matrix

207
00:07:21,840 --> 00:07:24,290
is going to be a three by three matrix.

208
00:07:24,300 --> 00:07:26,350
More generally, this matrix is

209
00:07:26,360 --> 00:07:28,260
a N plus one

210
00:07:28,270 --> 00:07:31,610
by N plus one dimensional matrix.

211
00:07:31,620 --> 00:07:33,360
So then N equals two then that

212
00:07:33,370 --> 00:07:35,970
matrix becomes something that looks like this.

213
00:07:35,980 --> 00:07:37,630
Zero, and then ones

214
00:07:37,640 --> 00:07:39,150
on the diagonals, and then

215
00:07:39,160 --> 00:07:42,380
zeros on the rest of the diagonals.

216
00:07:42,390 --> 00:07:44,610
And once again, you know, I'm not going to those this derivation.

217
00:07:44,620 --> 00:07:46,610
Which is frankly somewhat long and involved.

218
00:07:46,620 --> 00:07:47,960
But it is possible to prove

219
00:07:47,970 --> 00:07:49,930
that if you are

220
00:07:49,940 --> 00:07:51,240
using the new definition of

221
00:07:51,250 --> 00:07:54,770
J of theta, with the regularization objective.

222
00:07:54,780 --> 00:07:56,210
Then this new formula for

223
00:07:56,220 --> 00:07:57,380
theta is the one

224
00:07:57,390 --> 00:08:01,410
that will give you the global minimum of J of theta.

225
00:08:01,420 --> 00:08:02,600
So finally, I want to

226
00:08:02,610 --> 00:08:06,790
just quickly describe the issue of non-invertibility.

227
00:08:06,800 --> 00:08:08,590
This is relatively advanced material.

228
00:08:08,600 --> 00:08:09,760
So you should consider this as

229
00:08:09,770 --> 00:08:11,740
optional and feel free

230
00:08:11,750 --> 00:08:12,650
to skip it or if you

231
00:08:12,660 --> 00:08:14,310
listen to it and you know, possibly it

232
00:08:14,320 --> 00:08:16,390
don't really make sense, don't worry about it either.

233
00:08:16,400 --> 00:08:19,690
But earlier when I talked the normal equation method.

234
00:08:19,700 --> 00:08:21,790
We also had an optional video

235
00:08:21,800 --> 00:08:23,690
on the non-invertability issue.

236
00:08:23,700 --> 00:08:26,160
So this is another optional part,

237
00:08:26,170 --> 00:08:27,690
that is sort of add on

238
00:08:27,700 --> 00:08:31,600
earlier optional video on non-invertibility.

239
00:08:31,610 --> 00:08:33,840
Now considering setting where M

240
00:08:33,850 --> 00:08:35,680
the number of examples is less

241
00:08:35,690 --> 00:08:38,640
than or equal to N the number features.

242
00:08:38,650 --> 00:08:40,190
If you have fewer examples then

243
00:08:40,200 --> 00:08:42,160
features then this matrix

244
00:08:42,170 --> 00:08:44,060
X transpose X will be

245
00:08:44,070 --> 00:08:48,050
non-invertible or singular, or

246
00:08:48,060 --> 00:08:50,350
the other term

247
00:08:50,360 --> 00:08:51,520
for this is the matrix will

248
00:08:51,530 --> 00:08:53,850
be degenerate and if

249
00:08:53,860 --> 00:08:55,290
you implement this in Octave

250
00:08:55,300 --> 00:08:56,610
anyway, and you use the

251
00:08:56,620 --> 00:08:58,840
P in function to take the psuedo inverse.

252
00:08:58,850 --> 00:09:00,070
It will kind of do the

253
00:09:00,080 --> 00:09:02,230
right thing that is not

254
00:09:02,240 --> 00:09:03,550
clear that it will

255
00:09:03,560 --> 00:09:05,400
give you a very good hypothesis

256
00:09:05,410 --> 00:09:08,360
even though numerically the octave

257
00:09:08,370 --> 00:09:10,010
P in function

258
00:09:10,020 --> 00:09:11,330
will give you a result that

259
00:09:11,340 --> 00:09:13,430
kind of makes sense.

260
00:09:13,440 --> 00:09:16,260
But, if you were doing this in a different language.

261
00:09:16,270 --> 00:09:17,700
And if you were

262
00:09:17,710 --> 00:09:20,460
taking just the regular inverse

263
00:09:20,470 --> 00:09:23,230
which an octave is denoted with the function INV.

264
00:09:23,240 --> 00:09:24,320
We're trying to take the regular

265
00:09:24,330 --> 00:09:26,290
inverse of X transpose X,

266
00:09:26,300 --> 00:09:28,140
then in this setting you

267
00:09:28,150 --> 00:09:30,440
find that X transpose X

268
00:09:30,450 --> 00:09:32,780
is singular, is non-invertible and

269
00:09:32,790 --> 00:09:33,980
if you're doing this in a different

270
00:09:33,990 --> 00:09:36,220
programming language and using some

271
00:09:36,230 --> 00:09:39,830
linear algebra library try to take the inverse of this matrix.

272
00:09:39,840 --> 00:09:41,210
It just might not work because that

273
00:09:41,220 --> 00:09:44,560
matrix is non-invertible or singular.

274
00:09:44,650 --> 00:09:47,100
Fortunately, regularization also takes

275
00:09:47,110 --> 00:09:50,000
care of this for us, and concretely, so

276
00:09:50,010 --> 00:09:53,860
long as the regularization parameter is strictly greater than zero.

277
00:09:53,870 --> 00:09:55,290
It is actually possible to

278
00:09:55,300 --> 00:09:57,070
prove that this matrix X

279
00:09:57,080 --> 00:09:59,070
transpose X plus parameter time, you know,s

280
00:09:59,080 --> 00:10:00,960
this funny matrix here,

281
00:10:00,970 --> 00:10:02,460
is possible to prove that this

282
00:10:02,470 --> 00:10:03,750
matrix will not be

283
00:10:03,760 --> 00:10:07,210
singular and that this matrix will be invertible.

284
00:10:07,450 --> 00:10:09,690
So using regularization also takes

285
00:10:09,700 --> 00:10:12,570
care of any non-invertibility issues

286
00:10:12,580 --> 00:10:15,250
of the X transpose X matrix as well.

287
00:10:15,260 --> 00:10:18,860
So, you now know how to implement regularize linear regression.

288
00:10:18,870 --> 00:10:20,290
Using this, you'll be able

289
00:10:20,300 --> 00:10:22,200
to avoid overfitting, even

290
00:10:22,210 --> 00:10:25,350
if you have lots of features in a relatively small training set.

291
00:10:25,360 --> 00:10:26,970
And this should let you get

292
00:10:26,980 --> 00:10:30,050
linear regression to work much better for many problems.

293
00:10:30,060 --> 00:10:31,380
In the next video, we'll take

294
00:10:31,390 --> 00:10:35,130
this regularization idea and apply it to logistic regression.

295
00:10:35,140 --> 00:10:36,270
So that you'll be able to

296
00:10:36,280 --> 00:10:37,910
get logistic impression to avoid

297
00:10:37,920 --> 00:10:41,330
overfitting and perform much better as well.

