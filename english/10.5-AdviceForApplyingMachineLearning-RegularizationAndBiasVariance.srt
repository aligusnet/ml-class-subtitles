1
00:00:00,390 --> 00:00:02,600
You've seen how regularization can help

2
00:00:02,610 --> 00:00:04,950
prevent overfitting, but how

3
00:00:04,960 --> 00:00:06,450
does it affect the bias and

4
00:00:06,460 --> 00:00:08,620
variance of a learning algorithm?

5
00:00:08,630 --> 00:00:10,010
In this video, I like to

6
00:00:10,020 --> 00:00:11,540
go deeper into the issue

7
00:00:11,550 --> 00:00:13,510
of bias and variance, and

8
00:00:13,520 --> 00:00:15,060
talk about how it interacts

9
00:00:15,070 --> 00:00:16,060
with, and is effected by,

10
00:00:16,070 --> 00:00:19,970
the regularization of your learning algorithm.

11
00:00:22,180 --> 00:00:23,690
Suppose we fit a linear

12
00:00:23,700 --> 00:00:25,240
regression model with a very

13
00:00:25,250 --> 00:00:27,660
high order polynomial, but to

14
00:00:27,670 --> 00:00:28,780
prevent overfitting, we are

15
00:00:28,790 --> 00:00:31,550
going to use regularization as shown here.

16
00:00:31,560 --> 00:00:33,180
Suppose we're fitting a high

17
00:00:33,190 --> 00:00:35,110
order polynomial like that shown

18
00:00:35,120 --> 00:00:36,750
here, but to prevent

19
00:00:36,760 --> 00:00:37,900
overfitting, we're going to

20
00:00:37,910 --> 00:00:39,900
use regularization, like that shown

21
00:00:39,910 --> 00:00:41,870
here, so we have this regularization

22
00:00:41,880 --> 00:00:43,380
term to try to

23
00:00:43,390 --> 00:00:45,710
keep the values of the parameters small.

24
00:00:45,720 --> 00:00:47,760
And as usual, the regularization sums

25
00:00:47,770 --> 00:00:49,280
from j equals 1 to

26
00:00:49,290 --> 00:00:50,590
m rather than j equals 0

27
00:00:50,600 --> 00:00:53,730
to m.  Let's consider three cases.

28
00:00:53,740 --> 00:00:55,650
The first is the case of

29
00:00:55,660 --> 00:00:56,950
a very large value of the

30
00:00:56,960 --> 00:00:59,480
regularization parameter lambda, such

31
00:00:59,490 --> 00:01:00,780
as if lambda were

32
00:01:00,790 --> 00:01:01,770
equal to 10,000s of huge value.

33
00:01:01,780 --> 00:01:04,360
In this

34
00:01:04,370 --> 00:01:05,650
case, all of these

35
00:01:05,660 --> 00:01:07,570
parameters, theta 1, theta 2,

36
00:01:07,580 --> 00:01:08,480
theta 3 and so on will

37
00:01:08,490 --> 00:01:10,560
be heavily penalized and

38
00:01:10,570 --> 00:01:13,100
so, what ends up with most

39
00:01:13,110 --> 00:01:14,780
of these parameter values being close

40
00:01:14,790 --> 00:01:17,170
to 0 and the hypothesis will be

41
00:01:17,180 --> 00:01:18,270
roughly h or x

42
00:01:18,280 --> 00:01:20,260
just equal or approximately equal

43
00:01:20,270 --> 00:01:21,680
to theta 0, and so we

44
00:01:21,690 --> 00:01:23,790
end up a hypothesis that more

45
00:01:23,800 --> 00:01:25,360
or less looks like that. This is more or

46
00:01:25,370 --> 00:01:28,400
less a flat, constant straight line.

47
00:01:28,410 --> 00:01:30,650
And so this hypothesis has high

48
00:01:30,660 --> 00:01:32,960
bias and a value underfits this data set.

49
00:01:32,970 --> 00:01:34,830
So the horizontal straight

50
00:01:34,840 --> 00:01:35,930
line is just not a very

51
00:01:35,940 --> 00:01:38,690
good model for this data set.

52
00:01:38,700 --> 00:01:40,240
At the other extreme beam is if we have

53
00:01:40,250 --> 00:01:41,840
a very small value of

54
00:01:41,850 --> 00:01:43,700
lambda, such as if lambda

55
00:01:43,710 --> 00:01:45,710
were equal to 0.

56
00:01:45,720 --> 00:01:47,070
In that case, given that we're

57
00:01:47,080 --> 00:01:48,380
fitting a high order polynomial,

58
00:01:48,390 --> 00:01:49,930
this is a

59
00:01:49,940 --> 00:01:52,740
usual overfitting setting.

60
00:01:52,750 --> 00:01:54,180
In that case, given that we're

61
00:01:54,190 --> 00:01:56,160
fitting a high order polynomial,

62
00:01:56,170 --> 00:01:58,220
basically without regularization or with

63
00:01:58,230 --> 00:02:00,340
very minimal regularization, we end

64
00:02:00,350 --> 00:02:02,800
up with our usual high variance, overfitting

65
00:02:02,810 --> 00:02:04,620
setting, because basically if lambda is

66
00:02:04,630 --> 00:02:05,780
equal to zero, we are just

67
00:02:05,790 --> 00:02:08,430
fitting with our regularization so

68
00:02:08,440 --> 00:02:15,690
that overfits the hypothesis

69
00:02:15,700 --> 00:02:16,720
and is only if we have some

70
00:02:16,730 --> 00:02:19,210
intermediate value of lambda that is neither too large nor too small that we end up with parameters theta

71
00:02:19,220 --> 00:02:20,760
that we end up that give us a reasonable

72
00:02:20,770 --> 00:02:22,880
fit to this data.

73
00:02:22,890 --> 00:02:24,600
So how can we automatically

74
00:02:24,610 --> 00:02:26,570
choose a good value

75
00:02:26,580 --> 00:02:29,090
for the regularization parameter lambda?

76
00:02:29,100 --> 00:02:32,870
Just to reiterate, here is our model and here is our learning algorithm subjective.

77
00:02:33,670 --> 00:02:37,400
For the setting where we're using regularization, let me define

78
00:02:37,410 --> 00:02:40,400
j train of theta to be something different

79
00:02:40,410 --> 00:02:43,160
to be the optimization objective

80
00:02:43,170 --> 00:02:45,530
but without the regularization term.

81
00:02:45,540 --> 00:02:47,740
Previously, in earlier video

82
00:02:47,750 --> 00:02:49,030
when we are not using

83
00:02:49,040 --> 00:02:51,640
regularization, I define j train of theta to

84
00:02:51,650 --> 00:02:55,020
be the same as j of theta as the cost function but

85
00:02:55,030 --> 00:02:58,470
when we are using regularization with this extra lambda term

86
00:02:58,480 --> 00:03:01,070
we're going to

87
00:03:01,080 --> 00:03:02,490
define j train my training set error,

88
00:03:02,500 --> 00:03:03,820
to be just my sum of

89
00:03:03,830 --> 00:03:05,400
squared errors on the training

90
00:03:05,410 --> 00:03:07,110
set, or my average squared error

91
00:03:07,120 --> 00:03:10,930
on the training set without taking into account that regularization chart.

92
00:03:10,940 --> 00:03:12,400
And similarly, I'm then

93
00:03:12,410 --> 00:03:14,200
also going to define the

94
00:03:14,210 --> 00:03:16,260
cross-validation set error when the

95
00:03:16,270 --> 00:03:17,820
test set error, as before

96
00:03:17,830 --> 00:03:20,310
to be the average sum of squared errors

97
00:03:20,320 --> 00:03:23,230
on the cross-validation and the test sets.

98
00:03:23,240 --> 00:03:25,810
So just to summarize,

99
00:03:25,820 --> 00:03:27,480
my definitions of J train and

100
00:03:27,490 --> 00:03:28,610
J C V and J

101
00:03:28,620 --> 00:03:30,040
Test are just the

102
00:03:30,050 --> 00:03:31,400
average squared error, or one

103
00:03:31,410 --> 00:03:32,980
half of the average

104
00:03:32,990 --> 00:03:34,830
squared error on my training validation and

105
00:03:34,840 --> 00:03:38,270
test sets without the extra

106
00:03:38,310 --> 00:03:39,350
regularization chart.

107
00:03:39,360 --> 00:03:43,000
So, this is how we can automatically choose the regularization parameter lambda.

108
00:03:43,950 --> 00:03:45,710
What I usually do is may

109
00:03:45,720 --> 00:03:48,210
be have some range of values of lambda I want to try it.

110
00:03:48,220 --> 00:03:49,870
So I might be

111
00:03:49,880 --> 00:03:52,420
considering not using regularization,

112
00:03:52,430 --> 00:03:53,770
or here are a few values I might try.

113
00:03:53,780 --> 00:03:55,200
I might be considering along because

114
00:03:55,210 --> 00:03:57,970
of O1, O2 from O4 and so on.

115
00:03:57,980 --> 00:03:59,650
And you know, I usually step these

116
00:03:59,660 --> 00:04:02,300
up in multiples of

117
00:04:02,310 --> 00:04:04,950
two until some maybe larger value

118
00:04:04,960 --> 00:04:06,360
this in multiples of two you

119
00:04:06,370 --> 00:04:08,150
I actually end up with 10.24;

120
00:04:08,160 --> 00:04:10,860
it's ten exactly, but you

121
00:04:10,870 --> 00:04:12,740
know, this is close enough and

122
00:04:12,750 --> 00:04:14,490
the 35 decimal

123
00:04:14,500 --> 00:04:18,220
places won't affect your result that much.

124
00:04:19,830 --> 00:04:22,320
So, this gives me, maybe

125
00:04:22,330 --> 00:04:24,290
twelve different models, that I'm

126
00:04:24,300 --> 00:04:26,220
trying to select amongst, corresponding to

127
00:04:26,230 --> 00:04:28,200
12 different values of the

128
00:04:28,210 --> 00:04:34,260
regularization parameter lambda and

129
00:04:34,270 --> 00:04:35,590
of course, you can also go

130
00:04:35,600 --> 00:04:37,600
to values less than 0.01

131
00:04:37,610 --> 00:04:38,890
or values larger than 10,

132
00:04:38,900 --> 00:04:42,570
but I've just truncated it here for convenience.

133
00:04:46,400 --> 00:04:47,580
Given each of these 12

134
00:04:47,590 --> 00:04:48,960
models, what we can

135
00:04:48,970 --> 00:04:50,790
do is then the following:

136
00:04:50,800 --> 00:04:52,470
we take this first

137
00:04:52,480 --> 00:04:54,040
model with lambda equals 0,

138
00:04:54,050 --> 00:04:56,380
and minimize my cos

139
00:04:56,390 --> 00:04:58,770
function j of theta and this

140
00:04:58,780 --> 00:05:00,840
would give me some parameter vector theta

141
00:05:00,850 --> 00:05:02,190
and similar to the earlier video,

142
00:05:02,200 --> 00:05:05,540
let me just denote this as

143
00:05:05,550 --> 00:05:08,150
theta superscript 1.

144
00:05:08,580 --> 00:05:09,610
And then I can take my

145
00:05:09,620 --> 00:05:11,680
second model, with lambda

146
00:05:11,690 --> 00:05:13,840
set to 0.01 and

147
00:05:13,850 --> 00:05:15,930
minimize my cos function, now

148
00:05:15,940 --> 00:05:17,650
using lambda equals 0.01

149
00:05:17,660 --> 00:05:18,950
of course, to get some

150
00:05:18,960 --> 00:05:20,520
different parameter vector theta,

151
00:05:20,530 --> 00:05:21,540
we need to know that theta 2,

152
00:05:21,550 --> 00:05:22,920
and for that I

153
00:05:22,930 --> 00:05:24,400
end up with theta 3

154
00:05:24,410 --> 00:05:25,340
so that this is correct for my

155
00:05:25,350 --> 00:05:27,610
third model, and so on,

156
00:05:27,620 --> 00:05:29,440
until for for my final model

157
00:05:29,450 --> 00:05:32,040
with lambda set to 10,

158
00:05:32,050 --> 00:05:36,330
or 10.24, or I end up with this theta 12.

159
00:05:36,340 --> 00:05:38,040
Next I can take

160
00:05:38,050 --> 00:05:39,780
all of these hypotheses, all of

161
00:05:39,790 --> 00:05:42,150
these parameters, and use

162
00:05:42,160 --> 00:05:44,930
my cross-validation set to evaluate them.

163
00:05:44,940 --> 00:05:47,110
So I can look at my

164
00:05:47,120 --> 00:05:48,760
first model, my second

165
00:05:48,770 --> 00:05:49,390
model, fits with these different values

166
00:05:49,400 --> 00:06:00,430
of the regularization parameter and

167
00:06:00,440 --> 00:06:01,560
evaluate them on my cross-validation

168
00:06:01,570 --> 00:06:02,230
set - basically measure the average squared error of each of these parameter

169
00:06:02,240 --> 00:06:04,240
vectors theta on my cross-validation set.

170
00:06:04,250 --> 00:06:05,950
And I would then pick whichever one

171
00:06:05,960 --> 00:06:07,560
of these 12 models gives me

172
00:06:07,570 --> 00:06:11,040
the lowest error on the cross-validation set.

173
00:06:11,050 --> 00:06:12,060
And let's say, for the sake

174
00:06:12,070 --> 00:06:13,940
of this example, that I

175
00:06:13,950 --> 00:06:15,640
end up picking theta 5,

176
00:06:15,650 --> 00:06:18,640
the fifth order polynomial, because

177
00:06:18,650 --> 00:06:22,000
that has the Noah's cross-validation error.

178
00:06:22,010 --> 00:06:24,380
Having done that, finally, what

179
00:06:24,390 --> 00:06:25,480
I would do if I want

180
00:06:25,490 --> 00:06:27,360
to report a test set error

181
00:06:27,370 --> 00:06:28,990
is to take the parameter theta

182
00:06:29,000 --> 00:06:31,030
5 that I've

183
00:06:31,040 --> 00:06:32,660
selected and look at

184
00:06:32,670 --> 00:06:34,830
how well it does on my test set.

185
00:06:34,840 --> 00:06:36,470
And once again here is as

186
00:06:36,480 --> 00:06:38,220
if we fit this parameter

187
00:06:38,230 --> 00:06:41,260
theta to my cross-validation

188
00:06:41,270 --> 00:06:42,650
set, which is why I

189
00:06:42,660 --> 00:06:44,410
am saving aside a separate

190
00:06:44,420 --> 00:06:45,850
test set that I

191
00:06:45,860 --> 00:06:47,340
am going to use to get

192
00:06:47,350 --> 00:06:48,720
a better estimate of how

193
00:06:48,730 --> 00:06:50,180
well my a parameter vector

194
00:06:50,190 --> 00:06:53,190
theta will generalize to previously unseen examples.

195
00:06:54,120 --> 00:06:56,250
So that's model selection applied

196
00:06:56,260 --> 00:06:59,250
to selecting the regularization parameter

197
00:06:59,260 --> 00:07:00,480
lambda. The last thing

198
00:07:00,490 --> 00:07:01,760
I'd like to do in this

199
00:07:01,770 --> 00:07:02,960
video, is get a

200
00:07:02,970 --> 00:07:05,640
better understanding of how

201
00:07:05,650 --> 00:07:07,670
cross-validation and training error

202
00:07:07,680 --> 00:07:10,520
vary as we as

203
00:07:10,530 --> 00:07:13,450
we vary the regularization parameter lambda.

204
00:07:13,460 --> 00:07:15,350
And so just a reminder, that

205
00:07:15,360 --> 00:07:16,830
was our original cosine function j of

206
00:07:16,840 --> 00:07:18,390
theta, but for this

207
00:07:18,400 --> 00:07:20,440
purpose we're going to define

208
00:07:20,450 --> 00:07:22,230
training error without using

209
00:07:22,240 --> 00:07:24,850
the regularization parameter, and cross-validation

210
00:07:24,860 --> 00:07:26,350
error without using the

211
00:07:26,360 --> 00:07:29,200
regularization parameter and what I'd like

212
00:07:29,210 --> 00:07:31,740
to do is plot this J train

213
00:07:31,750 --> 00:07:34,690
and plot this Jcv, meaning just

214
00:07:34,700 --> 00:07:35,910
how well does my

215
00:07:35,920 --> 00:07:38,570
hypothesis do for on

216
00:07:38,580 --> 00:07:39,910
the training set and how well

217
00:07:39,920 --> 00:07:41,330
does my hypothesis do on the

218
00:07:41,340 --> 00:07:43,310
cross-validation set as I

219
00:07:43,320 --> 00:07:45,690
vary my regularization parameter

220
00:07:45,700 --> 00:07:49,310
lambda so as

221
00:07:49,320 --> 00:07:52,060
we saw earlier, if lambda

222
00:07:52,070 --> 00:07:53,910
is small, then we're

223
00:07:53,920 --> 00:07:56,760
not using much regularization and

224
00:07:56,770 --> 00:07:59,940
we run a larger risk of overfitting.

225
00:07:59,950 --> 00:08:01,920
Where as if lambda is

226
00:08:01,930 --> 00:08:03,300
large, that is if we

227
00:08:03,310 --> 00:08:05,180
were on the right part

228
00:08:05,190 --> 00:08:07,680
of this horizontal axis, then

229
00:08:07,690 --> 00:08:09,550
with a large value of lambda

230
00:08:09,560 --> 00:08:13,030
we run the high risk of having a bias problem.

231
00:08:13,040 --> 00:08:15,270
So if you plot J train

232
00:08:15,280 --> 00:08:16,970
and Jcv, what you

233
00:08:16,980 --> 00:08:19,090
find is that for small

234
00:08:19,100 --> 00:08:22,000
values of lambda you can

235
00:08:22,010 --> 00:08:23,630
fit the training set relatively

236
00:08:23,640 --> 00:08:25,590
well because you're not regularizing.

237
00:08:25,600 --> 00:08:26,980
So, for small values of

238
00:08:26,990 --> 00:08:28,950
lambda, the regularization term basically

239
00:08:28,960 --> 00:08:30,410
goes away and you're just

240
00:08:30,420 --> 00:08:32,860
minimizing pretty much your squared error.

241
00:08:32,870 --> 00:08:34,620
So when lambda is small, you

242
00:08:34,630 --> 00:08:36,160
end up with a small value

243
00:08:36,170 --> 00:08:37,890
for J train, whereas if

244
00:08:37,900 --> 00:08:39,730
lambda is large, then you

245
00:08:39,740 --> 00:08:42,630
have a high bias problem and you might not fit your training set so well.

246
00:08:42,640 --> 00:08:44,540
So you end up with a value up there.

247
00:08:44,550 --> 00:08:48,920
So, J train of

248
00:08:48,930 --> 00:08:50,310
theta will tend to

249
00:08:50,320 --> 00:08:53,040
increase when lambda increases

250
00:08:53,050 --> 00:08:54,910
because a large value of

251
00:08:54,920 --> 00:08:56,390
lambda corresponds a high bias

252
00:08:56,400 --> 00:08:57,580
where you might not even fit your

253
00:08:57,590 --> 00:08:59,280
training set well, whereas a

254
00:08:59,290 --> 00:09:01,640
small value of lambda corresponds to,

255
00:09:01,650 --> 00:09:03,840
if you can you know freely

256
00:09:03,850 --> 00:09:06,910
fit to very high degree polynomials, your data, let's say.

257
00:09:06,920 --> 00:09:12,070
As for the cross-validation error, we end up with a figure like this.

258
00:09:12,080 --> 00:09:13,920
Where, over here on

259
00:09:13,930 --> 00:09:15,520
the right, if we

260
00:09:15,530 --> 00:09:17,430
have a large value of lambda,

261
00:09:17,440 --> 00:09:19,890
we may end up underfitting.

262
00:09:19,900 --> 00:09:22,780
And so, this is the bias regime

263
00:09:22,950 --> 00:09:26,020
whereas and cross

264
00:09:26,030 --> 00:09:27,910
validation error will be

265
00:09:27,920 --> 00:09:29,240
high and let me just leave

266
00:09:29,250 --> 00:09:32,260
all that. So, that's Jcv of theta because with

267
00:09:32,270 --> 00:09:34,420
high bias we won't be fitting.

268
00:09:34,430 --> 00:09:38,040
We won't be doing well on the cross-validation set.

269
00:09:38,050 --> 00:09:42,110
Whereas here on the left, this is the high-variance regime.

270
00:09:42,120 --> 00:09:44,010
Where if we have two smaller

271
00:09:44,020 --> 00:09:46,060
value of then we

272
00:09:46,070 --> 00:09:47,860
may be overfitting the data

273
00:09:47,870 --> 00:09:49,220
and so by over fitting the

274
00:09:49,230 --> 00:09:51,700
data then it a cross validation error

275
00:09:51,710 --> 00:09:53,690
will also be high.

276
00:09:53,700 --> 00:09:56,610
And so, this is what the

277
00:09:56,620 --> 00:09:58,500
cross-validation error and what

278
00:09:58,510 --> 00:10:00,120
the training error may look

279
00:10:00,130 --> 00:10:01,810
like on a training set

280
00:10:01,820 --> 00:10:04,940
as we vary the parameter

281
00:10:04,950 --> 00:10:07,100
lambda, as we vary the regularization parameter lambda.

282
00:10:07,110 --> 00:10:08,420
And so, once again, it will

283
00:10:08,430 --> 00:10:10,780
often be some intermediate value

284
00:10:10,790 --> 00:10:13,710
of lambda that you know, subsequent just right

285
00:10:13,720 --> 00:10:15,110
or that works best in

286
00:10:15,120 --> 00:10:16,760
terms of having a small

287
00:10:16,770 --> 00:10:19,910
cross-validation error or a small test set error.

288
00:10:19,920 --> 00:10:21,290
And whereas the curves I've drawn

289
00:10:21,300 --> 00:10:24,640
here are somewhat cartoonish and somewhat idealized.

290
00:10:24,650 --> 00:10:26,200
So on a real data set

291
00:10:26,210 --> 00:10:27,500
the pros you get may

292
00:10:27,510 --> 00:10:28,680
end up looking a little bit more

293
00:10:28,690 --> 00:10:31,530
messy and just a little bit more noisy than this.

294
00:10:31,540 --> 00:10:33,170
For some data sets you will

295
00:10:33,180 --> 00:10:34,730
really see these poor

296
00:10:34,740 --> 00:10:36,440
source of trends and

297
00:10:36,450 --> 00:10:37,890
by looking at the plot

298
00:10:37,900 --> 00:10:39,810
of the whole or cross validation

299
00:10:39,820 --> 00:10:41,590
error, you can either

300
00:10:41,600 --> 00:10:43,670
manually, automatically try to

301
00:10:43,680 --> 00:10:45,540
select a point that minimizes

302
00:10:45,550 --> 00:10:48,870
the cross-validation error and

303
00:10:48,880 --> 00:10:51,270
select the value of lambda corresponding

304
00:10:51,280 --> 00:10:53,550
to low cross-validation error.

305
00:10:53,560 --> 00:10:54,910
When I'm trying to pick the

306
00:10:54,920 --> 00:10:57,190
regularization parameter lambda

307
00:10:57,200 --> 00:10:59,410
for a learning algorithm, often I

308
00:10:59,420 --> 00:11:00,790
find that plotting a figure

309
00:11:00,800 --> 00:11:02,740
like this one showed here, helps

310
00:11:02,750 --> 00:11:04,770
me understand better what's going

311
00:11:04,780 --> 00:11:06,870
on and helps me verify that

312
00:11:06,880 --> 00:11:08,310
I am indeed picking a good

313
00:11:08,320 --> 00:11:10,510
value for the regularization parameter

314
00:11:10,520 --> 00:11:12,510
lambda. So hopefully that

315
00:11:12,520 --> 00:11:15,640
gives you more insight into regularization

316
00:11:15,650 --> 00:11:17,390
and it's effects on the bias

317
00:11:17,400 --> 00:11:19,960
and variance of the learning algorithm.

318
00:11:19,970 --> 00:11:21,660
By know you've seen bias and

319
00:11:21,670 --> 00:11:24,170
variance from a lot of different perspectives.

320
00:11:24,180 --> 00:11:25,690
And what I'd like to do

321
00:11:25,700 --> 00:11:27,220
in the next video is take

322
00:11:27,230 --> 00:11:28,270
a lot of the insights

323
00:11:28,280 --> 00:11:30,310
that we've gone through and build

324
00:11:30,320 --> 00:11:31,910
on them to put together

325
00:11:31,920 --> 00:11:34,040
a diagnostic that's called learning

326
00:11:34,050 --> 00:11:35,140
curves, which is a

327
00:11:35,150 --> 00:11:36,710
tool that I often use

328
00:11:36,720 --> 00:11:38,180
to try to diagnose if a

329
00:11:38,190 --> 00:11:40,030
learning algorithm may be suffering

330
00:11:40,040 --> 00:11:41,550
from a bias problem or a

331
00:11:41,560 --> 00:11:44,450
variance problem or a little bit of both.

