1
00:00:00,270 --> 00:00:01,440
Neural Networks are one of

2
00:00:01,450 --> 00:00:04,300
the most powerful learning algorithms that we have today.

3
00:00:04,310 --> 00:00:05,580
In this, and in the

4
00:00:05,590 --> 00:00:06,750
next few videos, I'd like to

5
00:00:06,760 --> 00:00:08,370
start talking about a learning

6
00:00:08,380 --> 00:00:10,620
algorithm for fitting the parameters

7
00:00:10,630 --> 00:00:13,450
of the neural network given the training set.

8
00:00:13,460 --> 00:00:15,010
As for the discussion of most

9
00:00:15,020 --> 00:00:16,440
of the learning algorithms, we're going

10
00:00:16,450 --> 00:00:17,950
to begin by talking about the

11
00:00:17,960 --> 00:00:21,160
cost function for fitting the parameters of the network.

12
00:00:21,170 --> 00:00:23,260
I'm going to focus

13
00:00:23,270 --> 00:00:25,050
on the application of neural

14
00:00:25,060 --> 00:00:27,650
networks to classification problems.

15
00:00:27,660 --> 00:00:29,110
So, suppose we have a

16
00:00:29,120 --> 00:00:31,520
network like that shown on the left.

17
00:00:31,530 --> 00:00:32,700
And suppose we have a training

18
00:00:32,710 --> 00:00:33,970
set like this of this of

19
00:00:33,980 --> 00:00:37,910
Xi, Yi pairs of m training examples.

20
00:00:37,920 --> 00:00:39,440
I'm going to use upper case

21
00:00:39,450 --> 00:00:40,780
L to denote the

22
00:00:40,790 --> 00:00:43,320
total number of layers in this network.

23
00:00:43,330 --> 00:00:44,800
So, for the network shown

24
00:00:44,810 --> 00:00:46,360
on the left, we would have

25
00:00:46,370 --> 00:00:48,010
capital L equals 4.

26
00:00:48,020 --> 00:00:49,170
And, I'm going to use

27
00:00:49,180 --> 00:00:51,250
s subscript L, to denote

28
00:00:51,260 --> 00:00:52,720
the number of units, that is

29
00:00:52,730 --> 00:00:54,760
a number of neurons, not counting

30
00:00:54,770 --> 00:00:57,890
the bias unit in layer L of the network.

31
00:00:57,900 --> 00:00:59,570
So, for example, we would

32
00:00:59,580 --> 00:01:01,360
have a S1 which

33
00:01:01,370 --> 00:01:04,130
is the input layer equals S3 unit,

34
00:01:04,140 --> 00:01:06,890
S2 in my example is five units.

35
00:01:06,900 --> 00:01:09,930
And the output layer S4.

36
00:01:09,940 --> 00:01:12,980
Which is also equals SL, because capital L is equal to four.

37
00:01:12,990 --> 00:01:14,440
The output layer in my

38
00:01:14,450 --> 00:01:17,620
example in the left has four units.

39
00:01:17,630 --> 00:01:20,420
We're going to consider two types of classification problems.

40
00:01:20,430 --> 00:01:22,960
The first is binary classification,

41
00:01:22,970 --> 00:01:26,230
where the labels y are either zero or one.

42
00:01:26,240 --> 00:01:29,130
In this case, we would have one output unit.

43
00:01:29,140 --> 00:01:30,500
So, this neural network on top

44
00:01:30,510 --> 00:01:32,560
has four output units, but if

45
00:01:32,570 --> 00:01:34,110
we had binary classification, we would

46
00:01:34,120 --> 00:01:36,710
have only one output unit

47
00:01:36,720 --> 00:01:39,860
that computes h of x.

48
00:01:40,310 --> 00:01:41,620
And the outputs of the

49
00:01:41,630 --> 00:01:43,130
neural network would be h

50
00:01:43,140 --> 00:01:46,890
of x is going to be a real number.

51
00:01:46,900 --> 00:01:48,350
And in this case the number

52
00:01:48,360 --> 00:01:50,470
of output units, SL, where

53
00:01:50,480 --> 00:01:52,290
L is again the index

54
00:01:52,300 --> 00:01:54,230
of the final layer because that's

55
00:01:54,240 --> 00:01:56,560
the number of layers we have in the network.

56
00:01:56,570 --> 00:01:58,100
So the number of units we

57
00:01:58,110 --> 00:02:01,030
have in the output layer is going to be equal to one.

58
00:02:01,040 --> 00:02:02,940
In this case, to simplify notation

59
00:02:02,950 --> 00:02:05,450
later, I'm also going to set k equals 1.

60
00:02:05,460 --> 00:02:06,760
So, you can think of k as

61
00:02:06,770 --> 00:02:08,690
also denoting the number

62
00:02:08,700 --> 00:02:11,400
of units in the output layer.

63
00:02:11,410 --> 00:02:13,270
The second type of classification problem

64
00:02:13,280 --> 00:02:15,770
we'll consider will be multiclass classification

65
00:02:15,780 --> 00:02:19,150
problem where we may have k distinct classes.

66
00:02:19,160 --> 00:02:21,060
So, our early example, I

67
00:02:21,070 --> 00:02:23,070
had this representation for y

68
00:02:23,080 --> 00:02:25,150
if we have four classes and

69
00:02:25,160 --> 00:02:27,330
in this case, we would have caps lock K

70
00:02:27,340 --> 00:02:30,340
output units and our hypotheses

71
00:02:30,350 --> 00:02:34,970
will output vectors that are K dimensional.

72
00:02:34,980 --> 00:02:36,750
And the number of output units

73
00:02:36,760 --> 00:02:38,990
will be equal to K.

74
00:02:39,000 --> 00:02:40,360
And usually we will have

75
00:02:40,370 --> 00:02:41,810
K greater than or equal

76
00:02:41,820 --> 00:02:43,970
to three in this case, because

77
00:02:43,980 --> 00:02:45,700
if we had two classes then,

78
00:02:45,710 --> 00:02:46,680
you know, we don't need to

79
00:02:46,690 --> 00:02:48,710
use the one versus all method.

80
00:02:48,720 --> 00:02:49,960
We need to use the one versus

81
00:02:49,970 --> 00:02:51,100
all method only if we

82
00:02:51,110 --> 00:02:52,730
have K greater than or

83
00:02:52,740 --> 00:02:54,460
equal to three classes so we

84
00:02:54,470 --> 00:02:56,170
only have two classes we will

85
00:02:56,180 --> 00:02:58,240
need to use only one output unit.

86
00:02:58,250 --> 00:03:02,370
Now, let's define the cost function for our cost function for our neural network.

87
00:03:03,880 --> 00:03:05,230
The cost function we use for

88
00:03:05,240 --> 00:03:06,670
the neural network is going to

89
00:03:06,680 --> 00:03:08,350
be a generalization of the

90
00:03:08,360 --> 00:03:09,500
one that we use for logistic

91
00:03:09,510 --> 00:03:12,090
regression. 

For logistic regression,

92
00:03:12,100 --> 00:03:13,500
we used to minimize the

93
00:03:13,510 --> 00:03:15,260
cost function j of theta

94
00:03:15,270 --> 00:03:16,760
that was minus 1 over

95
00:03:16,770 --> 00:03:18,710
m of this cost function

96
00:03:18,720 --> 00:03:21,290
and then plus this extra regularization

97
00:03:21,300 --> 00:03:22,840
term here, where this was

98
00:03:22,850 --> 00:03:24,690
a sum from j equals

99
00:03:24,700 --> 00:03:26,260
1 through n, because we

100
00:03:26,270 --> 00:03:31,020
did not regularize the bias term theta zero.

101
00:03:31,030 --> 00:03:32,900
For a neural network our cost

102
00:03:32,910 --> 00:03:35,640
function is going to be a generalization of this.

103
00:03:35,650 --> 00:03:37,520
Where instead of having basically

104
00:03:37,530 --> 00:03:39,640
just one logistic regression output

105
00:03:39,650 --> 00:03:42,580
unit, we may instead have K of them.

106
00:03:42,590 --> 00:03:44,760
So here's our cost function.

107
00:03:44,770 --> 00:03:46,710
Neural network now outputs vectors

108
00:03:46,720 --> 00:03:48,160
in RK where K might

109
00:03:48,170 --> 00:03:49,190
be equal to 1 if we

110
00:03:49,200 --> 00:03:51,370
have the binary classification problem.

111
00:03:51,380 --> 00:03:53,290
I'm going to use this notation,

112
00:03:53,300 --> 00:03:57,430
h of x subscript i, to denote the ith output.

113
00:03:57,440 --> 00:04:00,830
That is h of x is a K dimensional vector.

114
00:04:00,840 --> 00:04:02,950
And so this subscript i just

115
00:04:02,960 --> 00:04:05,190
selects out the ith element

116
00:04:05,200 --> 00:04:08,890
of the vector that is output by my neural network.

117
00:04:08,900 --> 00:04:10,170
My cost function, j of

118
00:04:10,180 --> 00:04:11,750
theta is now going

119
00:04:11,760 --> 00:04:13,930
to be the following is minus one

120
00:04:13,940 --> 00:04:15,410
over m of a sum

121
00:04:15,420 --> 00:04:16,950
of a similar term to what

122
00:04:16,960 --> 00:04:19,290
we have in logistic regression.

Except that

123
00:04:19,300 --> 00:04:21,010
we have this sum from K

124
00:04:21,020 --> 00:04:22,590
equals one through K.  The

125
00:04:22,600 --> 00:04:23,710
summation is basically a

126
00:04:23,720 --> 00:04:26,050
sum over my K output unit.

127
00:04:26,060 --> 00:04:29,390
So, if I have four upper units.

128
00:04:29,400 --> 00:04:30,840
That is the final layer of my

129
00:04:30,850 --> 00:04:32,850
neural network has four output

130
00:04:32,860 --> 00:04:34,690
units then this sum

131
00:04:34,700 --> 00:04:35,890
from, this is a sum from

132
00:04:35,900 --> 00:04:38,040
K equals one through four

133
00:04:38,050 --> 00:04:42,050
of basically the logistic regression algorithms

134
00:04:42,070 --> 00:04:43,740
cost function but summing

135
00:04:43,750 --> 00:04:45,880
that cost function over each

136
00:04:45,890 --> 00:04:47,790
of my four output units in turn.

137
00:04:47,800 --> 00:04:49,370
And so, you notice

138
00:04:49,380 --> 00:04:51,390
in particular that this applies

139
00:04:51,400 --> 00:04:53,730
to YK, HK, because

140
00:04:53,740 --> 00:04:55,490
we're basically taking the K

141
00:04:55,500 --> 00:04:57,770
upper unit and comparing that

142
00:04:57,780 --> 00:04:59,800
to the value of YK, which

143
00:04:59,810 --> 00:05:02,200
is you know, which is

144
00:05:02,210 --> 00:05:03,730
that one of those vectors

145
00:05:03,740 --> 00:05:06,270
to say what cause it should be.

146
00:05:06,280 --> 00:05:08,350
And finally, the second term

147
00:05:08,360 --> 00:05:10,430
here is the regularization

148
00:05:10,440 --> 00:05:14,070
term similar to what we had for logistic regression.

149
00:05:14,080 --> 00:05:15,840
This summation terms looks really

150
00:05:15,850 --> 00:05:17,830
complicated and always doing

151
00:05:17,840 --> 00:05:19,940
is a summing over these terms,

152
00:05:19,950 --> 00:05:21,850
theta j i l for

153
00:05:21,860 --> 00:05:23,400
all values of i j

154
00:05:23,410 --> 00:05:25,000
and l. 

Except that we

155
00:05:25,010 --> 00:05:26,700
don't sum over the terms

156
00:05:26,710 --> 00:05:28,890
corresponding to these bias values

157
00:05:28,900 --> 00:05:30,890
like we had for logistic progression.

158
00:05:30,900 --> 00:05:32,230
Concretely, we don't sum

159
00:05:32,240 --> 00:05:34,290
over the terms corresponding

160
00:05:34,300 --> 00:05:36,770
to where i is equal to zero.

161
00:05:36,780 --> 00:05:38,910
So, that is because

162
00:05:38,920 --> 00:05:40,580
when we are computing the activation

163
00:05:40,590 --> 00:05:42,270
of the neuron, we have terms

164
00:05:42,280 --> 00:05:43,800
like these, you know theta, i0

165
00:05:43,810 --> 00:05:48,150
plus theta, i1,

166
00:05:48,160 --> 00:05:50,510
x1 plus, and

167
00:05:50,520 --> 00:05:52,010
so on, where I guess

168
00:05:52,020 --> 00:05:53,480
we could have a 2 there

169
00:05:53,490 --> 00:05:55,240
if this is the first hidden layer,

170
00:05:55,250 --> 00:05:57,220
and so the values with

171
00:05:57,230 --> 00:05:58,720
the 0 there at that corresponds to

172
00:05:58,730 --> 00:06:00,250
something that multiplies into an

173
00:06:00,260 --> 00:06:02,200
x0 or an a0 and

174
00:06:02,210 --> 00:06:03,110
so, this is kind of like

175
00:06:03,120 --> 00:06:04,970
a bias unit and by

176
00:06:04,980 --> 00:06:06,120
analogy to what we were

177
00:06:06,130 --> 00:06:07,880
doing for logistic progression, we won't

178
00:06:07,890 --> 00:06:09,150
sum over those terms in

179
00:06:09,160 --> 00:06:11,150
our regularization term because we

180
00:06:11,160 --> 00:06:13,660
don't want to regularize them and

181
00:06:13,670 --> 00:06:15,350
string the values 0.

182
00:06:15,360 --> 00:06:17,660
But this is just one possible convention

183
00:06:17,670 --> 00:06:18,830
and even if you were

184
00:06:18,840 --> 00:06:21,190
to sum over, you know, i equals 0 up

185
00:06:21,200 --> 00:06:23,150
to SL, it will work

186
00:06:23,160 --> 00:06:25,520
about the same and it doesn't make a big difference.

187
00:06:25,530 --> 00:06:27,490
But maybe this convention

188
00:06:27,500 --> 00:06:29,060
of not regularizing the bias

189
00:06:29,070 --> 00:06:31,820
term is just slightly more common.

190
00:06:32,960 --> 00:06:34,680
So, that's the cost function

191
00:06:34,690 --> 00:06:36,780
we're going to use to fill on your own network.

192
00:06:36,790 --> 00:06:38,470
In the next video, we'll start

193
00:06:38,480 --> 00:06:40,560
to talk about an algorithm for

194
00:06:40,570 --> 00:06:44,030
trying to optimize the cost function.

