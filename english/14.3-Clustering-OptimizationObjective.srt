1
00:00:00,090 --> 00:00:01,680
Most of the supervised learning algorithms

2
00:00:01,690 --> 00:00:03,120
we've seen, things like linear

3
00:00:03,130 --> 00:00:04,920
regression, logistic regression and

4
00:00:04,930 --> 00:00:06,290
so on. All of those

5
00:00:06,300 --> 00:00:08,660
algorithms have an optimization objective

6
00:00:08,670 --> 00:00:11,910
or some cost function that the algorithm was trying to minimize.

7
00:00:11,920 --> 00:00:13,760
It turns out that K-means also

8
00:00:13,770 --> 00:00:15,860
has an optimization objective or

9
00:00:15,870 --> 00:00:19,620
a cost function that is trying to minimize.

10
00:00:19,630 --> 00:00:20,220
And in this video, I'd like to tell

11
00:00:20,230 --> 00:00:23,720
you what that optimization objective is.

12
00:00:23,730 --> 00:00:24,740
And the reason I want to do so

13
00:00:24,750 --> 00:00:28,010
is because this will be useful to us for two purposes.

14
00:00:28,020 --> 00:00:29,470
First, knowing what is the

15
00:00:29,480 --> 00:00:31,140
optimization objective of K-means

16
00:00:31,150 --> 00:00:32,680
will help us to

17
00:00:32,690 --> 00:00:34,060
debug the learning algorithm and

18
00:00:34,070 --> 00:00:35,290
just make sure that K-means is

19
00:00:35,300 --> 00:00:37,600
running correctly, and second,

20
00:00:37,610 --> 00:00:39,520
and perhaps even more importantly, in

21
00:00:39,530 --> 00:00:41,480
a later video we'll talk

22
00:00:41,490 --> 00:00:42,720
about how we can use this to

23
00:00:42,730 --> 00:00:44,060
help K-means find better clusters

24
00:00:44,070 --> 00:00:46,400
and avoid local optima, but we'll do that in a later video that follows this one.

25
00:00:46,410 --> 00:00:48,830
Just as a quick reminder, while K-means is

26
00:00:49,680 --> 00:00:54,370
running we're going to be

27
00:00:54,450 --> 00:00:56,420
keeping track of two sets of variables.

28
00:00:56,430 --> 00:00:58,690
First is the CI's and

29
00:00:58,700 --> 00:01:00,180
that keeps track of the

30
00:01:00,190 --> 00:01:02,720
index or the number of the cluster

31
00:01:02,730 --> 00:01:05,220
to which an example x(i) is currently assigned.

32
00:01:05,230 --> 00:01:06,530
And then, the other set of variables

33
00:01:06,540 --> 00:01:08,110
we use as Mu subscript

34
00:01:08,120 --> 00:01:10,130
K, which is the location

35
00:01:10,140 --> 00:01:12,370
of cluster centroid K. And,

36
00:01:12,380 --> 00:01:14,020
again, for K-means

37
00:01:14,030 --> 00:01:17,880
we use capital K to denote the total number of clusters.

38
00:01:17,890 --> 00:01:20,000
And here lower case K,

39
00:01:20,010 --> 00:01:21,030
you know, is going to be an

40
00:01:21,040 --> 00:01:22,960
index into the cluster

41
00:01:22,970 --> 00:01:24,020
centroids, and so lower

42
00:01:24,030 --> 00:01:25,130
case k is going to be

43
00:01:25,140 --> 00:01:26,590
a number between 1 and

44
00:01:26,600 --> 00:01:29,830
capital K. Now, here's

45
00:01:29,840 --> 00:01:31,260
one more bit of notation which

46
00:01:31,270 --> 00:01:32,620
is going to use Mu

47
00:01:32,630 --> 00:01:34,960
subscript c(i) to denote

48
00:01:34,970 --> 00:01:36,770
the cluster centroid of the

49
00:01:36,780 --> 00:01:38,870
cluster to which example x(i)

50
00:01:38,880 --> 00:01:40,700
has been assigned and

51
00:01:40,710 --> 00:01:42,440
to explain that notation

52
00:01:42,450 --> 00:01:43,650
a little bit more, let's

53
00:01:43,660 --> 00:01:45,730
say that x(i) has been

54
00:01:45,740 --> 00:01:48,870
assigned to cluster number five.

55
00:01:48,880 --> 00:01:50,840
What that means is that c(i),

56
00:01:50,850 --> 00:01:53,120
that is the index of x(i),

57
00:01:53,130 --> 00:01:54,410
that that is equal to 5.

58
00:01:54,420 --> 00:01:57,790
Right? Because you know, having c(i) equals 5,

59
00:01:57,800 --> 00:02:00,490
that's what it means for the

60
00:02:00,500 --> 00:02:01,900
example x(i) to be

61
00:02:01,910 --> 00:02:03,500
assigned to cluster number 5.

62
00:02:03,510 --> 00:02:06,280
And so Mu subscript

63
00:02:06,290 --> 00:02:08,090
c(i) is going to

64
00:02:08,100 --> 00:02:10,070
be equal to Mu subscript

65
00:02:10,080 --> 00:02:13,690
5 because c(i) is equal

66
00:02:13,700 --> 00:02:15,090
to 5.

67
00:02:15,100 --> 00:02:16,650
This Mu substitute c(i) is the

68
00:02:16,660 --> 00:02:18,720
cluster centroid of cluster number

69
00:02:18,730 --> 00:02:20,110
5, which is the cluster

70
00:02:20,120 --> 00:02:23,460
to which my example x(i) has been assigned.

71
00:02:23,470 --> 00:02:24,950
With this notation, we're now

72
00:02:24,960 --> 00:02:26,190
ready to write out what

73
00:02:26,200 --> 00:02:28,280
is the optimization objective of

74
00:02:28,290 --> 00:02:30,750
the K Mu clustering algorithm.

75
00:02:30,760 --> 00:02:31,320
And here it is.

76
00:02:31,330 --> 00:02:33,030
The cost function that K-means

77
00:02:33,040 --> 00:02:34,560
is minimizing is the

78
00:02:34,570 --> 00:02:35,870
function J of all of

79
00:02:35,880 --> 00:02:37,880
these parameters c1 through

80
00:02:37,890 --> 00:02:39,780
cM, Mu1 through MuK, that

81
00:02:39,790 --> 00:02:42,090
K-means is varying as the algorithm runs.

82
00:02:42,100 --> 00:02:44,150
And the optimization objective is shown

83
00:02:44,160 --> 00:02:45,600
on the right, is the average of

84
00:02:45,610 --> 00:02:46,610
one over M of the sum

85
00:02:46,620 --> 00:02:50,230
of i equals one through M of this term here

86
00:02:50,400 --> 00:02:52,860
that I've just drawn the red box around.

87
00:02:52,870 --> 00:02:55,150
The squared distance between

88
00:02:55,160 --> 00:02:57,680
each example x(i) and the

89
00:02:57,690 --> 00:02:59,120
location of the cluster

90
00:02:59,130 --> 00:03:01,310
centroid to which x(i)

91
00:03:01,320 --> 00:03:03,230
has been assigned.

92
00:03:03,240 --> 00:03:06,230
So let me just draw this in, let me explain this.

93
00:03:06,240 --> 00:03:08,180
Here is the location of training

94
00:03:08,190 --> 00:03:10,400
example x(i), and here's the location

95
00:03:10,410 --> 00:03:11,960
of the cluster centroid to which

96
00:03:11,970 --> 00:03:14,550
example x(i) has been assigned.

97
00:03:14,560 --> 00:03:17,410
So to explain this in pictures, if here is X1, X2.

98
00:03:17,420 --> 00:03:19,750
And if a point

99
00:03:19,760 --> 00:03:22,070
here, is my example

100
00:03:22,080 --> 00:03:23,100
x(i), so if that

101
00:03:23,110 --> 00:03:25,850
is equal to my example x(i),

102
00:03:25,860 --> 00:03:27,230
and if x(i) has been assigned

103
00:03:27,240 --> 00:03:28,330
to some cluster centroid, and

104
00:03:28,340 --> 00:03:30,620
I'll denote my cluster centroid with a cross.

105
00:03:30,630 --> 00:03:32,290
So if that's the location of,

106
00:03:32,300 --> 00:03:34,360
you know, Mu 5, let's

107
00:03:34,370 --> 00:03:35,840
say, if x(i) has been

108
00:03:35,850 --> 00:03:38,800
assigned to cluster centroid 5 in my example up there.

109
00:03:38,810 --> 00:03:40,930
Then, the squared distance, that's

110
00:03:40,940 --> 00:03:43,340
the squared of the distance

111
00:03:43,810 --> 00:03:46,210
between the point x(i) and this

112
00:03:46,220 --> 00:03:49,560
cluster centroid, to which x(i) has been assigned.

113
00:03:49,570 --> 00:03:51,060
And what K-means can be shown

114
00:03:51,070 --> 00:03:52,670
to be doing is that, it

115
00:03:52,680 --> 00:03:55,260
is trying to find parameters c(i)

116
00:03:55,270 --> 00:03:57,560
and Mu(i), try to

117
00:03:57,570 --> 00:03:58,950
find cMU to try to

118
00:03:58,960 --> 00:04:01,430
minimize this cost function J.

119
00:04:01,440 --> 00:04:03,670
This cost function is sometimes

120
00:04:03,680 --> 00:04:07,050
also called the distortion cost

121
00:04:07,060 --> 00:04:10,230
function or the distortion of

122
00:04:10,240 --> 00:04:12,780
the K-means algorithm.

123
00:04:12,790 --> 00:04:13,620
And, just to provide a little bit more

124
00:04:13,630 --> 00:04:15,810
detail, here's the K-means algorithm,

125
00:04:15,820 --> 00:04:16,600
Here's exactly the algorithm as we have it, in the real

126
00:04:16,610 --> 00:04:18,940
form the earlier slide.

127
00:04:18,950 --> 00:04:21,020
And what this first step

128
00:04:21,030 --> 00:04:23,820
of this algorithm is, this was

129
00:04:23,830 --> 00:04:27,410
the cluster assignment step

130
00:04:27,920 --> 00:04:30,020
where we assign each

131
00:04:30,030 --> 00:04:33,000
point to the cluster centroid, and

132
00:04:33,010 --> 00:04:35,040
it's possible to show mathematically that

133
00:04:35,050 --> 00:04:36,440
what the cluster assignment step

134
00:04:36,450 --> 00:04:40,060
is doing is exactly minimizing

135
00:04:40,770 --> 00:04:43,410
J with respect

136
00:04:43,420 --> 00:04:46,370
to the variables, C1, C2

137
00:04:46,380 --> 00:04:48,160
and so on, up

138
00:04:48,170 --> 00:04:52,470
to C(m), while holding the

139
00:04:52,480 --> 00:04:54,710
closest centroids, MU1 up to

140
00:04:54,720 --> 00:04:58,500
MUK fixed.

141
00:04:58,580 --> 00:04:59,890
So, what the first assignment step

142
00:04:59,900 --> 00:05:01,230
does is you know, it doesn't change

143
00:05:01,240 --> 00:05:02,950
the cluster centroids, but what it's

144
00:05:02,960 --> 00:05:07,230
doing is, exactly picking the values of C1, C2, up to CM

145
00:05:07,790 --> 00:05:10,490
that minimizes the cost

146
00:05:10,500 --> 00:05:12,500
function or the distortion function,

147
00:05:12,510 --> 00:05:14,660
J. And it's possible to prove

148
00:05:14,670 --> 00:05:17,160
that mathematically but, I won't do so here.

149
00:05:17,170 --> 00:05:18,600
That has a pretty intuitive meaning

150
00:05:18,610 --> 00:05:20,080
of just, yo know, well let's assign

151
00:05:20,090 --> 00:05:21,520
these points to the cluster centroid

152
00:05:21,530 --> 00:05:23,110
that is closest to it, because

153
00:05:23,120 --> 00:05:24,650
that's what minimizes the square

154
00:05:24,660 --> 00:05:27,830
of distance between the points and the corresponding cluster centroid.

155
00:05:27,840 --> 00:05:29,780
And then the other part of

156
00:05:29,790 --> 00:05:33,950
the second step of K-means, this second step over here.

157
00:05:33,960 --> 00:05:35,680
This second step was the move

158
00:05:35,690 --> 00:05:38,990
centroid step and,

159
00:05:39,000 --> 00:05:40,500
once again, I won't prove it,

160
00:05:40,510 --> 00:05:41,510
but it can be shown

161
00:05:41,520 --> 00:05:43,130
mathematically, that what the

162
00:05:43,140 --> 00:05:45,140
root centroid step does, is

163
00:05:45,150 --> 00:05:47,250
it chooses the values

164
00:05:47,260 --> 00:05:50,140
of mu that minimizes J.

165
00:05:50,150 --> 00:05:51,640
So it minimizes the cost function

166
00:05:51,650 --> 00:05:53,370
J with respect to,

167
00:05:53,380 --> 00:05:54,910
where wrt is my

168
00:05:54,920 --> 00:05:57,020
abbreviation for with respect to.

169
00:05:57,030 --> 00:05:58,780
But it minimizes J with respect

170
00:05:58,790 --> 00:06:02,030
to the locations of the cluster centroids, Mu1 through MuK.

171
00:06:02,040 --> 00:06:05,780
So, what K-means really

172
00:06:05,790 --> 00:06:07,000
is doing is it's taking the

173
00:06:07,010 --> 00:06:09,060
two sets of variables and

174
00:06:09,070 --> 00:06:11,540
partitioning them into two halves right here.

175
00:06:11,550 --> 00:06:15,440
First the C set of variables and then you have the Mu sets of variables.

176
00:06:15,450 --> 00:06:16,550
And what it does is it first

177
00:06:16,560 --> 00:06:18,040
minimizes J with respect

178
00:06:18,050 --> 00:06:19,690
to the variable C, and then minimizes

179
00:06:19,700 --> 00:06:21,110
J with respect the variables

180
00:06:21,120 --> 00:06:24,090
Mu, and then it keeps on iterating.

181
00:06:25,180 --> 00:06:27,690
And so that's all that K-means does.

182
00:06:27,700 --> 00:06:29,140
And, now that we understand

183
00:06:29,150 --> 00:06:31,020
K-means, let's try to

184
00:06:31,030 --> 00:06:32,420
minimize this cost function J.  We

185
00:06:32,430 --> 00:06:33,790
can also use this to

186
00:06:33,800 --> 00:06:35,080
try to debug our learning

187
00:06:35,090 --> 00:06:36,510
algorithm and just kind

188
00:06:36,520 --> 00:06:38,890
of make sure that our implementation

189
00:06:38,900 --> 00:06:41,210
of K-means is running correctly.

190
00:06:41,220 --> 00:06:43,060
So, we now understand the

191
00:06:43,070 --> 00:06:44,600
K-means algorithm as trying to

192
00:06:44,610 --> 00:06:46,630
optimize this cost function J,

193
00:06:46,640 --> 00:06:50,290
which is also called the dispulsion function.

194
00:06:50,650 --> 00:06:52,080
We can use that to debug K-means

195
00:06:52,090 --> 00:06:53,120
and help me show that K-means

196
00:06:53,130 --> 00:06:54,500
is converging, and that it's

197
00:06:54,510 --> 00:06:56,230
running properly, and in the

198
00:06:56,240 --> 00:06:57,680
next video, we'll also see

199
00:06:57,690 --> 00:06:59,110
how we can us this to

200
00:06:59,120 --> 00:07:01,290
help K-means find better clusters

201
00:07:01,300 --> 00:07:04,740
and help K-means to avoid local optima.

