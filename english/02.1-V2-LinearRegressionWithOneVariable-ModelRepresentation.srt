1
00:00:00,030 --> 00:00:02,870
Our first learning algorithm will be linear regression.

2
00:00:02,880 --> 00:00:04,050
In this video, you see what

3
00:00:04,060 --> 00:00:05,620
the model looks like and more

4
00:00:05,630 --> 00:00:06,860
importantly, you also see

5
00:00:06,870 --> 00:00:10,050
what the overall process of Supervised Learning looks like.

6
00:00:12,730 --> 00:00:15,940
Let's use some motivating examples of predicting housing prices.

7
00:00:15,950 --> 00:00:17,270
We're going to use a data

8
00:00:17,280 --> 00:00:19,440
set of housing prices from

9
00:00:19,450 --> 00:00:21,670
the city of Portland, Oregon.

10
00:00:21,680 --> 00:00:23,250
And you're going to plot

11
00:00:23,260 --> 00:00:24,920
my data set of a

12
00:00:24,930 --> 00:00:26,200
number of houses that were

13
00:00:26,210 --> 00:00:28,300
of different sizes that were

14
00:00:28,310 --> 00:00:31,230
sold for a range of different prices.

15
00:00:31,360 --> 00:00:32,500
Let's say that given this data

16
00:00:32,510 --> 00:00:34,190
set you have a friend that's

17
00:00:34,200 --> 00:00:35,740
trying to sell a house and

18
00:00:35,750 --> 00:00:37,500
let's see, your friend's house is

19
00:00:37,510 --> 00:00:40,710
of size 1250 square feet,

20
00:00:40,720 --> 00:00:41,910
and you want to tell them

21
00:00:41,920 --> 00:00:44,620
how much they might be able to sell the house for.

22
00:00:44,630 --> 00:00:45,640
Well, one thing you could do

23
00:00:45,650 --> 00:00:48,100
is fit a model,

24
00:00:48,110 --> 00:00:49,200
maybe you put a straight line

25
00:00:49,210 --> 00:00:52,690
to this data, something like

26
00:00:52,700 --> 00:00:54,560
that, and based on that,

27
00:00:54,570 --> 00:00:57,180
maybe you can tell your friend that,

28
00:00:57,590 --> 00:00:58,540
it looks like he can maybe sell the

29
00:00:58,550 --> 00:01:01,480
house for around to a $1000.

30
00:01:01,490 --> 00:01:03,490
So, this is an

31
00:01:03,500 --> 00:01:05,720
example of a Supervised

32
00:01:05,730 --> 00:01:07,700
Learning algorithm and it's

33
00:01:07,710 --> 00:01:09,530
Supervised Learning because we've given

34
00:01:09,540 --> 00:01:12,020
the right answer for each

35
00:01:12,030 --> 00:01:14,240
of our examples, namely we're

36
00:01:14,250 --> 00:01:16,550
told what was the actual holes.

37
00:01:16,560 --> 00:01:19,500
What was the actual prices for each of the houses, you know, data server.

38
00:01:19,510 --> 00:01:21,980
And moreover, just an

39
00:01:21,990 --> 00:01:23,960
example of a regression problem where

40
00:01:23,970 --> 00:01:25,540
the term regression at first

41
00:01:25,550 --> 00:01:26,800
the back that we're predicting the

42
00:01:26,810 --> 00:01:28,110
raw value of what, maybe

43
00:01:28,120 --> 00:01:30,550
the price and just to

44
00:01:30,560 --> 00:01:31,630
remind you, the other

45
00:01:31,640 --> 00:01:33,490
most common type of Supervised Learning

46
00:01:33,500 --> 00:01:35,160
problem is called is

47
00:01:35,170 --> 00:01:36,800
classification problem where we

48
00:01:36,810 --> 00:01:40,450
predict discrete value outputs.

49
00:01:40,980 --> 00:01:42,880
Such as if we are

50
00:01:42,890 --> 00:01:45,330
looking at cancer tumors and

51
00:01:45,340 --> 00:01:47,260
trying to decide if the tumor

52
00:01:47,270 --> 00:01:48,760
is malignant or benign.

53
00:01:48,770 --> 00:01:51,100
So, that's a 0-1 value.

54
00:01:52,040 --> 00:01:53,890
Well, formally, in Supervised Learning,

55
00:01:53,900 --> 00:01:55,850
we have a data set and

56
00:01:55,860 --> 00:01:58,670
this data set is called a training set.

57
00:01:58,680 --> 00:02:00,130
So, for housing prices example for

58
00:02:00,140 --> 00:02:01,980
wherever training sets of different

59
00:02:01,990 --> 00:02:03,780
housing prices and our

60
00:02:03,790 --> 00:02:04,940
job is to learn from

61
00:02:04,950 --> 00:02:08,470
this data how to predict the prices of the houses.

62
00:02:08,480 --> 00:02:11,210
Let's define some notation that we're using throughout this course.

63
00:02:11,220 --> 00:02:13,520
We're going to find quite a lot of symbols.

64
00:02:13,530 --> 00:02:14,910
It's OK if you don't

65
00:02:14,920 --> 00:02:16,050
remember all the symbols right now,

66
00:02:16,060 --> 00:02:17,670
but as the course progresses, they'll

67
00:02:17,680 --> 00:02:20,760
useful if you need notation.

68
00:02:20,770 --> 00:02:23,060
I'm going to use lowercase "m" throughout

69
00:02:23,070 --> 00:02:25,900
this course to denote the number of training examples.

70
00:02:25,910 --> 00:02:27,480
So, in this data set, if

71
00:02:27,490 --> 00:02:30,040
I have, let's say

72
00:02:30,050 --> 00:02:31,310
47 rows in this table,

73
00:02:31,320 --> 00:02:33,280
then I have 47 training examples

74
00:02:33,290 --> 00:02:34,340
and m equals 47.

75
00:02:34,350 --> 00:02:36,990
Then I use lowercase

76
00:02:37,000 --> 00:02:38,200
x to denote the input

77
00:02:38,210 --> 00:02:40,650
variables often also called the purchase.

78
00:02:40,660 --> 00:02:42,210
So, that would be x's here,

79
00:02:42,220 --> 00:02:43,970
with the ID of the

80
00:02:43,980 --> 00:02:45,150
purchase and I'm going to use

81
00:02:45,160 --> 00:02:46,590
y to denote my on-point

82
00:02:46,600 --> 00:02:47,810
variables, or the target

83
00:02:47,820 --> 00:02:51,020
variable that I'm trying to predict.

84
00:02:51,030 --> 00:02:52,850
So that's this second column here.

85
00:02:52,860 --> 00:02:54,350
On notation, I'm going to

86
00:02:54,360 --> 00:02:56,640
use x y

87
00:02:56,650 --> 00:03:00,390
to denote a single training example.

88
00:03:04,140 --> 00:03:05,370
So, a single row in

89
00:03:05,380 --> 00:03:06,970
this table corresponds to the

90
00:03:06,980 --> 00:03:09,300
single training example and to

91
00:03:09,310 --> 00:03:10,820
refer to a specific training

92
00:03:10,830 --> 00:03:12,350
example, I'm going to

93
00:03:12,360 --> 00:03:17,010
use this notation xi, yi

94
00:03:17,020 --> 00:03:20,210
and I'm going

95
00:03:20,220 --> 00:03:21,680
to use this to refer to

96
00:03:21,690 --> 00:03:25,950
the i training example.

97
00:03:28,660 --> 00:03:31,230
So this superscript i over

98
00:03:31,240 --> 00:03:33,090
here, this is

99
00:03:33,100 --> 00:03:34,800
not exponentiation, right, this

100
00:03:34,810 --> 00:03:37,150
x(i), y(i), the superscript i

101
00:03:37,160 --> 00:03:39,220
in parentheses, that's just

102
00:03:39,230 --> 00:03:40,870
an index into my training

103
00:03:40,880 --> 00:03:43,640
set and that refers to the i-th row of this table.

104
00:03:43,650 --> 00:03:43,850
Okay?

105
00:03:43,860 --> 00:03:45,170
So, this is not x to

106
00:03:45,180 --> 00:03:46,220
the power of i, y to the

107
00:03:46,230 --> 00:03:47,930
power of i. Instead x(i), y(i)

108
00:03:47,940 --> 00:03:49,970
just refers to the

109
00:03:49,980 --> 00:03:52,660
i row of this table.

110
00:03:52,670 --> 00:03:58,900
So for example, x1 refers to.

111
00:03:58,920 --> 00:04:00,240
If a value for our first

112
00:04:00,250 --> 00:04:02,910
three examples, so this

113
00:04:02,920 --> 00:04:05,940
2, 1, or 4, that gives us x in the first row.

114
00:04:05,960 --> 00:04:07,410
x will be equal

115
00:04:07,420 --> 00:04:09,110
to 1460 -

116
00:04:09,120 --> 00:04:12,050
that's the second X

117
00:04:12,060 --> 00:04:16,660
- and my 1 will be equal to well, 60 because that's

118
00:04:16,820 --> 00:04:20,580
the first, the y value with my first training example.

119
00:04:20,590 --> 00:04:22,420
I guess with that one.

120
00:04:22,430 --> 00:04:24,230
The first two.

121
00:04:24,460 --> 00:04:26,800
So, as I mentioned, occasionally, I'll

122
00:04:26,810 --> 00:04:28,740
ask you a question, to let

123
00:04:28,750 --> 00:04:30,520
you check your own standing, in

124
00:04:30,530 --> 00:04:31,760
a few seconds in this video,

125
00:04:31,770 --> 00:04:34,860
a multiple choice question will pop up in the video.

126
00:04:34,870 --> 00:04:35,940
When it does, please use your

127
00:04:35,950 --> 00:04:39,490
mouse to select what you think is the right answer.

128
00:04:39,690 --> 00:04:41,070
We'll define what the training set

129
00:04:41,080 --> 00:04:44,670
is, and so here's what we supervise learning and how he works.

130
00:04:44,680 --> 00:04:45,870
We saw them with a training

131
00:04:45,880 --> 00:04:47,250
set like our training sets of

132
00:04:47,260 --> 00:04:48,940
housing prices and we feed

133
00:04:48,950 --> 00:04:51,340
that to our learning algorithm.

134
00:04:51,350 --> 00:04:52,480
It is the job of the learning

135
00:04:52,490 --> 00:04:54,070
algorithm to then output

136
00:04:54,080 --> 00:04:56,760
the function which by convention

137
00:04:56,770 --> 00:04:58,460
is usually denoted lower case

138
00:04:58,470 --> 00:05:01,260
h, and h stands for hypothesis.

139
00:05:02,490 --> 00:05:04,010
And what the job of the

140
00:05:04,020 --> 00:05:05,750
hypothesis is, is a

141
00:05:05,760 --> 00:05:07,330
function that takes it's

142
00:05:07,340 --> 00:05:08,780
input the size of

143
00:05:08,790 --> 00:05:10,010
a house like maybe the size

144
00:05:10,020 --> 00:05:11,110
of a new house that you know

145
00:05:11,120 --> 00:05:12,140
your friend's trying to sell,

146
00:05:12,150 --> 00:05:13,890
so assuming the value of

147
00:05:13,900 --> 00:05:15,230
x, and it try

148
00:05:15,240 --> 00:05:17,490
to output the estimated

149
00:05:17,500 --> 00:05:20,480
value of Y

150
00:05:20,490 --> 00:05:24,590
for their corresponding house.

151
00:05:24,600 --> 00:05:26,890
So h is a function

152
00:05:26,900 --> 00:05:31,700
that maps from x's

153
00:05:31,910 --> 00:05:33,050
to y's.

154
00:05:33,060 --> 00:05:37,650
People often ask me, you know, "Why is this function called hypothesis?"

155
00:05:37,660 --> 00:05:39,550
Some of you may know the meaning

156
00:05:39,560 --> 00:05:43,200
of the term "hypothesis" from the dictionary of science, or whatever.

157
00:05:43,210 --> 00:05:44,240
It turns out that in machine

158
00:05:44,250 --> 00:05:45,590
learning, this was a

159
00:05:45,600 --> 00:05:46,790
name that was used in

160
00:05:46,800 --> 00:05:48,270
the early days of machine learning

161
00:05:48,280 --> 00:05:49,530
and it's kinda stuck, Because

162
00:05:49,540 --> 00:05:50,860
it's maybe not a great name,

163
00:05:50,870 --> 00:05:52,510
for this sort of function for

164
00:05:52,520 --> 00:05:54,190
mapping, from sizes of houses

165
00:05:54,200 --> 00:05:56,060
to the predictions but, you know,

166
00:05:56,070 --> 00:05:58,440
I think the term hypothesis

167
00:05:58,450 --> 00:05:59,720
maybe isn't the best

168
00:05:59,730 --> 00:06:00,900
possible in need, but this is

169
00:06:00,910 --> 00:06:01,880
what, this is the

170
00:06:01,890 --> 00:06:04,290
standard terminology that people using should already know.

171
00:06:04,300 --> 00:06:05,630
So don't worry too, don 't

172
00:06:05,640 --> 00:06:08,980
worry too much about why people call it that.

173
00:06:09,170 --> 00:06:10,690
When designing a learning algorithm,

174
00:06:10,700 --> 00:06:12,050
the next thing you need to decide,

175
00:06:12,060 --> 00:06:13,200
is how do we

176
00:06:13,210 --> 00:06:16,320
represent this hypothesis, h.

177
00:06:16,330 --> 00:06:17,370
For this, and the next few

178
00:06:17,380 --> 00:06:19,930
videos, I'm going to choose

179
00:06:19,940 --> 00:06:21,800
our initial choice for representing

180
00:06:21,810 --> 00:06:23,090
a hypothesis will be the

181
00:06:23,100 --> 00:06:25,020
following we're gonna represent H,

182
00:06:25,030 --> 00:06:26,070
as follows, and we're gonna write

183
00:06:26,080 --> 00:06:27,620
to this h subscript theta of

184
00:06:27,630 --> 00:06:29,820
x equals to

185
00:06:29,830 --> 00:06:32,830
theta zero plus theta one

186
00:06:32,840 --> 00:06:35,580
of x, and as a

187
00:06:35,590 --> 00:06:37,880
short hand, sometimes instead

188
00:06:37,890 --> 00:06:39,190
of writing, you know, h

189
00:06:39,200 --> 00:06:41,130
subscript theta of x sometimes

190
00:06:41,140 --> 00:06:42,250
as a shorthand I'll just write

191
00:06:42,260 --> 00:06:43,690
this as h of x.

192
00:06:43,700 --> 00:06:44,940
But more often I'll write it

193
00:06:44,950 --> 00:06:48,090
as a subscript data, over there.

194
00:06:48,100 --> 00:06:49,670
And, plotting to some pictures,

195
00:06:49,680 --> 00:06:51,310
all this means is that, we

196
00:06:51,320 --> 00:06:53,600
are going to, you know

197
00:06:53,610 --> 00:06:55,960
predict that Y, is the

198
00:06:55,970 --> 00:06:59,200
linear function of x. Right?

199
00:06:59,210 --> 00:07:00,970
So, that's our data set,

200
00:07:00,980 --> 00:07:02,560
and what this function is

201
00:07:02,570 --> 00:07:04,750
doing, is it's predicting, that

202
00:07:04,760 --> 00:07:07,130
At Y, is some straight

203
00:07:07,140 --> 00:07:09,930
line function of x.  X

204
00:07:09,940 --> 00:07:11,350
equals three to zero, plus

205
00:07:11,360 --> 00:07:15,470
three to 1x, OK?

206
00:07:15,480 --> 00:07:17,750
And, why a linear function?

207
00:07:17,760 --> 00:07:19,780
Well, sometimes we'll want

208
00:07:19,790 --> 00:07:23,060
to fit more complicated, perhaps non-linear functions as well.

209
00:07:23,070 --> 00:07:25,330
But since this linear case

210
00:07:25,340 --> 00:07:26,860
is a simple building block, we'll start

211
00:07:26,870 --> 00:07:28,330
with this example first of fitting

212
00:07:28,340 --> 00:07:30,080
linear functions, and we'll build

213
00:07:30,090 --> 00:07:31,890
on this to eventually have

214
00:07:31,900 --> 00:07:33,320
more complex models and more

215
00:07:33,330 --> 00:07:36,170
complex learning algorithms,

216
00:07:36,180 --> 00:07:38,450
let me also give this particular model a name.

217
00:07:38,460 --> 00:07:40,330
This model is called

218
00:07:40,340 --> 00:07:42,130
linear regression, or this particular

219
00:07:42,140 --> 00:07:44,330
example is actually linear regression

220
00:07:44,340 --> 00:07:46,050
with one variable, with the

221
00:07:46,060 --> 00:07:48,600
variable being x, right?

222
00:07:48,610 --> 00:07:49,770
So predicting housing prices as functions

223
00:07:49,780 --> 00:07:51,970
of one variable x and, another

224
00:07:51,980 --> 00:07:53,850
name for this model is univariate

225
00:07:53,860 --> 00:07:56,410
linear regression, and univariate

226
00:07:56,420 --> 00:07:58,020
is just a fancy way

227
00:07:58,030 --> 00:08:01,020
of saying, one variable.

228
00:08:02,020 --> 00:08:04,330
So that's linear regression.

229
00:08:04,340 --> 00:08:05,860
In the next video we'll

230
00:08:05,870 --> 00:08:10,100
start to talk about just how we go about implementing this model.

