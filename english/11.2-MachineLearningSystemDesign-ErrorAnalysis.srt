1
00:00:00,210 --> 00:00:01,590
In the last video, I talked

2
00:00:01,600 --> 00:00:03,510
about how when faced with

3
00:00:03,520 --> 00:00:04,970
a machine learning problem, there are

4
00:00:04,980 --> 00:00:08,450
often lots of different ideas on how to improve the algorithm.

5
00:00:08,460 --> 00:00:09,640
In this video let's

6
00:00:09,650 --> 00:00:11,320
talk about the concepts of error

7
00:00:11,330 --> 00:00:13,060
analysis which will help

8
00:00:13,070 --> 00:00:14,290
me give you a way to more

9
00:00:14,300 --> 00:00:17,330
systematically make some of these decisions.

10
00:00:18,070 --> 00:00:19,530
If you're starting work on a

11
00:00:19,540 --> 00:00:21,390
machine learning product or building

12
00:00:21,400 --> 00:00:23,470
a machine learning application, it is

13
00:00:23,480 --> 00:00:25,830
often considered very good practice

14
00:00:25,840 --> 00:00:27,510
to start, not by building

15
00:00:27,520 --> 00:00:29,210
a very complicated system with

16
00:00:29,220 --> 00:00:30,920
lots of complex features and so

17
00:00:30,930 --> 00:00:33,050
on, but to instead start

18
00:00:33,060 --> 00:00:34,500
by building a very simple

19
00:00:34,510 --> 00:00:37,260
algorithm, the you can implement quickly.

20
00:00:37,480 --> 00:00:38,730
And when I start on

21
00:00:38,740 --> 00:00:40,140
a learning problem, what I usually

22
00:00:40,150 --> 00:00:41,560
do is spend at most one

23
00:00:41,570 --> 00:00:43,450
day, literally at most 24

24
00:00:43,460 --> 00:00:47,030
hours to try to get something really quick and dirty.

25
00:00:47,040 --> 00:00:49,360
Frankly not at all sophisticated system.

26
00:00:49,370 --> 00:00:50,390
But get something really quick and

27
00:00:50,400 --> 00:00:52,580
dirty running and implement

28
00:00:52,590 --> 00:00:53,870
it and then test it on

29
00:00:53,880 --> 00:00:56,040
my cross validation data. Once

30
00:00:56,050 --> 00:00:57,470
you've done that, you can

31
00:00:57,480 --> 00:00:59,950
then plot learning curves.

32
00:00:59,960 --> 00:01:03,220
This is what we talked about in the previous set of videos.

33
00:01:03,230 --> 00:01:05,360
But plot learning curves of the

34
00:01:05,370 --> 00:01:07,300
training and test errors to

35
00:01:07,310 --> 00:01:08,390
try to figure out if your

36
00:01:08,400 --> 00:01:10,110
learning algorithm may be suffering

37
00:01:10,120 --> 00:01:11,430
from high bias or high

38
00:01:11,440 --> 00:01:13,430
variance or something else and

39
00:01:13,440 --> 00:01:14,480
use that to try to

40
00:01:14,490 --> 00:01:16,070
decide if having more data

41
00:01:16,080 --> 00:01:18,660
and more features and so on are likely to help.

42
00:01:18,670 --> 00:01:19,990
And the reason that this

43
00:01:20,000 --> 00:01:21,930
is a good approach is often

44
00:01:21,940 --> 00:01:23,090
when you're just starting out on

45
00:01:23,100 --> 00:01:24,670
a learning problem, there's really no

46
00:01:24,680 --> 00:01:26,470
way to tell in advance

47
00:01:26,480 --> 00:01:27,780
whether you need more complex

48
00:01:27,790 --> 00:01:29,240
features or whether you need

49
00:01:29,250 --> 00:01:31,270
more data or something else.

50
00:01:31,280 --> 00:01:32,500
And it's just very hard to tell

51
00:01:32,510 --> 00:01:33,960
in advance, that is in

52
00:01:33,970 --> 00:01:36,150
the absence of evidence, in

53
00:01:36,160 --> 00:01:37,960
the absence of seeing a

54
00:01:37,970 --> 00:01:39,740
learning curve, it's just incredibly

55
00:01:39,750 --> 00:01:43,750
difficult to figure out where you should be spending your time.

56
00:01:43,760 --> 00:01:45,720
And it's often by implementing even

57
00:01:45,730 --> 00:01:46,970
a very, very quick and dirty

58
00:01:46,980 --> 00:01:48,530
implementation and by plotting

59
00:01:48,540 --> 00:01:52,570
learning curves that that helps you make these decisions.

60
00:01:52,580 --> 00:01:53,550
So if you like, you can think

61
00:01:53,560 --> 00:01:54,610
of this as a way of

62
00:01:54,620 --> 00:01:56,560
avoiding what's sometimes called

63
00:01:56,570 --> 00:01:59,990
premature optimization in computer programming.

64
00:02:00,000 --> 00:02:01,190
And this is idea that just

65
00:02:01,200 --> 00:02:03,450
says that we should let

66
00:02:03,460 --> 00:02:05,640
evidence guide our decisions

67
00:02:05,650 --> 00:02:07,150
on where to spend our time

68
00:02:07,160 --> 00:02:09,060
rather than use gut feeling,

69
00:02:09,070 --> 00:02:10,920
which is often wrong.

70
00:02:10,930 --> 00:02:12,380
In addition to plotting learning

71
00:02:12,390 --> 00:02:13,800
curves, one other thing

72
00:02:13,810 --> 00:02:17,940
that's often very useful to do is what's called error analysis.

73
00:02:18,120 --> 00:02:19,270
And what I mean by that is

74
00:02:19,280 --> 00:02:20,760
that when building, say

75
00:02:20,770 --> 00:02:22,460
a spam classifier, I will

76
00:02:22,470 --> 00:02:24,720
often look at my

77
00:02:24,730 --> 00:02:27,350
cross validation set and manually

78
00:02:27,360 --> 00:02:29,300
look at the emails that my

79
00:02:29,310 --> 00:02:31,170
algorithm is making errors on.

80
00:02:31,180 --> 00:02:32,620
So, look at the spam emails

81
00:02:32,630 --> 00:02:34,630
and non-spam emails that the

82
00:02:34,640 --> 00:02:37,420
algorithm is misclassifying, and see

83
00:02:37,430 --> 00:02:39,200
if you can spot any systematic

84
00:02:39,210 --> 00:02:42,800
patterns in what type of examples it is misclassifying.

85
00:02:42,980 --> 00:02:44,800
And often by doing that, this

86
00:02:44,810 --> 00:02:47,160
is the process that would inspire

87
00:02:47,170 --> 00:02:49,420
you to design new features.

88
00:02:49,430 --> 00:02:50,910
Or they'll tell you whether the

89
00:02:50,920 --> 00:02:52,390
current things or current

90
00:02:52,400 --> 00:02:54,260
shortcomings of the system

91
00:02:54,270 --> 00:02:55,650
and give you the inspiration you

92
00:02:55,660 --> 00:02:58,250
need to come up with improvements to it.

93
00:02:58,260 --> 00:03:01,340
Concretely, here's a specific example.

94
00:03:01,350 --> 00:03:02,770
Let's say you've built a spam

95
00:03:02,780 --> 00:03:05,830
classifier and you

96
00:03:05,840 --> 00:03:07,930
have 500 examples in

97
00:03:07,940 --> 00:03:10,400
your cross-validation set.

98
00:03:10,410 --> 00:03:12,000
And let's say in this example, that the

99
00:03:12,010 --> 00:03:13,330
algorithm has a very high error

100
00:03:13,340 --> 00:03:14,900
rate, and it misclassifies a

101
00:03:14,910 --> 00:03:18,000
hundred of these cross-validation examples.

102
00:03:18,770 --> 00:03:20,440
So what I do is manually

103
00:03:20,450 --> 00:03:22,520
examine these 100 errors, and

104
00:03:22,530 --> 00:03:24,690
manually categorize them, based

105
00:03:24,700 --> 00:03:25,970
on things like what type

106
00:03:25,980 --> 00:03:27,260
of email it is and

107
00:03:27,270 --> 00:03:28,700
what cues or what features you

108
00:03:28,710 --> 00:03:32,440
think might have helped the algorithm classify them incorrectly.

109
00:03:32,450 --> 00:03:34,070
So, specifically, by what

110
00:03:34,080 --> 00:03:35,550
type of email it is,

111
00:03:35,560 --> 00:03:37,130
you know, if I look through these

112
00:03:37,140 --> 00:03:38,510
hundred errors I may find

113
00:03:38,520 --> 00:03:39,960
that maybe the most

114
00:03:39,970 --> 00:03:41,830
common types of spam

115
00:03:41,840 --> 00:03:44,000
emails in misclassifies are maybe

116
00:03:44,010 --> 00:03:45,600
emails on pharmacy, so basically

117
00:03:45,610 --> 00:03:48,600
these are emails trying to

118
00:03:48,610 --> 00:03:50,170
sell drugs, maybe emails that are

119
00:03:50,180 --> 00:03:51,750
trying to sell replicas -

120
00:03:51,760 --> 00:03:55,830
those are those fake watches fake you know, random things.

121
00:03:56,160 --> 00:04:00,230
Maybe have some emails trying to steal passwords.

122
00:04:00,240 --> 00:04:02,170
These are also called phishing emails.

123
00:04:02,180 --> 00:04:06,150
But that's another big category of emails and maybe other categories.

124
00:04:06,160 --> 00:04:08,110
So, in terms

125
00:04:08,120 --> 00:04:09,520
of classify what type of email

126
00:04:09,530 --> 00:04:10,880
it is, I would actually go through

127
00:04:10,890 --> 00:04:12,190
and count up, you know, of

128
00:04:12,200 --> 00:04:14,390
my 100 emails, maybe I

129
00:04:14,400 --> 00:04:15,610
find that twelve of the

130
00:04:15,620 --> 00:04:18,090
mislabeled emails are pharma emails.

131
00:04:18,100 --> 00:04:19,690
And maybe four of them

132
00:04:19,700 --> 00:04:20,970
are emails trying to sell

133
00:04:20,980 --> 00:04:23,710
replicas, they sell fake watches or something.

134
00:04:23,720 --> 00:04:25,640
And maybe I find that 53

135
00:04:25,650 --> 00:04:27,710
of them are these,

136
00:04:27,720 --> 00:04:29,720
what's called phishing emails, basically emails

137
00:04:29,730 --> 00:04:31,010
trying to persuade you to

138
00:04:31,020 --> 00:04:34,260
give them your password, and 31 emails are other types of emails.

139
00:04:35,330 --> 00:04:37,270
And it's by counting up the

140
00:04:37,280 --> 00:04:38,420
number of emails in these

141
00:04:38,430 --> 00:04:39,780
different categories that you might

142
00:04:39,790 --> 00:04:41,860
discover, for example, that the

143
00:04:41,870 --> 00:04:44,160
algorithm is doing really particularly

144
00:04:44,170 --> 00:04:45,770
poorly on emails trying to

145
00:04:45,780 --> 00:04:47,390
steal passwords, and that

146
00:04:47,400 --> 00:04:49,370
may suggest that it might

147
00:04:49,380 --> 00:04:50,680
be worth your effort to look

148
00:04:50,690 --> 00:04:51,890
more carefully at that type

149
00:04:51,900 --> 00:04:53,440
of email, and see if

150
00:04:53,450 --> 00:04:55,060
you can come up with better features

151
00:04:55,070 --> 00:04:57,540
to categorize them correctly.

152
00:04:57,550 --> 00:04:58,990
And also, what I might

153
00:04:59,000 --> 00:05:00,540
do is look at what cues,

154
00:05:00,550 --> 00:05:02,610
or what features, additional features

155
00:05:02,620 --> 00:05:06,080
might have helped the algorithm classify the emails.

156
00:05:06,090 --> 00:05:07,050
So let's say that some of

157
00:05:07,060 --> 00:05:09,830
our hypotheses about things or

158
00:05:09,840 --> 00:05:10,910
features that might help us

159
00:05:10,920 --> 00:05:13,480
classify emails better are trying

160
00:05:13,490 --> 00:05:16,210
to detect deliberate misspellings versus

161
00:05:16,220 --> 00:05:19,940
unusual email routing versus unusual, you know,

162
00:05:19,950 --> 00:05:21,780
spamming punctuation, such as

163
00:05:21,790 --> 00:05:23,690
people use a lot of exclamation marks.

164
00:05:23,700 --> 00:05:24,850
And once again, I would manually

165
00:05:24,860 --> 00:05:25,750
go through and let's say

166
00:05:25,760 --> 00:05:27,610
I find five cases of

167
00:05:27,620 --> 00:05:29,490
this, and 16 of

168
00:05:29,500 --> 00:05:31,170
this, and 32 of this and

169
00:05:31,180 --> 00:05:34,760
a bunch of other types of emails as well.

170
00:05:34,770 --> 00:05:36,340
And if this is what

171
00:05:36,350 --> 00:05:38,060
you get on your cross validation

172
00:05:38,070 --> 00:05:39,290
set then it really tells

173
00:05:39,300 --> 00:05:41,650
you that, you know, maybe deliberate spelling

174
00:05:41,660 --> 00:05:43,490
is a sufficiently rare phenomenon

175
00:05:43,500 --> 00:05:44,830
that maybe is not really worth

176
00:05:44,840 --> 00:05:47,700
all your time trying to write

177
00:05:47,710 --> 00:05:49,470
algorithms to detect that.

178
00:05:49,480 --> 00:05:50,770
But if you find a lot

179
00:05:50,780 --> 00:05:52,130
of spammers are using, you

180
00:05:52,140 --> 00:05:54,280
know, unusual punctuation then

181
00:05:54,290 --> 00:05:55,660
maybe that's a strong sign

182
00:05:55,670 --> 00:05:56,990
that it might actually be

183
00:05:57,000 --> 00:05:58,770
worth your while to spend

184
00:05:58,780 --> 00:06:00,900
the time to develop more sophisticated

185
00:06:00,910 --> 00:06:03,320
features based on the punctuation.

186
00:06:03,330 --> 00:06:05,030
So, this sort of error

187
00:06:05,040 --> 00:06:06,680
analysis which is really

188
00:06:06,690 --> 00:06:09,180
the process of manually examining

189
00:06:09,190 --> 00:06:10,770
the mistakes that the algorithm

190
00:06:10,780 --> 00:06:12,550
makes, can often help

191
00:06:12,560 --> 00:06:15,990
guide you to the most fruitful avenues to pursue.

192
00:06:16,000 --> 00:06:17,580
And this also explains why I

193
00:06:17,590 --> 00:06:19,540
often recommend implementing a quick

194
00:06:19,550 --> 00:06:22,030
and dirty implementation of an algorithm.

195
00:06:22,040 --> 00:06:23,250
What we really want to do

196
00:06:23,260 --> 00:06:24,300
is figure out what are

197
00:06:24,310 --> 00:06:27,850
the most difficult examples for an algorithm to classify.

198
00:06:27,860 --> 00:06:30,450
And very often for different

199
00:06:30,460 --> 00:06:32,000
algorithms, for different learning algorithms,

200
00:06:32,010 --> 00:06:33,550
they'll often find, you

201
00:06:33,560 --> 00:06:37,000
know, similar categories of examples difficult.

202
00:06:37,010 --> 00:06:38,050
And by having a quick and

203
00:06:38,060 --> 00:06:39,900
dirty implementation, that's often a

204
00:06:39,910 --> 00:06:41,420
quick way to let you

205
00:06:41,430 --> 00:06:43,610
identify some errors and quickly

206
00:06:43,620 --> 00:06:44,780
identify what are the

207
00:06:44,790 --> 00:06:49,220
hard examples so that you can focus your efforts on those.

208
00:06:49,230 --> 00:06:52,250
Lastly, when developing learning algorithms,

209
00:06:52,260 --> 00:06:54,180
one other useful tip is

210
00:06:54,190 --> 00:06:55,580
to make sure that you have

211
00:06:55,590 --> 00:06:56,800
a way, that you have a

212
00:06:56,810 --> 00:07:01,210
numerical evaluation of your learning algorithm.

213
00:07:02,130 --> 00:07:03,450
Now what I mean by that is that

214
00:07:03,460 --> 00:07:05,220
if you're developing a learning algorithm,

215
00:07:05,230 --> 00:07:08,050
it is often incredibly helpful

216
00:07:08,060 --> 00:07:09,450
if you have a way of

217
00:07:09,460 --> 00:07:11,280
evaluating your learning algorithm

218
00:07:11,290 --> 00:07:13,640
that just gives you back a single real number.

219
00:07:13,650 --> 00:07:15,610
Maybe accuracy, maybe error.

220
00:07:15,620 --> 00:07:19,890
But the single real number that tells you how well your learning algorithm is doing.

221
00:07:20,280 --> 00:07:21,760
I'll talk more about this specific

222
00:07:21,770 --> 00:07:25,780
concepts in later videos, but here's a specific example.

223
00:07:25,790 --> 00:07:26,680
Let's say we are trying to

224
00:07:26,690 --> 00:07:28,050
decide whether or not we

225
00:07:28,060 --> 00:07:29,580
should treat words like discount,

226
00:07:29,590 --> 00:07:32,360
discounts, discounter, discounting, as the same word.

227
00:07:32,370 --> 00:07:33,510
So maybe one way to

228
00:07:33,520 --> 00:07:35,390
do that is to just

229
00:07:35,400 --> 00:07:38,950
look at the first few characters in a word.

230
00:07:38,960 --> 00:07:40,290
Like, you know, if you just look at

231
00:07:40,300 --> 00:07:41,770
the first few characters of

232
00:07:41,780 --> 00:07:44,910
a word, then you figure

233
00:07:44,920 --> 00:07:46,120
out that maybe all of these

234
00:07:46,130 --> 00:07:49,490
words are roughly -   have similar meanings.

235
00:07:50,460 --> 00:07:52,240
In natural language processing, the

236
00:07:52,250 --> 00:07:53,500
way that this is done is

237
00:07:53,510 --> 00:07:56,930
actually using a type of software called stemming software.

238
00:07:56,940 --> 00:07:58,150
If you ever want to do

239
00:07:58,160 --> 00:07:59,940
this yourself, search on a

240
00:07:59,950 --> 00:08:01,490
web search engine for the

241
00:08:01,500 --> 00:08:02,950
Porter Stemmer and that

242
00:08:02,960 --> 00:08:04,610
would be, you know, one reasonable piece of

243
00:08:04,620 --> 00:08:06,100
software for doing this sort

244
00:08:06,110 --> 00:08:07,120
of stemming, which will let

245
00:08:07,130 --> 00:08:08,790
you treat all of these discount,

246
00:08:08,800 --> 00:08:12,040
discounts, and so on as the same word.

247
00:08:13,950 --> 00:08:16,620
But using a stemming software

248
00:08:16,630 --> 00:08:17,820
that basically looks at the

249
00:08:17,830 --> 00:08:19,440
first few alphabets of the

250
00:08:19,450 --> 00:08:22,230
word more or less, it can help but it can hurt.

251
00:08:22,240 --> 00:08:23,890
And it can hurt because, for

252
00:08:23,900 --> 00:08:25,920
example, this software may

253
00:08:25,930 --> 00:08:27,980
mistake the words universe and

254
00:08:27,990 --> 00:08:30,060
university as being the

255
00:08:30,070 --> 00:08:31,440
same thing because, you know,

256
00:08:31,450 --> 00:08:33,470
these two words start off

257
00:08:33,480 --> 00:08:36,980
with very similar characters, with the same alphabets.

258
00:08:37,300 --> 00:08:39,270
So if you're trying

259
00:08:39,280 --> 00:08:40,620
to decide whether or not

260
00:08:40,630 --> 00:08:42,660
to use stemming software for

261
00:08:42,670 --> 00:08:46,340
a stem classifier, it is not always easy to tell.

262
00:08:46,350 --> 00:08:48,500
And in particular, error analysis

263
00:08:48,510 --> 00:08:51,020
may not actually be helpful

264
00:08:51,030 --> 00:08:53,050
for deciding if this

265
00:08:53,060 --> 00:08:55,560
sort of stemming idea is a good idea.

266
00:08:55,570 --> 00:08:57,010
Instead, the best way

267
00:08:57,020 --> 00:08:58,680
to figure out if using stemming

268
00:08:58,690 --> 00:09:00,180
software is good to help

269
00:09:00,190 --> 00:09:01,730
your classifier is if you

270
00:09:01,740 --> 00:09:03,360
have a way to very quickly

271
00:09:03,370 --> 00:09:06,670
just try it and see if it works.

272
00:09:08,560 --> 00:09:10,250
And in order to do this,

273
00:09:10,260 --> 00:09:12,240
having a way to numerically

274
00:09:12,250 --> 00:09:15,930
evaluate your algorithm, is going to be very helpful.

275
00:09:15,940 --> 00:09:18,100
Concretely, maybe the most

276
00:09:18,110 --> 00:09:19,340
natural thing to do is

277
00:09:19,350 --> 00:09:20,890
to look at the cross validation

278
00:09:20,900 --> 00:09:24,580
error of the algorithm's performance with and without stemming.

279
00:09:24,590 --> 00:09:25,790
So, if you run your

280
00:09:25,800 --> 00:09:27,320
algorithm without stemming and you

281
00:09:27,330 --> 00:09:29,070
end up with, let's say,

282
00:09:29,080 --> 00:09:31,350
five percent classification error, and

283
00:09:31,360 --> 00:09:32,530
you re-run it and you

284
00:09:32,540 --> 00:09:34,100
end up with, let's say, three

285
00:09:34,110 --> 00:09:36,430
percent classification error, then this

286
00:09:36,440 --> 00:09:38,630
decrease in error very quickly

287
00:09:38,640 --> 00:09:40,300
allows you to decide that,

288
00:09:40,310 --> 00:09:43,070
you know, it looks like using stemming is a good idea.

289
00:09:43,080 --> 00:09:44,930
For this particular problem, there's

290
00:09:44,940 --> 00:09:46,820
a very natural single real

291
00:09:46,830 --> 00:09:50,920
number evaluation metric, namely, the cross validation error.

292
00:09:50,930 --> 00:09:53,070
We'll see later, examples where coming

293
00:09:53,080 --> 00:09:54,780
up with this, sort of, single

294
00:09:54,790 --> 00:09:58,780
row number evaluation metric may need a little bit more work.

295
00:09:58,790 --> 00:09:59,920
But as we'll see in

296
00:09:59,930 --> 00:10:01,740
the later video, doing so would

297
00:10:01,750 --> 00:10:02,980
also then let you

298
00:10:02,990 --> 00:10:04,750
make these decisions much more quickly

299
00:10:04,760 --> 00:10:07,880
of, say, whether or not to use stemming.

300
00:10:08,700 --> 00:10:10,670
And just this one more quick example.

301
00:10:10,680 --> 00:10:12,030
Let's say that you're also trying

302
00:10:12,040 --> 00:10:13,640
to decide whether or not

303
00:10:13,650 --> 00:10:15,980
to distinguish between upper versus lower case.

304
00:10:15,990 --> 00:10:17,050
So, you know, is the red

305
00:10:17,060 --> 00:10:19,050
mom with uppercase M

306
00:10:19,060 --> 00:10:20,690
versus lower case m,

307
00:10:20,700 --> 00:10:21,770
I mean, should that be treated as

308
00:10:21,780 --> 00:10:23,960
the same word or as different words?

309
00:10:23,970 --> 00:10:27,000
Should these be treated as the same feature or as different features?

310
00:10:27,010 --> 00:10:28,340
And so once again,

311
00:10:28,350 --> 00:10:29,290
because we have a way

312
00:10:29,300 --> 00:10:31,050
to evaluate our algorithm, if

313
00:10:31,060 --> 00:10:32,640
you try this out here, if

314
00:10:32,650 --> 00:10:35,130
I stop distinguishing upper

315
00:10:35,140 --> 00:10:36,590
and lower case, maybe I

316
00:10:36,600 --> 00:10:38,690
end up with 3.2%

317
00:10:38,700 --> 00:10:40,010
error and I find that

318
00:10:40,020 --> 00:10:42,250
therefore this does worse

319
00:10:42,260 --> 00:10:43,630
than, you know, if I use only

320
00:10:43,640 --> 00:10:45,360
stemming, and so this lets

321
00:10:45,370 --> 00:10:48,260
me very quickly decide to go

322
00:10:48,270 --> 00:10:49,810
ahead and to distinguish or to

323
00:10:49,820 --> 00:10:52,130
not distinguish between upper and lower case.

324
00:10:52,140 --> 00:10:53,680
So when you' re developing

325
00:10:53,690 --> 00:10:55,640
a learning algorithm, very often

326
00:10:55,650 --> 00:10:57,040
you'll be trying out lots of

327
00:10:57,050 --> 00:11:00,950
new ideas and lots of new versions of your learning algorithm.

328
00:11:00,960 --> 00:11:02,340
If every time you try

329
00:11:02,350 --> 00:11:03,830
out a new idea if you

330
00:11:03,840 --> 00:11:05,740
end up manually examining a

331
00:11:05,750 --> 00:11:06,850
bunch of examples, you begin to

332
00:11:06,860 --> 00:11:08,630
see better or worse, you

333
00:11:08,640 --> 00:11:09,570
know, that's going to make it

334
00:11:09,580 --> 00:11:10,970
really hard to make decisions

335
00:11:10,980 --> 00:11:12,570
on do you use stemming or not.

336
00:11:12,580 --> 00:11:15,140
Do you distinguish upper or lowercase or not?

337
00:11:15,180 --> 00:11:16,760
But by having a single rule

338
00:11:16,770 --> 00:11:18,670
number evaluation metric, you can

339
00:11:18,680 --> 00:11:22,410
then just look and see oh, did the error go up or go down?

340
00:11:22,420 --> 00:11:23,930
And you can use that much

341
00:11:23,940 --> 00:11:25,830
more rapidly, try out

342
00:11:25,840 --> 00:11:27,980
new ideas and almost right

343
00:11:27,990 --> 00:11:29,680
away tell if your new

344
00:11:29,690 --> 00:11:32,430
idea has improved or worsened

345
00:11:32,440 --> 00:11:33,920
the performance of the learning algorithm

346
00:11:33,930 --> 00:11:35,550
and this will let

347
00:11:35,560 --> 00:11:38,520
you often make much faster progress.

348
00:11:38,530 --> 00:11:40,210
So the recommended, strongly recommended

349
00:11:40,220 --> 00:11:42,360
way to do error analysis is

350
00:11:42,370 --> 00:11:45,480
on the cross validation set rather than the test set.

351
00:11:45,490 --> 00:11:47,230
But, you know, there are

352
00:11:47,240 --> 00:11:48,360
people that will do this on

353
00:11:48,370 --> 00:11:49,720
the test set even though that's

354
00:11:49,730 --> 00:11:52,180
definitely a less mathematically appropriate

355
00:11:52,190 --> 00:11:54,720
set of your list, recommended what

356
00:11:54,730 --> 00:11:55,770
you think to do than to

357
00:11:55,780 --> 00:11:57,440
do error analysis on your

358
00:11:57,450 --> 00:11:59,130
cross validation sector.

359
00:11:59,140 --> 00:12:01,820
So, to wrap up this video, when starting

360
00:12:01,830 --> 00:12:03,600
on the new machine learning problem, what

361
00:12:03,610 --> 00:12:05,600
I almost always recommend is

362
00:12:05,610 --> 00:12:07,020
to implement a quick and

363
00:12:07,030 --> 00:12:09,770
dirty implementation of your learning algorithm.

364
00:12:09,780 --> 00:12:12,110
And I've almost never seen

365
00:12:12,120 --> 00:12:16,870
anyone spend too little time on this quick and dirty implementation.

366
00:12:18,640 --> 00:12:20,470
I pretty much only ever see

367
00:12:20,480 --> 00:12:22,360
people spend much too much

368
00:12:22,370 --> 00:12:24,570
time building their first, you know,

369
00:12:24,580 --> 00:12:26,580
supposedly quick and dirty implementations.

370
00:12:26,590 --> 00:12:29,060
So really, don't worry about

371
00:12:29,070 --> 00:12:32,110
it being too quick, or don't worry about it being too dirty.

372
00:12:32,120 --> 00:12:33,680
But really implement something as

373
00:12:33,690 --> 00:12:35,440
quickly as you can, and once

374
00:12:35,450 --> 00:12:37,810
you have the initial implementation this

375
00:12:37,820 --> 00:12:39,220
is then a powerful tool for

376
00:12:39,230 --> 00:12:40,600
deciding where to spend your

377
00:12:40,610 --> 00:12:42,380
time next, because first we

378
00:12:42,390 --> 00:12:43,620
can look at the errors it makes,

379
00:12:43,630 --> 00:12:45,270
and do this sort of error analysis

380
00:12:45,280 --> 00:12:47,000
to see what mistakes it makes

381
00:12:47,010 --> 00:12:49,020
and use that to inspire further development.

382
00:12:49,030 --> 00:12:50,990
And second, assuming your

383
00:12:51,000 --> 00:12:53,610
quick and dirty implementation incorporated a

384
00:12:53,620 --> 00:12:55,930
single real number evaluation metric, this

385
00:12:55,940 --> 00:12:57,720
can then be a vehicle for

386
00:12:57,730 --> 00:12:59,800
you to try out different ideas

387
00:12:59,810 --> 00:13:01,020
and quickly see if the

388
00:13:01,030 --> 00:13:02,430
different ideas you're trying out

389
00:13:02,440 --> 00:13:03,910
are improving the performance of

390
00:13:03,920 --> 00:13:05,560
your algorithm and therefore let

391
00:13:05,570 --> 00:13:06,850
you maybe much more quickly

392
00:13:06,860 --> 00:13:08,750
make decisions about what things

393
00:13:08,760 --> 00:13:10,230
to fold, and what things to

394
00:13:10,240 --> 00:13:13,020
incorporate into your learning algorithm.

