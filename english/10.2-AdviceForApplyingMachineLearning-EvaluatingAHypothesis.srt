1
00:00:00,090 --> 00:00:01,220
In this video, I would

2
00:00:01,230 --> 00:00:02,550
like to talk about how to

3
00:00:02,560 --> 00:00:06,960
evaluate a hypothesis that has been learned by your algorithm.

4
00:00:06,970 --> 00:00:09,740
In later videos, we will build on this to talk about

5
00:00:09,750 --> 00:00:10,980
how to prevent in the

6
00:00:10,990 --> 00:00:12,690
problems of overfitting

7
00:00:12,700 --> 00:00:15,960
and underfitting as well.

8
00:00:15,970 --> 00:00:17,640
When we fit the parameters of

9
00:00:17,650 --> 00:00:19,760
our learning algorithm we think about

10
00:00:19,770 --> 00:00:23,350
choosing the parameters to minimize the training error.

11
00:00:23,360 --> 00:00:24,840
One might think that getting a

12
00:00:24,850 --> 00:00:27,190
really low value of training error might be a

13
00:00:27,200 --> 00:00:28,540
good thing, but we have

14
00:00:28,550 --> 00:00:30,060
already seen that just because

15
00:00:30,070 --> 00:00:31,370
a hypothesis has low

16
00:00:31,380 --> 00:00:35,410
training error, that doesn't mean it is necessarily a good hypothesis.

17
00:00:35,420 --> 00:00:37,230
And we've already seen the example

18
00:00:37,240 --> 00:00:40,770
of how a hypothesis can overfit.

19
00:00:40,840 --> 00:00:42,930
And therefore fail to generalize

20
00:00:42,940 --> 00:00:46,120
the new examples not in the training set.

21
00:00:46,130 --> 00:00:50,100
So how do you tell if the hypothesis might be overfitting.

22
00:00:50,110 --> 00:00:52,100
In this simple example we could

23
00:00:52,110 --> 00:00:54,140
plot the hypothesis h of

24
00:00:54,150 --> 00:00:56,610
x and just see what was going on.

25
00:00:56,620 --> 00:00:58,450
But in general for problems with

26
00:00:58,460 --> 00:00:59,870
more features than just one

27
00:00:59,880 --> 00:01:01,540
feature, for problems with a

28
00:01:01,550 --> 00:01:03,610
large number of features like these

29
00:01:03,620 --> 00:01:05,540
it becomes hard or may

30
00:01:05,550 --> 00:01:07,590
be impossible to plot what

31
00:01:07,600 --> 00:01:09,670
the hypothesis looks like

32
00:01:09,680 --> 00:01:12,870
and so we need some other way to evaluate our hypothesis.

33
00:01:12,880 --> 00:01:14,890
The standard way to evaluate

34
00:01:14,900 --> 00:01:17,470
a learned hypothesis is as follows.

35
00:01:17,480 --> 00:01:19,300
Suppose we have a data set like this.

36
00:01:19,310 --> 00:01:20,660
Here I have just shown 10

37
00:01:20,670 --> 00:01:24,040
training examples, but of course usually we may have

38
00:01:24,050 --> 00:01:27,410
dozens or hundreds or maybe thousands of training examples.

39
00:01:27,420 --> 00:01:28,540
In order to make sure we

40
00:01:28,550 --> 00:01:30,510
can evaluate our hypothesis, what

41
00:01:30,520 --> 00:01:32,890
we are going to do is split

42
00:01:32,900 --> 00:01:34,880
the data we have into two portions.

43
00:01:34,890 --> 00:01:37,020
The first portion is

44
00:01:37,030 --> 00:01:38,930
going to be our usual

45
00:01:38,940 --> 00:01:43,420
training set and the

46
00:01:43,430 --> 00:01:44,960
second portion is going to

47
00:01:44,970 --> 00:01:47,730
be our test set,

48
00:01:47,740 --> 00:01:50,470
and a pretty typical split of

49
00:01:50,480 --> 00:01:52,000
all the data we have into a

50
00:01:52,010 --> 00:01:53,690
training set and test set

51
00:01:53,700 --> 00:01:56,250
might be around say a 70%,

52
00:01:56,260 --> 00:01:57,960
30% split.

53
00:01:57,970 --> 00:01:59,220
Worth more today to grade

54
00:01:59,230 --> 00:02:02,270
the training set and relatively less to the test set.

55
00:02:02,280 --> 00:02:04,140
And so now, if

56
00:02:04,150 --> 00:02:06,000
we have some data set, we

57
00:02:06,010 --> 00:02:07,440
run a sine of say

58
00:02:07,450 --> 00:02:09,220
70% of the data

59
00:02:09,230 --> 00:02:10,720
to be our training set where

60
00:02:10,730 --> 00:02:12,710
here "m" is as usual

61
00:02:12,720 --> 00:02:14,770
our number of training examples and

62
00:02:14,780 --> 00:02:16,990
the remainder of our data

63
00:02:17,000 --> 00:02:19,340
might then be assigned to become our test set.

64
00:02:19,350 --> 00:02:20,710
And here, I'm going to use

65
00:02:20,720 --> 00:02:23,160
the notation m subscript test

66
00:02:23,170 --> 00:02:27,640
to denote the number of test examples.

67
00:02:28,350 --> 00:02:30,870
And so in general, this

68
00:02:30,880 --> 00:02:32,080
subscript test is going

69
00:02:32,090 --> 00:02:33,990
to denote examples that come

70
00:02:34,000 --> 00:02:35,720
from a test set so that

71
00:02:35,730 --> 00:02:39,460
x1 subscript test, y1 subscript

72
00:02:39,470 --> 00:02:41,480
test is my first

73
00:02:41,490 --> 00:02:42,990
test example which I guess

74
00:02:43,000 --> 00:02:46,960
in this example might be this example over here.

75
00:02:47,790 --> 00:02:50,000
So a fairly typical procedure

76
00:02:50,010 --> 00:02:51,680
for training and testing a

77
00:02:51,690 --> 00:02:53,920
learning algorithm, maybe linear regression,

78
00:02:53,930 --> 00:02:55,140
would be to first learn the

79
00:02:55,150 --> 00:02:57,800
parameter vector from the training data.

80
00:02:57,810 --> 00:02:59,360
And here the training data is only

81
00:02:59,370 --> 00:03:03,520
that first 70% of the dataset.

82
00:03:03,530 --> 00:03:05,480
Finally, one last detail

83
00:03:05,490 --> 00:03:07,160
whereas here I've drawn this

84
00:03:07,170 --> 00:03:08,570
as though the first 70% goes

85
00:03:08,580 --> 00:03:10,080
to the training set and

86
00:03:10,090 --> 00:03:12,590
the last 30% to the test set.

87
00:03:12,600 --> 00:03:15,500
If there is any sort of ordinary to the data.

88
00:03:15,510 --> 00:03:17,520
That should be better to send

89
00:03:17,530 --> 00:03:19,050
a random 70% of

90
00:03:19,060 --> 00:03:20,630
your data to the training set

91
00:03:20,640 --> 00:03:23,560
and a random 30% of your data to the test set.

92
00:03:23,570 --> 00:03:24,990
So if your data were

93
00:03:25,000 --> 00:03:26,620
already randomly sorted, you could

94
00:03:26,630 --> 00:03:28,790
just take the first 70% and

95
00:03:28,800 --> 00:03:30,480
last 30%  that if

96
00:03:30,490 --> 00:03:32,130
your data were not randomly ordered,

97
00:03:32,140 --> 00:03:33,570
it would be better to

98
00:03:33,580 --> 00:03:34,800
randomly shuffle or to

99
00:03:34,810 --> 00:03:37,850
randomly reorder the examples in your training set.

100
00:03:37,860 --> 00:03:38,940
Before you know sending the

101
00:03:38,950 --> 00:03:40,440
first 70% in the

102
00:03:40,450 --> 00:03:41,940
training set and the last

103
00:03:41,950 --> 00:03:44,060
30% of the test set.

104
00:03:44,070 --> 00:03:45,650
Here then is a fairly typical

105
00:03:45,660 --> 00:03:47,140
procedure for how you

106
00:03:47,150 --> 00:03:48,790
would train and test the

107
00:03:48,800 --> 00:03:50,850
learning algorithm and the learning regression.

108
00:03:50,860 --> 00:03:52,130
First, you learn the

109
00:03:52,140 --> 00:03:54,010
parameters theta from the

110
00:03:54,020 --> 00:03:55,390
training set so you minimize

111
00:03:55,400 --> 00:03:57,420
the usual training error objective j of

112
00:03:57,430 --> 00:03:59,140
theta, where j of theta

113
00:03:59,150 --> 00:04:02,000
here was defined using that

114
00:04:02,010 --> 00:04:03,520
70% of all the data you have.

115
00:04:03,530 --> 00:04:06,140
There is only the training data.

116
00:04:06,490 --> 00:04:08,690
And then you would compute the test error.

117
00:04:08,700 --> 00:04:10,140
And I am going to denote the test

118
00:04:10,150 --> 00:04:12,920
error as j subscript test.

119
00:04:12,930 --> 00:04:13,880
And so what you do is

120
00:04:13,890 --> 00:04:15,680
take your parameter theta that

121
00:04:15,690 --> 00:04:16,720
you have learned from the training

122
00:04:16,730 --> 00:04:17,940
set, and plug it in

123
00:04:17,950 --> 00:04:21,500
here and compute your test set error.

124
00:04:21,510 --> 00:04:25,270
Which I am going to write as follows.

125
00:04:25,590 --> 00:04:29,950
So this is basically the average

126
00:04:29,960 --> 00:04:33,460
squared error as measured on your test set.

127
00:04:33,470 --> 00:04:36,230
It's pretty much what you'd expect.

128
00:04:36,240 --> 00:04:37,430
So if we run every test

129
00:04:37,440 --> 00:04:40,040
example through your hypothesis

130
00:04:40,610 --> 00:04:42,820
with parameter theta and just measure

131
00:04:42,830 --> 00:04:44,580
the error

132
00:04:44,590 --> 00:04:46,590
that your hypothesis has on your

133
00:04:46,600 --> 00:04:50,380
m subscript test, test examples.

134
00:04:50,400 --> 00:04:52,020
And of course, this is the

135
00:04:52,030 --> 00:04:54,430
definition of the test set

136
00:04:54,440 --> 00:04:56,050
error if we are using

137
00:04:56,060 --> 00:04:58,030
linear regression and using

138
00:04:58,040 --> 00:05:01,140
the squared error metric.

139
00:05:06,700 --> 00:05:09,720
How about if we were doing a classification problem

140
00:05:09,730 --> 00:05:13,610
and say using logistic regression instead. In that

141
00:05:13,620 --> 00:05:15,830
case, the procedure for training

142
00:05:15,840 --> 00:05:19,120
and testing say logistic regression is pretty similar

143
00:05:19,290 --> 00:05:21,110
first we will do the parameters

144
00:05:21,120 --> 00:05:23,590
from the training data, that first 70% of the data.

145
00:05:23,600 --> 00:05:27,060
And it will compute the test error as follows.

146
00:05:27,070 --> 00:05:29,250
It's the same objective function

147
00:05:29,260 --> 00:05:30,480
as we always use but we

148
00:05:30,490 --> 00:05:32,170
just leave a ration, except

149
00:05:32,180 --> 00:05:34,120
that now is define using our

150
00:05:34,130 --> 00:05:37,400
m subscript test, test examples.

151
00:05:37,410 --> 00:05:38,860
While this definition of the

152
00:05:38,870 --> 00:05:42,400
test set error test is perfectly reasonable.

153
00:05:42,410 --> 00:05:44,500
Sometimes there is an alternative

154
00:05:44,510 --> 00:05:46,260
test sets metric that might

155
00:05:46,270 --> 00:05:49,260
be easier to interpret, and that's the misclassification error.

156
00:05:49,270 --> 00:05:50,810
It's also called the

157
00:05:50,820 --> 00:05:53,090
zero one misclassification error, with

158
00:05:53,100 --> 00:05:55,100
zero one denoting that you

159
00:05:55,110 --> 00:05:58,030
either get an example right or you get an example wrong.

160
00:05:58,040 --> 00:06:00,090
Here's what I mean.

161
00:06:00,100 --> 00:06:04,020
Let me define the error of a prediction.

162
00:06:04,510 --> 00:06:06,690
That is h of x. And

163
00:06:06,700 --> 00:06:10,660
given the label y as equal to

164
00:06:10,670 --> 00:06:14,030
one if my hypothesis

165
00:06:14,040 --> 00:06:15,690
outputs the value greater than

166
00:06:15,700 --> 00:06:17,530
equal to five and Y

167
00:06:17,540 --> 00:06:23,160
is equal to zero or if

168
00:06:23,170 --> 00:06:24,670
my hypothesis outputs a value

169
00:06:24,680 --> 00:06:26,360
of less than 0.5 and y is

170
00:06:26,370 --> 00:06:28,000
equal to one, right, so

171
00:06:28,010 --> 00:06:30,470
both of these cases basic respond

172
00:06:30,480 --> 00:06:33,900
to if your hypothesis mislabeled

173
00:06:33,910 --> 00:06:37,340
the example assuming your threshold at an 0.5.

174
00:06:37,350 --> 00:06:38,030
So either thought it was more

175
00:06:38,040 --> 00:06:39,090
likely to be 1, but

176
00:06:39,100 --> 00:06:41,070
it was actually 0, or your

177
00:06:41,080 --> 00:06:42,170
hypothesis stored

178
00:06:42,180 --> 00:06:43,230
was more likely to be

179
00:06:43,240 --> 00:06:45,930
0, but the label was actually 1.

180
00:06:45,940 --> 00:06:50,840
And otherwise, we define this error function to be zero.

181
00:06:50,850 --> 00:06:53,580
If your hypothesis

182
00:06:53,590 --> 00:06:57,480
basically classified the example y correctly.

183
00:06:57,660 --> 00:06:59,620
We could then define the

184
00:06:59,630 --> 00:07:01,940
test error, using the

185
00:07:01,950 --> 00:07:03,630
misclassification error metric to

186
00:07:03,640 --> 00:07:05,380
be one of

187
00:07:05,390 --> 00:07:07,640
the m tests of sum from

188
00:07:07,650 --> 00:07:10,090
i equals one to m

189
00:07:10,100 --> 00:07:12,100
subscript test of the

190
00:07:12,110 --> 00:07:15,730
error of h of

191
00:07:15,740 --> 00:07:18,990
x(i) test comma y(i).

192
00:07:19,000 --> 00:07:20,800
And so that's

193
00:07:20,810 --> 00:07:22,210
just my way of

194
00:07:22,220 --> 00:07:23,230
writing out that this is

195
00:07:23,240 --> 00:07:25,620
exactly the fraction of

196
00:07:25,630 --> 00:07:27,080
the examples in my test

197
00:07:27,090 --> 00:07:30,860
set that my hypothesis has mislabeled.

198
00:07:31,330 --> 00:07:32,740
And so that's the definition of

199
00:07:32,750 --> 00:07:34,380
the test set error using the

200
00:07:34,390 --> 00:07:36,550
misclassification error of the

201
00:07:36,560 --> 00:07:40,000
0, 1 misclassification metric. So that's

202
00:07:40,010 --> 00:07:42,140
the standard technique for evaluating

203
00:07:42,150 --> 00:07:45,100
how good a learned hypothesis is.

204
00:07:45,110 --> 00:07:46,600
In the next video, we will

205
00:07:46,610 --> 00:07:48,490
adapt these ideas to helping us

206
00:07:48,500 --> 00:07:50,800
do things like choose what features like the

207
00:07:50,810 --> 00:07:52,470
degree polynomial to use

208
00:07:52,480 --> 00:07:54,310
with the learning algorithm or choose

209
00:07:54,320 --> 00:07:57,610
the regularization parameter for learning algorithm.

