1
00:00:00,120 --> 00:00:01,700
If you run the learning algorithm

2
00:00:01,710 --> 00:00:02,830
and it doesn't do as well

3
00:00:02,840 --> 00:00:04,730
as you are hoping, almost all

4
00:00:04,740 --> 00:00:06,090
the time it will be because

5
00:00:06,100 --> 00:00:08,000
you have either a high bias

6
00:00:08,010 --> 00:00:09,850
problem or a high variance problem.

7
00:00:09,860 --> 00:00:11,120
In other words they're either an

8
00:00:11,130 --> 00:00:14,250
underfitting problem or an overfitting problem.

9
00:00:14,260 --> 00:00:15,340
And in this case it's very

10
00:00:15,350 --> 00:00:16,780
important to figure out

11
00:00:16,790 --> 00:00:18,270
which of these two problems is

12
00:00:18,280 --> 00:00:20,200
bias or variance or a bit of both that you

13
00:00:20,210 --> 00:00:21,040
actually have.

14
00:00:21,050 --> 00:00:22,430
Because knowing which of these

15
00:00:22,440 --> 00:00:24,050
two things is happening would give

16
00:00:24,060 --> 00:00:26,170
a very strong indicator for whether

17
00:00:26,180 --> 00:00:27,760
the useful and promising ways

18
00:00:27,770 --> 00:00:30,220
to try to improve your algorithm.

19
00:00:30,230 --> 00:00:31,370
In this video, I would like

20
00:00:31,380 --> 00:00:33,210
to delve more deeply into

21
00:00:33,220 --> 00:00:35,170
this bias and various issue and

22
00:00:35,180 --> 00:00:36,780
understand them better as well

23
00:00:36,790 --> 00:00:38,600
as figure out how to look

24
00:00:38,610 --> 00:00:43,020
at and evaluate knows whether or not we might have a bias problem or a variance problem.

25
00:00:43,030 --> 00:00:45,890
Since this would be critical to

26
00:00:45,900 --> 00:00:48,630
figuring out how to improve the performance of learning algorithm that you implement.

27
00:00:48,640 --> 00:00:52,670
So you've already

28
00:00:52,680 --> 00:00:54,180
seen this figure a few times,

29
00:00:54,190 --> 00:00:55,700
where if you fit two simple

30
00:00:55,710 --> 00:00:59,400
hypothesis, like a straight line that that underfits the data.

31
00:00:59,660 --> 00:01:01,240
If you fit a two complex

32
00:01:01,250 --> 00:01:03,390
hypothesis, then that might

33
00:01:03,400 --> 00:01:05,260
fit the training set perfectly but

34
00:01:05,270 --> 00:01:06,920
overfit the data and this

35
00:01:06,930 --> 00:01:09,330
may be hypothesis of some

36
00:01:09,340 --> 00:01:11,800
intermediate level of complexity,

37
00:01:11,810 --> 00:01:13,380
of some, maybe degree two

38
00:01:13,390 --> 00:01:16,550
polynomials are not too low and not too high degree.

39
00:01:16,560 --> 00:01:17,550
That's just right.

40
00:01:17,560 --> 00:01:19,090
And gives you the best

41
00:01:19,100 --> 00:01:21,760
generalization error out of these options.

42
00:01:21,770 --> 00:01:23,020
Now that we're armed with the

43
00:01:23,030 --> 00:01:26,090
notion of training and validation

44
00:01:26,100 --> 00:01:28,280
in test sets, we can understand

45
00:01:28,290 --> 00:01:31,300
the concepts of bias and variance a little bit better.

46
00:01:31,310 --> 00:01:33,360
Concretely, let our

47
00:01:33,370 --> 00:01:35,040
training error and cross

48
00:01:35,050 --> 00:01:36,840
validation error be defined as

49
00:01:36,850 --> 00:01:38,670
in the previous videos, just say,

50
00:01:38,680 --> 00:01:40,440
the squared error, the average

51
00:01:40,450 --> 00:01:41,820
squared error as measured

52
00:01:41,830 --> 00:01:42,920
on the 20 sets or as

53
00:01:42,930 --> 00:01:46,210
measured on the cross validation set.

54
00:01:46,560 --> 00:01:48,460
Now let's plot the following figure.

55
00:01:48,470 --> 00:01:50,000
On the horizontal axis I am

56
00:01:50,010 --> 00:01:52,390
going to plot the degree of polynomial,

57
00:01:52,400 --> 00:01:54,800
so as I go the right

58
00:01:54,810 --> 00:01:58,550
I'm going to be fitting higher and higher order polynomials.

59
00:01:58,590 --> 00:01:59,800
So, we'll do that for this

60
00:01:59,810 --> 00:02:01,710
figure, where maybe d equals 1,

61
00:02:01,720 --> 00:02:03,680
were going to be fitting

62
00:02:03,690 --> 00:02:05,730
very simple functions where as

63
00:02:05,740 --> 00:02:07,140
we are the right of this

64
00:02:07,150 --> 00:02:09,730
this may be

65
00:02:09,740 --> 00:02:11,640
d equals 4 or relatively may

66
00:02:11,650 --> 00:02:14,110
be even larger numbers. I'm going to be fitting

67
00:02:14,120 --> 00:02:17,410
very complex high order polynomials that

68
00:02:17,420 --> 00:02:21,480
might fit the training set with much more complex functions

69
00:02:23,550 --> 00:02:26,880
whereas we're

70
00:02:26,890 --> 00:02:28,150
here on the right of the

71
00:02:28,160 --> 00:02:31,720
horizontal axis, I have much larger values of these

72
00:02:31,730 --> 00:02:34,450
of a much higher degree polynomial, and

73
00:02:34,460 --> 00:02:35,590
so here that is going

74
00:02:35,600 --> 00:02:37,750
to correspond to fitting much

75
00:02:37,760 --> 00:02:40,100
more complex functions to your

76
00:02:40,110 --> 00:02:42,000
training set.

Let's look at

77
00:02:42,010 --> 00:02:44,390
the training error and cause-validation error

78
00:02:44,400 --> 00:02:46,560
and plot them on this figure.

79
00:02:46,570 --> 00:02:49,810
Let's start with the training error.

80
00:02:49,820 --> 00:02:50,670
As we increase the degree of the

81
00:02:50,680 --> 00:02:53,250
polynomial, we're going to

82
00:02:53,260 --> 00:02:55,790
fit our training set better and better and so, if d equals 1

83
00:02:55,800 --> 00:02:57,310


84
00:02:57,320 --> 00:02:58,420
that ever rose to the high training error.

85
00:02:58,430 --> 00:02:59,190
If we have a

86
00:02:59,200 --> 00:03:00,800
very high degree of

87
00:03:00,810 --> 00:03:02,830
polynomial, our training error is going to be really low.

88
00:03:02,840 --> 00:03:05,840
Maybe even zero, because it will fit the training set really well.

89
00:03:05,850 --> 00:03:07,380
And so as we increase

90
00:03:07,390 --> 00:03:09,120
of the greater polynomial we find

91
00:03:09,130 --> 00:03:10,540
typically that the training

92
00:03:10,550 --> 00:03:11,950
error decreases, so I'm

93
00:03:11,960 --> 00:03:15,970
going to write j subscript

94
00:03:15,980 --> 00:03:18,200
train of theta there, because

95
00:03:18,210 --> 00:03:19,740
our training error tends to

96
00:03:19,750 --> 00:03:22,780
decrease with the degree

97
00:03:22,790 --> 00:03:25,400
of the polynomial that we fit to the data.

98
00:03:25,410 --> 00:03:28,290
Next, let's look at the cross validation error. Often that matter, if

99
00:03:28,300 --> 00:03:31,470
we look at the test set error

100
00:03:31,480 --> 00:03:33,500
we'll get a pretty similar result as

101
00:03:33,510 --> 00:03:36,220
if we were to plot the

102
00:03:36,710 --> 00:03:40,610
cross validation error. So, we know that if d equals 1, we're fitting

103
00:03:40,620 --> 00:03:42,330
a very simple function, and

104
00:03:42,340 --> 00:03:44,530
so we may be underfitting the

105
00:03:44,540 --> 00:03:45,700
training set, and so we're

106
00:03:45,710 --> 00:03:47,380
going to go very high cross-validation error.

107
00:03:47,390 --> 00:03:49,670
If we fit, you

108
00:03:49,680 --> 00:03:52,100
know, an intermediate degree polynomial; we

109
00:03:52,110 --> 00:03:54,080
have a d equals 2 in our

110
00:03:54,090 --> 00:03:55,380
example in the previous slide,

111
00:03:55,390 --> 00:03:56,240
we are going to have a

112
00:03:56,250 --> 00:03:57,560
much lower cross-validation error, because

113
00:03:57,570 --> 00:03:59,850
we are just fitting, finding

114
00:03:59,860 --> 00:04:02,160
a much better fit to the data.

115
00:04:02,170 --> 00:04:03,340
And conversely if d were

116
00:04:03,350 --> 00:04:04,530
too high, so if d

117
00:04:04,540 --> 00:04:06,280
took on say a value of

118
00:04:06,290 --> 00:04:07,720
four, then we're again

119
00:04:07,730 --> 00:04:08,940
overfitting and so we

120
00:04:08,950 --> 00:04:12,270
end up with a high value for cross-validation error.

121
00:04:12,280 --> 00:04:13,890
So if you were to vary

122
00:04:13,900 --> 00:04:15,380
this smoothly and plot a

123
00:04:15,390 --> 00:04:17,030
curve you might end up

124
00:04:17,040 --> 00:04:19,200
with a curve like that, where

125
00:04:19,210 --> 00:04:21,670
that's Jcv of theta,

126
00:04:21,680 --> 00:04:23,450
and again if you plot j

127
00:04:23,460 --> 00:04:27,120
test of theta you get something very similar.

128
00:04:27,130 --> 00:04:28,520
And so this sort of

129
00:04:28,530 --> 00:04:30,520
plot also helps us

130
00:04:30,530 --> 00:04:32,550
to better understand the notions

131
00:04:32,560 --> 00:04:35,660
of bias and variance. Concretely, if you

132
00:04:35,670 --> 00:04:37,230
have a learning algorithm that's

133
00:04:37,240 --> 00:04:39,050
not performing as well as

134
00:04:39,060 --> 00:04:41,050
you wanted it to, how

135
00:04:41,060 --> 00:04:44,910
can you figure out if your learning algorithm is suffering.

136
00:04:44,920 --> 00:04:46,770
Concretly, suppose you have applied a

137
00:04:46,780 --> 00:04:48,240
learning algorithm and it is

138
00:04:48,250 --> 00:04:49,920
not performing as well

139
00:04:49,930 --> 00:04:52,230
as your are hoping, so your

140
00:04:52,240 --> 00:04:55,950
cross-validation set error or your test set error is high.

141
00:04:55,960 --> 00:04:56,940
How can we figure out if

142
00:04:56,950 --> 00:04:58,570
the learning algorithm is suffering

143
00:04:58,580 --> 00:05:02,570
from high bias or if it is suffering from high variance.

144
00:05:02,580 --> 00:05:04,130
So the setting of a cross-validation

145
00:05:04,140 --> 00:05:07,140
error being high corresponds to

146
00:05:07,150 --> 00:05:10,460
either this regime or this regime.

147
00:05:10,470 --> 00:05:11,700
So this regime on the

148
00:05:11,710 --> 00:05:13,740
left corresponds to a

149
00:05:13,750 --> 00:05:15,670
high bias problem, that is,

150
00:05:15,680 --> 00:05:17,550
if you are fitting an overly

151
00:05:17,560 --> 00:05:19,270
low order polynomial such as

152
00:05:19,280 --> 00:05:21,160
a plus one, when we

153
00:05:21,170 --> 00:05:24,700
really needed a higher order polynomial to fit the data.

154
00:05:24,710 --> 00:05:26,840
Whereas in contrast, this regime

155
00:05:26,850 --> 00:05:29,830
corresponds to a high variance problem.

156
00:05:29,840 --> 00:05:32,780
That is, if d--the degree of polynomial--was

157
00:05:32,820 --> 00:05:35,980
too large for the data set that we have.

158
00:05:35,990 --> 00:05:37,730
And this figure gives us

159
00:05:37,740 --> 00:05:41,270
a clue for how to distinguish between these two cases.

160
00:05:41,280 --> 00:05:43,130
Concretely, for the high

161
00:05:43,140 --> 00:05:45,960
bias case, that is,

162
00:05:45,970 --> 00:05:47,750
the case of under fitting, what

163
00:05:47,760 --> 00:05:50,220
we find is that both

164
00:05:50,230 --> 00:05:52,200
the cross validation error and

165
00:05:52,210 --> 00:05:54,980
the training error are going to be high.

166
00:05:54,990 --> 00:05:56,210
So, if your algorithm is

167
00:05:56,220 --> 00:05:58,910
suffering from a bias problem,

168
00:05:59,550 --> 00:06:02,950
the training set error

169
00:06:03,080 --> 00:06:06,060
would be high and you

170
00:06:06,070 --> 00:06:07,860
may find that the cross

171
00:06:07,870 --> 00:06:11,670
validation error will also be high.

172
00:06:11,680 --> 00:06:14,690
It might be close, maybe

173
00:06:14,700 --> 00:06:17,090
just slightly higher then a training error.

174
00:06:17,100 --> 00:06:19,230
And so, if you see this combination,

175
00:06:19,240 --> 00:06:20,990
that's a sign that your algorithm

176
00:06:21,000 --> 00:06:23,400
may be suffering from high bias.

177
00:06:23,410 --> 00:06:25,840
In contrast; if

178
00:06:25,850 --> 00:06:27,200
your algorithm is suffering from high

179
00:06:27,210 --> 00:06:30,700
variance; then, if you look here,

180
00:06:30,710 --> 00:06:33,720
we'll notice that, J

181
00:06:33,730 --> 00:06:35,310
train, that is the training

182
00:06:35,320 --> 00:06:38,720
error, is going to be low.

183
00:06:39,480 --> 00:06:43,200
That is, you're fitting the training set very well.

184
00:06:43,210 --> 00:06:48,270
Whereas, your cross validation error, assuming

185
00:06:48,280 --> 00:06:50,280
that this say the squared

186
00:06:50,290 --> 00:06:51,650
error which we're trying to minimize.

187
00:06:51,660 --> 00:06:53,980
Whereas in contrast; your

188
00:06:53,990 --> 00:06:55,630
error on a cross validation

189
00:06:55,640 --> 00:06:57,110
set or your cross function like cross

190
00:06:57,120 --> 00:06:58,740
validation set, will be

191
00:06:58,750 --> 00:07:02,850
much bigger than your training set error.

192
00:07:02,860 --> 00:07:04,670
This double greater than sign,

193
00:07:04,680 --> 00:07:07,340
here, it means much bigger than, all right. So, it's much greater than to multiply great to great.

194
00:07:07,350 --> 00:07:09,390


195
00:07:10,480 --> 00:07:12,100
So this is a double greater

196
00:07:12,110 --> 00:07:13,260
than sign, that is the

197
00:07:13,270 --> 00:07:14,900
map symbol for much greater

198
00:07:14,910 --> 00:07:18,480
than denoted by two greater than signs.

199
00:07:18,500 --> 00:07:19,570
And so if you see this

200
00:07:19,580 --> 00:07:21,540
combination, then what you

201
00:07:21,550 --> 00:07:29,570
find. And so if you see this combination of values, then

202
00:07:29,580 --> 00:07:31,390
that is a clue that

203
00:07:31,400 --> 00:07:33,350
your learning algorithm may be suffering

204
00:07:33,360 --> 00:07:36,370
from high variance and might be overfitting.

205
00:07:36,380 --> 00:07:38,060
And the key that distinguishes these two

206
00:07:38,070 --> 00:07:39,400
cases is if you

207
00:07:39,410 --> 00:07:41,520
have a high bias problem your

208
00:07:41,530 --> 00:07:42,950
training set error will also

209
00:07:42,960 --> 00:07:44,040
be high as your

210
00:07:44,050 --> 00:07:46,930
hypothesis just not fitting the training set well.

211
00:07:46,940 --> 00:07:47,930
And if you have a high

212
00:07:47,940 --> 00:07:49,770
variance problem, your training

213
00:07:49,780 --> 00:07:51,350
set error will usually be low,

214
00:07:51,360 --> 00:07:55,230
that is much lower than the cross validation error.

215
00:07:55,780 --> 00:07:57,090
So, hopefully that gives you

216
00:07:57,100 --> 00:07:58,900
a somewhat better understanding of the

217
00:07:58,910 --> 00:08:01,270
two problems of bias and variance.

218
00:08:01,280 --> 00:08:02,350
I still have a lot more

219
00:08:02,360 --> 00:08:05,400
to say about bias and variance in the next few videos.

220
00:08:05,410 --> 00:08:06,830
But what we will see later; is

221
00:08:06,840 --> 00:08:08,510
that by diagnosing, whether a learning

222
00:08:08,520 --> 00:08:11,890
algorithm may be suffering from high bias or a high variance.

223
00:08:11,900 --> 00:08:15,590
I'll show you even more details on how to do that in later videos.

224
00:08:15,600 --> 00:08:17,150
We'll see that by figuring out

225
00:08:17,160 --> 00:08:18,730
whether a learning algorithm may be

226
00:08:18,740 --> 00:08:20,750
suffering from high bias or

227
00:08:20,760 --> 00:08:22,520
a combination of both that

228
00:08:22,530 --> 00:08:23,510
that would give us much better

229
00:08:23,520 --> 00:08:24,780
guidance for what might be

230
00:08:24,790 --> 00:08:26,120
promising things to try

231
00:08:26,130 --> 00:08:29,690
in order to improve the performance of the learning algorithm.

