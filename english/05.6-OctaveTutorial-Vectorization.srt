1
00:00:00,280 --> 00:00:04,410
In this video, I'd like to tell you about the idea of vectorization.

2
00:00:04,480 --> 00:00:06,430
So, whether you're using Octave

3
00:00:06,440 --> 00:00:08,240
or a similar language like MATLAB

4
00:00:08,250 --> 00:00:09,570
or whether you're using Python

5
00:00:09,580 --> 00:00:12,420
and NumPy or Java CC++.

6
00:00:12,500 --> 00:00:14,780
All of these languages have either

7
00:00:14,790 --> 00:00:16,710
built into them or have

8
00:00:16,720 --> 00:00:19,410
readily and easily accessible, different

9
00:00:19,420 --> 00:00:21,810
numerical linear algebra libraries.

10
00:00:21,820 --> 00:00:23,250
They're usually very well written,

11
00:00:23,260 --> 00:00:25,650
highly optimized, often so that developed by

12
00:00:25,660 --> 00:00:29,170
people that, you know, have PhDs in numerical computing or

13
00:00:29,180 --> 00:00:32,040
they are really specializing numerical computing.

14
00:00:32,050 --> 00:00:33,950
And when you're implementing machine

15
00:00:33,960 --> 00:00:35,920
learning algorithms, if you're able

16
00:00:35,930 --> 00:00:37,800
to take advantage of these

17
00:00:37,810 --> 00:00:39,300
linear algebra libraries or these

18
00:00:39,310 --> 00:00:41,610
numerical linear algebra libraries and

19
00:00:41,620 --> 00:00:43,360
mix the routine calls to them

20
00:00:43,370 --> 00:00:45,170
rather than sort of right call

21
00:00:45,180 --> 00:00:48,030
yourself to do things that these libraries could be doing.

22
00:00:48,040 --> 00:00:49,550
If you do that then

23
00:00:49,560 --> 00:00:51,870
often you get that "first is more efficient".

24
00:00:51,880 --> 00:00:53,120
So, just run more quickly and

25
00:00:53,130 --> 00:00:54,860
take better advantage of

26
00:00:54,870 --> 00:00:56,610
any parallel hardware your computer

27
00:00:56,620 --> 00:00:58,260
may have and so on.

28
00:00:58,270 --> 00:01:00,530
And second, it also means

29
00:01:00,540 --> 00:01:03,040
that you end up with less code that you need to write.

30
00:01:03,050 --> 00:01:04,930
So have a simpler implementation

31
00:01:04,940 --> 00:01:08,540
that is, therefore, maybe also more likely to be bug free.

32
00:01:08,550 --> 00:01:10,560
And as a concrete example.

33
00:01:10,570 --> 00:01:12,710
Rather than writing code

34
00:01:12,720 --> 00:01:14,940
yourself to multiply matrices, if

35
00:01:14,950 --> 00:01:16,240
you let Octave do it by

36
00:01:16,250 --> 00:01:18,130
typing a times b,

37
00:01:18,140 --> 00:01:19,800
that will use a very efficient

38
00:01:19,810 --> 00:01:22,330
routine to multiply the 2 matrices.

39
00:01:22,340 --> 00:01:24,000
And there's a bunch of examples like

40
00:01:24,010 --> 00:01:27,180
these where you use appropriate vectorized implementations.

41
00:01:27,190 --> 00:01:30,270
You get much simpler code, and much more efficient code.

42
00:01:30,280 --> 00:01:32,340
Let's look at some examples.

43
00:01:33,050 --> 00:01:34,900
Here's a usual hypothesis of linear

44
00:01:34,910 --> 00:01:36,330
regression and if you

45
00:01:36,340 --> 00:01:37,310
want to compute H of

46
00:01:37,320 --> 00:01:40,010
X, notice that there is a sum on the right.

47
00:01:40,020 --> 00:01:41,090
And so one thing you could

48
00:01:41,100 --> 00:01:42,760
do is compute the sum

49
00:01:42,770 --> 00:01:46,610
from J equals 0 to J equals N yourself.

50
00:01:46,620 --> 00:01:47,990
Another way to think of this

51
00:01:48,000 --> 00:01:49,190
is to think of h

52
00:01:49,200 --> 00:01:51,990
of x as theta transpose x

53
00:01:52,000 --> 00:01:53,220
and what you can do is

54
00:01:53,230 --> 00:01:55,650
think of this as you know, computing this

55
00:01:55,660 --> 00:01:57,830
in a product between 2 vectors

56
00:01:57,840 --> 00:02:00,080
where theta is, you know, your

57
00:02:00,090 --> 00:02:01,790
vector say theta 0, theta 1,

58
00:02:01,800 --> 00:02:04,800
theta 2 if you have 2 features.

59
00:02:04,810 --> 00:02:06,440
If n equals 2 and if

60
00:02:06,450 --> 00:02:08,100
you think of x as this

61
00:02:08,110 --> 00:02:13,120
vector, x0, x1, x2

62
00:02:13,130 --> 00:02:14,500
and these 2 views can

63
00:02:14,510 --> 00:02:17,470
give you 2 different implementations.

64
00:02:17,560 --> 00:02:18,770
Here's what I mean.

65
00:02:18,780 --> 00:02:21,030
Here's an unvectorized implementation for

66
00:02:21,040 --> 00:02:22,390
how to compute h of

67
00:02:22,400 --> 00:02:26,120
x and by unvectorized I mean, without vectorization.

68
00:02:26,130 --> 00:02:29,460
We might first initialize, you know, prediction to be 0.0.

69
00:02:29,470 --> 00:02:32,340
This is going to eventually, the

70
00:02:32,350 --> 00:02:34,290
prediction is going to be

71
00:02:34,300 --> 00:02:36,040
h of x and then

72
00:02:36,050 --> 00:02:37,260
I'm going to have a for loop for

73
00:02:37,270 --> 00:02:38,330
j equals one through n+1

74
00:02:38,340 --> 00:02:40,760
prediction gets incremented by

75
00:02:40,770 --> 00:02:41,760
theta j times xj.

76
00:02:41,770 --> 00:02:44,690
So, it's kind of this expression over here.

77
00:02:44,700 --> 00:02:47,190
By the way, I should mention in these

78
00:02:47,200 --> 00:02:48,890
vectors right over here, I

79
00:02:48,900 --> 00:02:51,100
had these vectors being 0 index.

80
00:02:51,110 --> 00:02:52,550
So, I had theta 0 theta 1,

81
00:02:52,560 --> 00:02:54,290
theta 2, but because MATLAB

82
00:02:54,300 --> 00:02:56,660
is one index, theta 0

83
00:02:56,670 --> 00:02:57,980
in MATLAB, we might

84
00:02:57,990 --> 00:03:00,170
end up representing as theta

85
00:03:00,180 --> 00:03:02,030
1 and this second element

86
00:03:02,040 --> 00:03:04,380
ends up as theta

87
00:03:04,390 --> 00:03:05,870
2 and this third element

88
00:03:05,880 --> 00:03:07,980
may end up as theta

89
00:03:07,990 --> 00:03:09,950
3 just because vectors in

90
00:03:09,960 --> 00:03:11,960
MATLAB are indexed starting

91
00:03:11,970 --> 00:03:13,510
from 1 even though our real

92
00:03:13,520 --> 00:03:15,440
theta and x here starting,

93
00:03:15,450 --> 00:03:16,990
indexing from 0, which

94
00:03:17,000 --> 00:03:18,750
is why here I have a for loop

95
00:03:18,760 --> 00:03:20,480
j goes from 1 through n+1

96
00:03:20,490 --> 00:03:22,140
rather than j go through

97
00:03:22,150 --> 00:03:26,290
0 up to n, right? But

98
00:03:26,300 --> 00:03:27,780
so, this is an

99
00:03:27,790 --> 00:03:29,490
unvectorized implementation in that we

100
00:03:29,500 --> 00:03:31,330
have a for loop that summing up

101
00:03:31,340 --> 00:03:34,040
the n elements of the sum.

102
00:03:34,050 --> 00:03:35,620
In contrast, here's how you

103
00:03:35,630 --> 00:03:38,400
write a vectorized implementation which

104
00:03:38,410 --> 00:03:39,930
is that you would think

105
00:03:39,940 --> 00:03:42,560
of x and theta

106
00:03:42,570 --> 00:03:43,940
as vectors, and you just set

107
00:03:43,950 --> 00:03:46,010
prediction equals theta transpose

108
00:03:46,020 --> 00:03:48,350
times x. You're just computing like so.

109
00:03:48,360 --> 00:03:50,980
Instead of writing all these

110
00:03:50,990 --> 00:03:52,900
lines of code with the for loop,

111
00:03:52,910 --> 00:03:54,190
you instead have one line

112
00:03:54,200 --> 00:03:56,620
of code and what this

113
00:03:56,630 --> 00:03:57,520
line of code on the right

114
00:03:57,530 --> 00:03:59,180
will do is it use

115
00:03:59,190 --> 00:04:01,830
Octaves highly optimized numerical

116
00:04:01,840 --> 00:04:03,790
linear algebra routines to compute

117
00:04:03,800 --> 00:04:06,220
this inner product between the

118
00:04:06,230 --> 00:04:08,180
two vectors, theta and X.

And not

119
00:04:08,190 --> 00:04:10,170
only is the vectorized implementation

120
00:04:10,180 --> 00:04:14,210
simpler, it will also run more efficiently.

121
00:04:15,820 --> 00:04:17,770
So, that was Octave, but

122
00:04:17,780 --> 00:04:19,910
issue of vectorization applies to

123
00:04:19,920 --> 00:04:22,030
other programming languages as well.

124
00:04:22,040 --> 00:04:24,920
Let's look at an example in C++.

125
00:04:24,930 --> 00:04:27,900
Here's what an unvectorized implementation might look like.

126
00:04:27,910 --> 00:04:31,330
We again initialize prediction, you know, to

127
00:04:31,340 --> 00:04:32,470
0.0 and then we now have a full

128
00:04:32,480 --> 00:04:34,490
loop for J0 up to

129
00:04:34,500 --> 00:04:36,820
n.  Prediction + equals

130
00:04:36,830 --> 00:04:38,550
theta j times x j where

131
00:04:38,560 --> 00:04:42,740
again, you have this x + for loop that you write yourself.

132
00:04:42,750 --> 00:04:44,840
In contrast, using a good

133
00:04:44,850 --> 00:04:46,460
numerical linear algebra library in

134
00:04:46,470 --> 00:04:48,980
C++, you could use

135
00:04:48,990 --> 00:04:54,550
write the function like or rather.

136
00:04:54,560 --> 00:04:56,450
In contrast, using a good

137
00:04:56,460 --> 00:04:58,130
numerical linear algebra library in

138
00:04:58,140 --> 00:05:00,670
C++, you can instead

139
00:05:00,680 --> 00:05:02,450
write code that might look like this.

140
00:05:02,460 --> 00:05:03,940
So, depending on the details

141
00:05:03,950 --> 00:05:05,520
of your numerical linear algebra

142
00:05:05,530 --> 00:05:06,820
library, you might be

143
00:05:06,830 --> 00:05:08,570
able to have an object that

144
00:05:08,580 --> 00:05:09,890
is a C++ object which is

145
00:05:09,900 --> 00:05:11,340
vector theta and a C++

146
00:05:11,350 --> 00:05:13,410
object which is a vector X,

147
00:05:13,420 --> 00:05:15,500
and you just take theta dot

148
00:05:15,510 --> 00:05:18,110
transpose times x where

149
00:05:18,120 --> 00:05:20,050
this times becomes C++ to

150
00:05:20,060 --> 00:05:21,960
overload the operator so

151
00:05:21,970 --> 00:05:26,130
that you can just multiply these two vectors in C++.

152
00:05:26,140 --> 00:05:28,100
And depending on, you know,  the details

153
00:05:28,110 --> 00:05:29,460
of your numerical and linear algebra

154
00:05:29,470 --> 00:05:30,810
library, you might end

155
00:05:30,820 --> 00:05:31,830
up using a slightly different and

156
00:05:31,840 --> 00:05:33,620
syntax, but by relying

157
00:05:33,630 --> 00:05:35,750
on a library to do this in a product.

158
00:05:35,760 --> 00:05:36,920
You can get a much simpler piece

159
00:05:36,930 --> 00:05:40,570
of code and a much more efficient one.

160
00:05:40,580 --> 00:05:43,570
Let's now look at a more sophisticated example.

161
00:05:43,580 --> 00:05:44,960
Just to remind you here's our

162
00:05:44,970 --> 00:05:46,720
update rule for gradient descent

163
00:05:46,730 --> 00:05:48,770
for linear regression and so,

164
00:05:48,780 --> 00:05:50,380
we update theta j using this

165
00:05:50,390 --> 00:05:53,650
rule for all values of J equals 0, 1, 2, and so on.

166
00:05:53,660 --> 00:05:56,250
And if I just write

167
00:05:56,260 --> 00:05:58,150
out these equations for

168
00:05:58,160 --> 00:06:00,020
theta 0 Theta one, theta two.

169
00:06:00,030 --> 00:06:02,140
Assuming we have two features.

170
00:06:02,150 --> 00:06:03,400
So N equals 2.

171
00:06:03,410 --> 00:06:04,600
Then these are the updates we

172
00:06:04,610 --> 00:06:07,400
perform to theta zero, theta one, theta two.

173
00:06:07,410 --> 00:06:08,920
where you might remember my

174
00:06:08,930 --> 00:06:10,800
saying in an earlier video

175
00:06:10,810 --> 00:06:14,160
that these should be simultaneous updates.

176
00:06:14,760 --> 00:06:16,200
So let's see if

177
00:06:16,210 --> 00:06:17,680
we can come up with a

178
00:06:17,690 --> 00:06:20,700
vectorized implementation of this.

179
00:06:20,740 --> 00:06:22,570
Here are my same 3 equations written

180
00:06:22,580 --> 00:06:24,160
on a slightly smaller font and you

181
00:06:24,170 --> 00:06:25,510
can imagine that 1 wait

182
00:06:25,520 --> 00:06:26,710
to implement this three lines

183
00:06:26,720 --> 00:06:27,750
of code is to have a

184
00:06:27,760 --> 00:06:28,930
for loop that says, you

185
00:06:28,940 --> 00:06:31,640
know, for j equals 0,

186
00:06:31,650 --> 00:06:33,290
1 through 2 the update

187
00:06:33,300 --> 00:06:35,590
theta J or something like that.

188
00:06:35,600 --> 00:06:36,700
But instead, let's come up

189
00:06:36,710 --> 00:06:40,960
with a vectorized implementation and see if we can have a simpler way.

190
00:06:40,970 --> 00:06:42,740
So, basically compress these three

191
00:06:42,750 --> 00:06:44,290
lines of code or a

192
00:06:44,300 --> 00:06:48,490
for loop that, you know, effectively does these 3 sets, 1 set at a time.

193
00:06:48,500 --> 00:06:49,650
Let's see who can these 3

194
00:06:49,660 --> 00:06:51,350
steps and compress them into

195
00:06:51,360 --> 00:06:54,190
1 line of vectorized code.

196
00:06:54,200 --> 00:06:55,470
Here's the idea.

197
00:06:55,480 --> 00:06:56,430
What I'm going to do is I'm

198
00:06:56,440 --> 00:06:59,110
going to think of theta

199
00:06:59,120 --> 00:07:00,600
as a vector and I'm

200
00:07:00,610 --> 00:07:03,760
going to update theta as theta

201
00:07:04,270 --> 00:07:07,450
minus alpha times some

202
00:07:07,460 --> 00:07:12,640
other vector, delta, where

203
00:07:12,650 --> 00:07:13,690
delta is going to be

204
00:07:13,700 --> 00:07:15,830
equal to 1 over

205
00:07:15,840 --> 00:07:18,440
m, sum from I equals

206
00:07:18,450 --> 00:07:22,170
one through m and then

207
00:07:22,180 --> 00:07:25,710
this term on the

208
00:07:25,720 --> 00:07:28,080
right, okay?

209
00:07:28,090 --> 00:07:31,090
So, let me explain what's going on here.

210
00:07:31,220 --> 00:07:32,600
Here, I'm going to treat

211
00:07:32,610 --> 00:07:35,340
theta as a vector

212
00:07:35,350 --> 00:07:38,100
so, there's an N+1 dimensional vector.

213
00:07:38,110 --> 00:07:40,300
I'm saying that theta gets, you know, updated

214
00:07:40,310 --> 00:07:43,760
as--that's the vector, our N+1.

215
00:07:43,920 --> 00:07:45,220
Alpha is a real

216
00:07:45,230 --> 00:07:47,400
number and delta

217
00:07:47,410 --> 00:07:49,950
here is a vector.

218
00:07:49,960 --> 00:07:54,360
So, this subtraction operation, that's a vector subtraction.

219
00:07:54,370 --> 00:07:54,820
Okay?

220
00:07:54,830 --> 00:07:56,950
Because alpha times delta

221
00:07:56,960 --> 00:07:58,340
is a vector and so

222
00:07:58,350 --> 00:08:00,350
I'm saying if theta gets, you know, this

223
00:08:00,360 --> 00:08:04,040
vector, alpha times delta subtracted from it.

224
00:08:04,240 --> 00:08:06,540
So, what is the vector delta?

225
00:08:06,550 --> 00:08:11,000
Well, this vector delta looks like this.

226
00:08:11,010 --> 00:08:12,080
And what this meant to

227
00:08:12,090 --> 00:08:14,610
be is really meant to be

228
00:08:14,620 --> 00:08:16,940
this thing over here.

229
00:08:17,140 --> 00:08:19,210
Concretely, delta will be

230
00:08:19,220 --> 00:08:22,150
a N+1 dimensional vector and

231
00:08:22,160 --> 00:08:23,890
the very first element of

232
00:08:23,900 --> 00:08:27,760
the vector delta is going to be equal to that.

233
00:08:27,770 --> 00:08:29,490
So, if we have

234
00:08:29,500 --> 00:08:31,510
the delta, you know, if we index it

235
00:08:31,520 --> 00:08:34,430
from 0--this is delta 0, delta 1, delta 2.

236
00:08:34,440 --> 00:08:36,550
What I want is that

237
00:08:36,560 --> 00:08:39,030
delta 0 is equal

238
00:08:39,040 --> 00:08:41,240
to, you know, this

239
00:08:41,250 --> 00:08:42,350
first box also green up

240
00:08:42,360 --> 00:08:45,250
above and indeed, you might

241
00:08:45,260 --> 00:08:47,080
be able to convince yourself that delta

242
00:08:47,090 --> 00:08:48,620
0 is this 1 of m,

243
00:08:48,630 --> 00:08:50,080
sum of, you know, h of

244
00:08:50,090 --> 00:08:53,390
x.   xi minus

245
00:08:53,400 --> 00:08:57,780
yi times xi0.

246
00:08:57,790 --> 00:08:59,710
So, let's just make

247
00:08:59,720 --> 00:09:00,970
sure that we're on the

248
00:09:00,980 --> 00:09:03,980
same page about how delta really is computed.

249
00:09:03,990 --> 00:09:05,430
Delta is one of m

250
00:09:05,440 --> 00:09:08,270
times the sum over here

251
00:09:08,280 --> 00:09:09,860
and, you know, what is this sum?

252
00:09:09,870 --> 00:09:11,400
Well, this term over

253
00:09:11,410 --> 00:09:16,400
here, that's a real number.

254
00:09:17,150 --> 00:09:21,170
And the second term over here, xi.

255
00:09:21,180 --> 00:09:23,900
This term over there is a

256
00:09:23,910 --> 00:09:26,330
vector, right? Because xi might

257
00:09:26,340 --> 00:09:26,980
be a vector.

258
00:09:26,990 --> 00:09:31,130
That would be

259
00:09:31,160 --> 00:09:34,800
xi0, xi1, xi2 right?

260
00:09:36,130 --> 00:09:38,190
And what is the summation?

261
00:09:38,200 --> 00:09:40,240
Well, what does summation say

262
00:09:40,250 --> 00:09:43,500
is that this term

263
00:09:45,640 --> 00:09:47,270
over here.

264
00:09:47,280 --> 00:09:54,860
This is equal to h+x1-y1 times

265
00:09:54,870 --> 00:09:59,850
x1 + h of

266
00:09:59,930 --> 00:10:02,740
x2-y2 times x2

267
00:10:02,750 --> 00:10:05,760
+ you know, and so on.

268
00:10:05,770 --> 00:10:05,990
Okay?

269
00:10:06,000 --> 00:10:07,350
Because this is a summation of

270
00:10:07,360 --> 00:10:08,950
the I. So, as I

271
00:10:08,960 --> 00:10:11,330
ranges from I1 through m,

272
00:10:11,340 --> 00:10:15,090
you get these different terms and you're summing up these terms.

273
00:10:15,160 --> 00:10:16,180
And the meaning of each of these

274
00:10:16,190 --> 00:10:18,170
terms is a lot like

275
00:10:18,180 --> 00:10:19,730
- if you remember actually from

276
00:10:19,740 --> 00:10:24,100
the earlier quiz in this, if you solve this equation.

277
00:10:24,110 --> 00:10:25,490
We said that in order to

278
00:10:25,500 --> 00:10:27,220
vectorize this code, we

279
00:10:27,230 --> 00:10:30,760
will instead set u2v+5w. So,

280
00:10:30,770 --> 00:10:32,370
we're saying that the vector u

281
00:10:32,380 --> 00:10:33,650
is equal to 2 times

282
00:10:33,660 --> 00:10:35,560
the vector v plus 5 times

283
00:10:35,570 --> 00:10:37,140
the vector w. So, just an

284
00:10:37,150 --> 00:10:38,900
example of how to

285
00:10:38,910 --> 00:10:42,430
add different vectors and this summation is the same thing.

286
00:10:42,440 --> 00:10:44,940
It's a saying that this

287
00:10:44,950 --> 00:10:49,820
summation over here is just some real number right?

288
00:10:49,840 --> 00:10:51,000
That's kind of like the number

289
00:10:51,010 --> 00:10:52,870
2 and some other number

290
00:10:52,880 --> 00:10:53,750
times the vector x1.

291
00:10:53,760 --> 00:10:56,780
This is like 2 times v instead

292
00:10:56,790 --> 00:10:59,120
with some other number times x1

293
00:10:59,130 --> 00:11:01,680
and then plus, you know, instead of

294
00:11:01,690 --> 00:11:03,460
5xw, we instead have some

295
00:11:03,470 --> 00:11:05,180
other real number plus some

296
00:11:05,190 --> 00:11:06,850
other vector and then you

297
00:11:06,860 --> 00:11:08,880
add on other vectors, you know,

298
00:11:08,890 --> 00:11:10,530
plus ... plus the other

299
00:11:10,540 --> 00:11:12,200
vectors, which is why

300
00:11:12,210 --> 00:11:15,140
overall, this thing

301
00:11:15,150 --> 00:11:17,000
over here, that whole

302
00:11:17,010 --> 00:11:19,760
quantity, that delta is

303
00:11:19,770 --> 00:11:23,670
just some vector, and concretely, the

304
00:11:23,680 --> 00:11:26,290
3 elements of delta correspond

305
00:11:26,300 --> 00:11:28,810
if n2, the 3 elements

306
00:11:28,820 --> 00:11:31,500
of delta correspond exactly to

307
00:11:31,510 --> 00:11:33,320
this thing to the second

308
00:11:33,330 --> 00:11:35,050
thing and this third

309
00:11:35,060 --> 00:11:36,400
thing, which is why

310
00:11:36,410 --> 00:11:38,260
when you update theta, according to

311
00:11:38,270 --> 00:11:41,000
theta minus alpha delta,

312
00:11:41,010 --> 00:11:42,820
we end up having exactly the

313
00:11:42,830 --> 00:11:44,950
same simultaneous updates as the

314
00:11:44,960 --> 00:11:47,830
update rules that we have on top.

315
00:11:47,840 --> 00:11:48,890
So, I know that there

316
00:11:48,900 --> 00:11:50,490
was a lot that happened on

317
00:11:50,500 --> 00:11:52,640
the slides, but again, feel

318
00:11:52,650 --> 00:11:54,500
free to pause the video and

319
00:11:54,510 --> 00:11:56,520
I either encourage you to

320
00:11:56,530 --> 00:11:58,230
step through the difference. If

321
00:11:58,240 --> 00:11:59,360
you're unsure of what just happen,

322
00:11:59,370 --> 00:12:01,680
I encourage you to step through

323
00:12:01,690 --> 00:12:02,860
the slide to make sure you

324
00:12:02,870 --> 00:12:04,570
understand why is it

325
00:12:04,580 --> 00:12:07,050
that this update here with

326
00:12:07,060 --> 00:12:09,740
this definition of delta, right?

327
00:12:09,750 --> 00:12:10,910
Why is it that that equal

328
00:12:10,920 --> 00:12:13,660
to this update on top and

329
00:12:13,670 --> 00:12:15,010
it's still not clear when insight is

330
00:12:15,020 --> 00:12:18,390
that, you know, this thing over here.

331
00:12:18,400 --> 00:12:20,590
That's exactly the vector

332
00:12:20,600 --> 00:12:22,080
x and so, we're

333
00:12:22,090 --> 00:12:23,270
just taking, you know, all

334
00:12:23,280 --> 00:12:25,480
3 of these computations and compressing

335
00:12:25,490 --> 00:12:27,040
them into one step

336
00:12:27,050 --> 00:12:29,760
with the this vector delta,

337
00:12:29,770 --> 00:12:31,260
which is why we can come

338
00:12:31,270 --> 00:12:33,480
up with a vectorized implementation of

339
00:12:33,490 --> 00:12:36,930
this step of linear regression this way.

340
00:12:36,940 --> 00:12:38,650
So I hope this

341
00:12:38,660 --> 00:12:40,600
step makes sense, and do

342
00:12:40,610 --> 00:12:41,720
look at the video and make

343
00:12:41,730 --> 00:12:43,980
sure and see if you can understand it.

344
00:12:43,990 --> 00:12:46,030
In case you don't understand The

345
00:12:46,040 --> 00:12:47,990
equivalence of this math if

346
00:12:48,000 --> 00:12:49,390
you implement this, this turns

347
00:12:49,400 --> 00:12:50,890
out to be the right answer anyway,

348
00:12:50,900 --> 00:12:52,190
so even if you didn't

349
00:12:52,200 --> 00:12:56,400
quite understand the equivalence, if you just implement it this way,

350
00:12:56,410 --> 00:12:58,950
you'll be able to get linear regressions to work.

351
00:12:58,960 --> 00:13:00,650
So, if you're able to

352
00:13:00,660 --> 00:13:02,190
figure out why these 2 steps

353
00:13:02,200 --> 00:13:04,100
are equivalent then hopefully that

354
00:13:04,110 --> 00:13:06,220
would give you a better understanding of vectorization

355
00:13:06,230 --> 00:13:10,080
as well, and finally,

356
00:13:10,090 --> 00:13:12,360
if you're implementing linear

357
00:13:12,370 --> 00:13:14,850
regression using more than one or two features.

358
00:13:14,860 --> 00:13:16,540
So, sometimes we use linear

359
00:13:16,550 --> 00:13:18,060
regression with tens or hundreds

360
00:13:18,070 --> 00:13:19,970
thousands of features, but if

361
00:13:19,980 --> 00:13:21,820
you use the vectorized implementation

362
00:13:21,830 --> 00:13:23,660
of linear regression, usually that

363
00:13:23,670 --> 00:13:25,580
will run much faster than if

364
00:13:25,590 --> 00:13:26,870
you had say your old

365
00:13:26,880 --> 00:13:28,140
for loop that was you

366
00:13:28,150 --> 00:13:31,490
know, updating theta 0 then theta 1 then theta 2 yourself.

367
00:13:31,500 --> 00:13:33,710
So, using a vectorized implementation, you

368
00:13:33,720 --> 00:13:34,570
should be able to get a

369
00:13:34,580 --> 00:13:37,780
much more efficient implementation of linear regression.

370
00:13:37,790 --> 00:13:39,330
And when you vectorize later

371
00:13:39,340 --> 00:13:40,350
algorithms that we'll see in

372
00:13:40,360 --> 00:13:41,490
this class is a good

373
00:13:41,500 --> 00:13:43,300
trick whether an octave

374
00:13:43,310 --> 00:13:44,740
or some of the language, the C++

375
00:13:44,750 --> 00:13:48,620
Java for getting your code to run more efficiently.

