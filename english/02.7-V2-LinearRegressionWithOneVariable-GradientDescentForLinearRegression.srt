1
00:00:00,460 --> 00:00:02,010
In previous videos, we talked

2
00:00:02,020 --> 00:00:04,330
about the gradient descent algorithm

3
00:00:04,340 --> 00:00:05,700
and talked about the linear

4
00:00:05,710 --> 00:00:09,440
regression model and the squared error cost function.

5
00:00:09,450 --> 00:00:10,690
In this video, we're going to

6
00:00:10,700 --> 00:00:12,700
put together gradient descent with

7
00:00:12,710 --> 00:00:14,650
our cost function, and that

8
00:00:14,660 --> 00:00:16,370
will give us an algorithm for

9
00:00:16,380 --> 00:00:20,000
linear regression for fitting a straight line to our data.

10
00:00:20,750 --> 00:00:22,660
So, this is

11
00:00:22,670 --> 00:00:24,890
what we worked out in the previous videos.

12
00:00:24,900 --> 00:00:27,220
That's our gradient descent algorithm, which

13
00:00:27,230 --> 00:00:29,010
should be familiar, and you

14
00:00:29,020 --> 00:00:30,700
see the linear linear regression model

15
00:00:30,710 --> 00:00:36,570
with our linear hypothesis and our squared error cost function.

16
00:00:36,580 --> 00:00:38,490
What we're going to do is apply

17
00:00:38,500 --> 00:00:42,460
gradient descent to minimize

18
00:00:44,500 --> 00:00:46,960
our squared error cost function.

19
00:00:47,730 --> 00:00:49,200
Now, in order to apply

20
00:00:49,210 --> 00:00:50,680
gradient descent, in order

21
00:00:50,690 --> 00:00:51,950
to write this piece of

22
00:00:51,960 --> 00:00:54,010
code, the key term

23
00:00:54,020 --> 00:00:58,360
we need is this derivative term over here.

24
00:00:59,520 --> 00:01:00,590
So, we need to figure out

25
00:01:00,600 --> 00:01:02,780
what is this partial derivative term,

26
00:01:02,790 --> 00:01:04,480
and plug in the

27
00:01:04,490 --> 00:01:06,380
definition of the cost

28
00:01:06,390 --> 00:01:08,310
function J, this turns

29
00:01:08,320 --> 00:01:13,040
out to be this "inaudible"

30
00:01:13,440 --> 00:01:15,670
equals 1-3 M of

31
00:01:15,680 --> 00:01:19,470
this squared error

32
00:01:20,090 --> 00:01:21,780
cost function term, and all

33
00:01:21,790 --> 00:01:23,460
I did here was I just

34
00:01:23,470 --> 00:01:25,140
you know plugged in the definition of

35
00:01:25,150 --> 00:01:27,940
the cost function there, and simplifying

36
00:01:27,950 --> 00:01:30,140
little bit more, this turns

37
00:01:30,150 --> 00:01:33,820
out to be equal to, this

38
00:01:33,830 --> 00:01:37,140
"inaudible" equals 1-3 M

39
00:01:37,150 --> 00:01:40,480
of course either one, XI

40
00:01:40,900 --> 00:01:43,270
minus YI squared.

41
00:01:43,280 --> 00:01:44,600
And all I did there was took

42
00:01:44,610 --> 00:01:47,420
the definition for my hypothesis

43
00:01:47,430 --> 00:01:48,890
and plug that in there.

44
00:01:48,900 --> 00:01:50,820
And it turns out we need

45
00:01:50,830 --> 00:01:52,020
to figure out what is

46
00:01:52,030 --> 00:01:53,330
the partial derivative of two

47
00:01:53,340 --> 00:01:55,680
cases for J equals

48
00:01:55,690 --> 00:01:57,240
0 and for J equals 1 want

49
00:01:57,250 --> 00:01:58,480
to figure out what is this

50
00:01:58,490 --> 00:02:00,590
partial derivative for both the

51
00:02:00,600 --> 00:02:03,920
theta(0) case and the theta(1) case.

52
00:02:03,930 --> 00:02:06,850
And I'm just going to write out the answers.

53
00:02:06,860 --> 00:02:10,690
It turns out this firstterm simplifies

54
00:02:10,700 --> 00:02:14,220
to 1/M, sum over

55
00:02:14,230 --> 00:02:16,790
my training set of just

56
00:02:16,800 --> 00:02:21,230
that, X(i)-  Y(i).

57
00:02:21,240 --> 00:02:23,610
And for this term, partial derivative

58
00:02:23,620 --> 00:02:26,120
with respect to theta(1), it turns

59
00:02:26,130 --> 00:02:31,850
out I get this term: -Y(i)*X(i).

60
00:02:32,270 --> 00:02:33,770
Okay.

61
00:02:36,080 --> 00:02:38,750
Andcomputing these partial

62
00:02:38,760 --> 00:02:40,980
derivatives, so going from

63
00:02:40,990 --> 00:02:44,400
this equation to either

64
00:02:44,410 --> 00:02:46,510
of these equations down there, computing

65
00:02:46,520 --> 00:02:51,110
those partial derivative terms requires some multivariate calculus.

66
00:02:51,120 --> 00:02:52,910
If you know calculus, feel free

67
00:02:52,920 --> 00:02:54,700
to work through the derivations yourself

68
00:02:54,710 --> 00:02:56,730
and check take the derivatives

69
00:02:56,740 --> 00:02:59,430
you actually get the answers that I got.

70
00:02:59,440 --> 00:03:00,620
But if you are less

71
00:03:00,630 --> 00:03:02,500
familiar with calculus you don't

72
00:03:02,510 --> 00:03:04,170
worry about it, and it

73
00:03:04,180 --> 00:03:06,110
is fine to take these equations

74
00:03:06,120 --> 00:03:07,800
worked out, and you

75
00:03:07,810 --> 00:03:09,220
won't need to know calculus or

76
00:03:09,230 --> 00:03:10,560
anything like that in order to

77
00:03:10,570 --> 00:03:14,500
do the homework, so to implement gradient descent you'd have to work.

78
00:03:14,610 --> 00:03:16,440
But so, after these definitions,

79
00:03:16,450 --> 00:03:17,990
or after what we've worked

80
00:03:18,000 --> 00:03:19,740
out to be the derivatives, which

81
00:03:19,750 --> 00:03:21,130
is really just the slope of

82
00:03:21,140 --> 00:03:23,480
the cost function j.  We

83
00:03:23,490 --> 00:03:25,020
can now plug them back into

84
00:03:25,030 --> 00:03:26,870
our gradient descent algorithm.

85
00:03:26,880 --> 00:03:28,670
So here's gradient descent, or

86
00:03:28,680 --> 00:03:29,970
the regression, which is going

87
00:03:29,980 --> 00:03:33,160
to repeat until convergence, theta 0

88
00:03:33,170 --> 00:03:35,360
and theta one get updated as,

89
00:03:35,370 --> 00:03:37,170
you know, the same minus alpha

90
00:03:37,180 --> 00:03:39,370
times the derivative term.

91
00:03:39,380 --> 00:03:42,140
So, this term here.

92
00:03:43,020 --> 00:03:47,010
So, here's our linear regression algorithm.

93
00:03:47,020 --> 00:03:52,800
This first term here that

94
00:03:52,810 --> 00:03:54,490
term is, of course, just

95
00:03:54,500 --> 00:03:56,060
a posh derivative of respective

96
00:03:56,070 --> 00:03:59,810
theta zero, that we worked on in the previous slide.

97
00:03:59,820 --> 00:04:01,890
And this second term here,

98
00:04:01,900 --> 00:04:04,390
that term is just

99
00:04:04,400 --> 00:04:06,110
a partial derivative with respect to

100
00:04:06,120 --> 00:04:11,370
theta one that we worked out on the previous line.

101
00:04:11,380 --> 00:04:13,290
And just as a quick reminder,

102
00:04:13,300 --> 00:04:15,300
you must, when implementing gradient descent,

103
00:04:15,310 --> 00:04:16,750
there's actually there's detail that, you

104
00:04:16,760 --> 00:04:18,680
know, you should be implementing

105
00:04:18,690 --> 00:04:22,800
it so the update theta zero and theta one simultaneously.

106
00:04:24,230 --> 00:04:28,000
So, let's see how gradient descent works.

107
00:04:28,010 --> 00:04:29,350
One of the issues we solved

108
00:04:29,360 --> 00:04:32,760
gradient descent is that it can be susceptible to local optima.

109
00:04:32,770 --> 00:04:34,080
So, when I first explained gradient

110
00:04:34,090 --> 00:04:35,910
descent, I showed you this picture

111
00:04:35,920 --> 00:04:37,430
of it, you know, going downhill

112
00:04:37,440 --> 00:04:38,770
on the surface and we

113
00:04:38,780 --> 00:04:40,050
saw how, depending on where

114
00:04:40,060 --> 00:04:43,470
you're initializing, you can end up with different local optima.

115
00:04:43,530 --> 00:04:45,410
You know, you can end up here or here.

116
00:04:45,420 --> 00:04:47,170
But, it turns out that

117
00:04:47,180 --> 00:04:49,420
the cost function for gradient

118
00:04:49,430 --> 00:04:51,710
of cost function for linear regression

119
00:04:51,720 --> 00:04:53,000
is always going to be

120
00:04:53,010 --> 00:04:55,960
a bow-shaped function like this.

121
00:04:55,970 --> 00:04:57,400
The technical term for this

122
00:04:57,410 --> 00:05:01,290
is that this is called a convex function.

123
00:05:03,230 --> 00:05:05,000
And I'm not going

124
00:05:05,010 --> 00:05:06,530
to give the formal definition for what

125
00:05:06,540 --> 00:05:09,590
is a convex function, c-o-n-v-e-x, but

126
00:05:09,600 --> 00:05:11,360
informally a convex function

127
00:05:11,370 --> 00:05:15,730
means a bow-shaped function, you know, kind of like a bow shaped.

128
00:05:15,740 --> 00:05:17,950
And so, this function doesn't

129
00:05:17,960 --> 00:05:20,250
have any local optima, except

130
00:05:20,260 --> 00:05:22,330
for the one global optimum.

131
00:05:22,340 --> 00:05:24,220
And does gradient descent on

132
00:05:24,230 --> 00:05:25,670
this type of cost function which

133
00:05:25,680 --> 00:05:27,450
you get whenever you're using linear

134
00:05:27,460 --> 00:05:29,270
regression, it will always convert

135
00:05:29,280 --> 00:05:33,020
to the global optimum, because there are no other local optima other than global optimum.

136
00:05:34,280 --> 00:05:37,720
So now, let's see this algorithm in action.

137
00:05:38,210 --> 00:05:40,040
As usual, zero plus of

138
00:05:40,050 --> 00:05:42,440
the hypothesis function and of

139
00:05:42,450 --> 00:05:45,870
my cost function J.

140
00:05:45,880 --> 00:05:47,060
And so, let's see how

141
00:05:47,070 --> 00:05:49,860
to initialize my parameters at this value.

142
00:05:49,870 --> 00:05:51,880
You know, let's say, usually you

143
00:05:51,890 --> 00:05:53,570
initialize your parameters at zero

144
00:05:53,580 --> 00:05:56,170
for zero, theta zero and zero.

145
00:05:56,180 --> 00:05:58,510
For illustration in this

146
00:05:58,520 --> 00:06:01,290
specific presentation, I have

147
00:06:01,300 --> 00:06:03,000
initialised theta zero at

148
00:06:03,010 --> 00:06:06,710
about 900, and theta one at about minus 0.1, okay?

149
00:06:06,720 --> 00:06:09,540
And so, this corresponds to H

150
00:06:09,550 --> 00:06:11,840
over X, equals, you know,

151
00:06:11,850 --> 00:06:13,370
minus 900 minus 0.1 x

152
00:06:13,380 --> 00:06:19,270
is this line, so out here on the cost function.

153
00:06:19,280 --> 00:06:20,420
Now if we take one

154
00:06:20,430 --> 00:06:22,190
step of gradient descent, we end

155
00:06:22,200 --> 00:06:24,150
up going from this point

156
00:06:24,160 --> 00:06:27,110
out here, a little

157
00:06:27,120 --> 00:06:29,310
bit to the down left

158
00:06:29,320 --> 00:06:31,350
to that second point over there.

159
00:06:31,360 --> 00:06:35,120
And, you notice that my line changed a little bit.

160
00:06:35,130 --> 00:06:36,350
And, as I take another step

161
00:06:36,360 --> 00:06:40,700
at gradient descent, my line on the left will change.

162
00:06:41,160 --> 00:06:41,450
Right.

163
00:06:41,460 --> 00:06:43,550
And I have also

164
00:06:43,560 --> 00:06:47,400
moved to a new point on my cost function.

165
00:06:47,600 --> 00:06:48,920
And as I think further step

166
00:06:48,930 --> 00:06:50,920
is gradient descent, I'm going

167
00:06:50,930 --> 00:06:52,970
down in cost, right, so

168
00:06:52,980 --> 00:06:55,080
my parameter is following

169
00:06:55,090 --> 00:06:57,950
this trajectory, and if

170
00:06:57,960 --> 00:07:00,520
you look on the left, this corresponds

171
00:07:00,530 --> 00:07:03,720
to hypotheses that seem

172
00:07:03,730 --> 00:07:04,870
to be getting to be

173
00:07:04,880 --> 00:07:06,060
better and better fits for the

174
00:07:06,070 --> 00:07:09,970
data until eventually,

175
00:07:10,010 --> 00:07:12,990
I have now wound up at the global minimum.

176
00:07:13,000 --> 00:07:16,110
And this global minimum corresponds to

177
00:07:16,120 --> 00:07:21,230
this hypothesis, which gives me a good fit to the data.

178
00:07:21,380 --> 00:07:23,650
And so that's gradient

179
00:07:23,660 --> 00:07:24,930
descent, and we've just run

180
00:07:24,940 --> 00:07:26,800
it and gotten a good

181
00:07:26,810 --> 00:07:31,050
fit to my data set of housing prices.

182
00:07:31,060 --> 00:07:34,040
And you can now use it to predict.

183
00:07:34,050 --> 00:07:35,160
You know, if your friend has a

184
00:07:35,170 --> 00:07:37,580
house with a

185
00:07:37,590 --> 00:07:38,980
size 1250 square feet, you

186
00:07:38,990 --> 00:07:40,400
can now read off the value

187
00:07:40,410 --> 00:07:41,970
and tell them that, I don't

188
00:07:41,980 --> 00:07:43,420
know, maybe they can get

189
00:07:43,430 --> 00:07:46,980
$350,000 for their house.

190
00:07:48,670 --> 00:07:49,780
Finally, just to give

191
00:07:49,790 --> 00:07:51,430
this another name, it turns out

192
00:07:51,440 --> 00:07:52,880
that the algorithm that we

193
00:07:52,890 --> 00:07:55,080
just went over is sometimes

194
00:07:55,090 --> 00:07:57,500
called batch gradient descent.

195
00:07:57,510 --> 00:07:58,730
And it turns out in machine

196
00:07:58,740 --> 00:08:00,370
learning, I feel like us machine

197
00:08:00,380 --> 00:08:01,950
learning people, we're not always

198
00:08:01,960 --> 00:08:04,240
created has given me some algorithms.

199
00:08:04,250 --> 00:08:06,600
But the term batch gradient descent

200
00:08:06,610 --> 00:08:07,930
means that refers to the

201
00:08:07,940 --> 00:08:09,440
fact that, in every step

202
00:08:09,450 --> 00:08:11,440
of gradient descent we're looking

203
00:08:11,450 --> 00:08:13,780
at all of the training examples.

204
00:08:13,790 --> 00:08:15,640
So, in gradient descent, you

205
00:08:15,650 --> 00:08:18,430
know, when computing derivatives, we're computing

206
00:08:18,440 --> 00:08:19,930
these sums, this sum of.

207
00:08:19,940 --> 00:08:22,100
So, in every separate

208
00:08:22,110 --> 00:08:23,220
gradient descent, we end up

209
00:08:23,230 --> 00:08:25,140
computing something like this, that

210
00:08:25,150 --> 00:08:28,410
sums over our M training examples.

211
00:08:28,420 --> 00:08:29,860
And so the term batch gradient

212
00:08:29,870 --> 00:08:31,150
descent refers to the fact

213
00:08:31,160 --> 00:08:33,010
when looking at the entire batch

214
00:08:33,020 --> 00:08:34,710
of training examples, and again,

215
00:08:34,720 --> 00:08:35,650
this is really, really not

216
00:08:35,660 --> 00:08:36,750
a great name, but this is

217
00:08:36,760 --> 00:08:39,560
what Mission Learning people call it.

218
00:08:39,570 --> 00:08:41,290
And it turns out there are

219
00:08:41,300 --> 00:08:42,790
sometimes other versions of

220
00:08:42,800 --> 00:08:44,070
gradient descent that are not

221
00:08:44,080 --> 00:08:46,320
back versions but instead do

222
00:08:46,330 --> 00:08:48,210
not look at the entire trading

223
00:08:48,220 --> 00:08:50,050
but look at small subsets

224
00:08:50,060 --> 00:08:51,730
of the training sets at the time,

225
00:08:51,740 --> 00:08:55,140
and we'll talk about those versions later in this course as well.

226
00:08:55,150 --> 00:08:56,100
But for now, using the algorithm

227
00:08:56,110 --> 00:08:57,250
you just learned, now we're

228
00:08:57,260 --> 00:08:59,240
using batch gradient descent, you

229
00:08:59,250 --> 00:09:01,300
now know how to implement

230
00:09:01,310 --> 00:09:04,480
gradient descent, or linear regression.

231
00:09:05,990 --> 00:09:09,450
So that's linear regression with gradient descent.

232
00:09:09,460 --> 00:09:11,750
If you've seen advanced linear algebra

233
00:09:11,760 --> 00:09:12,680
before so some you may

234
00:09:12,690 --> 00:09:14,130
have taken a class with advanced

235
00:09:14,140 --> 00:09:16,030
linear algebra, you might

236
00:09:16,040 --> 00:09:18,110
know that there exists a solution

237
00:09:18,120 --> 00:09:19,860
for numerically solving for the

238
00:09:19,870 --> 00:09:21,200
minimum of the cost function

239
00:09:21,210 --> 00:09:22,670
J, without needing to

240
00:09:22,680 --> 00:09:25,800
use and iterative algorithm like gradient descent.

241
00:09:25,810 --> 00:09:27,100
Later in this course we will

242
00:09:27,110 --> 00:09:28,200
talk about that method as

243
00:09:28,210 --> 00:09:29,740
well that just solves for the

244
00:09:29,750 --> 00:09:31,400
minimum cost function J without

245
00:09:31,410 --> 00:09:34,450
needing this multiple steps of gradient descent.

246
00:09:34,460 --> 00:09:37,680
That other method is called normal equations methods.

247
00:09:37,690 --> 00:09:38,950
And, but in case you

248
00:09:38,960 --> 00:09:40,210
have heard of that method, it turns

249
00:09:40,220 --> 00:09:41,890
out gradient descent will

250
00:09:41,900 --> 00:09:43,690
scale better to larger data

251
00:09:43,700 --> 00:09:45,320
sets than that normal equals

252
00:09:45,330 --> 00:09:47,300
method and, now that

253
00:09:47,310 --> 00:09:48,870
we know about gradient descent, we'll

254
00:09:48,880 --> 00:09:50,020
be able to use it in

255
00:09:50,030 --> 00:09:51,450
lots of different contexts, and we'll

256
00:09:51,460 --> 00:09:55,120
use it in lots of different Mission Learning problems as well.

257
00:09:55,240 --> 00:09:57,480
So, congrats on learning

258
00:09:57,490 --> 00:10:00,120
about your first Mission Learning algorithm.

259
00:10:00,130 --> 00:10:02,380
We'll later have exercises in

260
00:10:02,390 --> 00:10:03,470
which we'll ask you to

261
00:10:03,480 --> 00:10:05,020
implement gradient descent and

262
00:10:05,030 --> 00:10:07,410
hopefully see these algorithms work for yourselves.

263
00:10:07,420 --> 00:10:08,920
But before that I first

264
00:10:08,930 --> 00:10:10,300
want to tell you in

265
00:10:10,310 --> 00:10:11,500
the next set of videos, the

266
00:10:11,510 --> 00:10:13,200
first want to tell you about

267
00:10:13,210 --> 00:10:15,170
a generalization of the gradient descent

268
00:10:15,180 --> 00:10:16,440
algorithm that will make

269
00:10:16,450 --> 00:10:18,130
it much more powerful and I

270
00:10:18,140 --> 00:10:21,700
guess I will tell you about that in the next video.

