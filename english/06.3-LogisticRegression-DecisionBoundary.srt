1
00:00:00,980 --> 00:00:02,370
In the last video, we talked

2
00:00:02,380 --> 00:00:06,690
about the hypothesis representation for logistic progression.

3
00:00:06,700 --> 00:00:07,920
What I'd like to do now is

4
00:00:07,930 --> 00:00:09,360
tell you about something called the

5
00:00:09,370 --> 00:00:11,370
decision boundary, and this

6
00:00:11,380 --> 00:00:12,860
will give us a better sense

7
00:00:12,870 --> 00:00:15,020
of what the logistic regression

8
00:00:15,030 --> 00:00:17,770
hypothesis function is computing.

9
00:00:17,780 --> 00:00:19,980
To recap, this is

10
00:00:19,990 --> 00:00:21,270
what we wrote out last time,

11
00:00:21,280 --> 00:00:22,620
where we said that the

12
00:00:22,630 --> 00:00:24,920
hypothesis is represented as

13
00:00:24,930 --> 00:00:26,100
H of X equals G of

14
00:00:26,110 --> 00:00:28,340
theta transpose X, where G

15
00:00:28,350 --> 00:00:29,850
is this function called the

16
00:00:29,860 --> 00:00:32,740
sigmoid function, which looks like this.

17
00:00:32,750 --> 00:00:35,100
So, it slowly increases from zero

18
00:00:35,110 --> 00:00:38,620
to one, asymptoting at one.

19
00:00:38,890 --> 00:00:40,340
What I want to do now is

20
00:00:40,350 --> 00:00:42,430
try to understand better when

21
00:00:42,440 --> 00:00:44,060
this hypothesis will make

22
00:00:44,070 --> 00:00:45,310
predictions that Y is

23
00:00:45,320 --> 00:00:47,000
equal to one versus when it

24
00:00:47,010 --> 00:00:48,320
might make predictions that Y

25
00:00:48,330 --> 00:00:50,620
is equal to zero and understand

26
00:00:50,630 --> 00:00:52,330
better what the hypothesis function

27
00:00:52,340 --> 00:00:56,630
looks like, particularly when we have more than one feature.

28
00:00:56,640 --> 00:00:59,030
Concretely, this hypothesis is

29
00:00:59,040 --> 00:01:00,800
out putting estimates of the

30
00:01:00,810 --> 00:01:02,050
probability that Y is

31
00:01:02,060 --> 00:01:05,240
equal to one given X is prime.

32
00:01:05,530 --> 00:01:06,770
So if we wanted to

33
00:01:06,780 --> 00:01:08,140
predict is Y equal to

34
00:01:08,150 --> 00:01:09,430
one or is Y equal

35
00:01:09,440 --> 00:01:12,230
to zero here's something we might do.

36
00:01:12,240 --> 00:01:14,680
When ever the hypothesis its that

37
00:01:14,690 --> 00:01:16,310
the problem with y being one

38
00:01:16,320 --> 00:01:17,550
is greater than or equal

39
00:01:17,560 --> 00:01:19,340
to 0.5 so this means

40
00:01:19,350 --> 00:01:21,010
that it is more likely to

41
00:01:21,020 --> 00:01:22,220
be y equals one than y

42
00:01:22,230 --> 00:01:26,400
equals zero then let's predict Y equals one.

43
00:01:26,410 --> 00:01:27,950
And otherwise, if the probability

44
00:01:27,960 --> 00:01:30,170
of, the estimated probability of

45
00:01:30,180 --> 00:01:31,820
Y being one is less

46
00:01:31,830 --> 00:01:34,990
than 0.5, then let's predict Y equals zero.

47
00:01:35,000 --> 00:01:36,260
And I chose a greater

48
00:01:36,270 --> 00:01:39,660
than or equal to here and less than here.

49
00:01:39,670 --> 00:01:40,970
If H of X is equal

50
00:01:40,980 --> 00:01:43,050
to 0.5 exactly, then

51
00:01:43,060 --> 00:01:44,660
we could predict positive or

52
00:01:44,670 --> 00:01:45,740
negative vector but a put a

53
00:01:45,750 --> 00:01:47,440
great deal on to here

54
00:01:47,450 --> 00:01:49,190
so we default maybe to predicting

55
00:01:49,200 --> 00:01:51,440
a positive if your

56
00:01:51,450 --> 00:01:52,790
vector is 0.5 but that's

57
00:01:52,800 --> 00:01:56,160
a detail that really doesn't matter that much.

58
00:01:56,680 --> 00:01:58,130
What I want to do is understand

59
00:01:58,140 --> 00:01:59,240
better when it is

60
00:01:59,250 --> 00:02:01,340
exactly that H of

61
00:02:01,350 --> 00:02:02,960
X will be greater or equal

62
00:02:02,970 --> 00:02:04,640
to 0.5, so that

63
00:02:04,650 --> 00:02:08,550
we end up predicting Y is equal to one.

64
00:02:09,530 --> 00:02:11,530
If we look at this plot

65
00:02:11,540 --> 00:02:14,180
of the sigmoid function, we'll notice

66
00:02:14,190 --> 00:02:17,010
that the sigmoid function, G

67
00:02:17,090 --> 00:02:18,970
of Z, is greater than

68
00:02:18,980 --> 00:02:21,020
or equal to 0.5

69
00:02:21,030 --> 00:02:24,290
whenever Z is

70
00:02:24,300 --> 00:02:25,970
greater than or equal to zero.

71
00:02:25,980 --> 00:02:28,140
So is in this half of

72
00:02:28,150 --> 00:02:29,890
the figure that, G takes

73
00:02:29,900 --> 00:02:32,500
on values that are 0.5 and higher.

74
00:02:32,510 --> 00:02:34,460
This is not clear, that's the 0.5.

75
00:02:34,470 --> 00:02:35,960
So when Z is

76
00:02:35,970 --> 00:02:38,330
positive, G of Z,

77
00:02:38,340 --> 00:02:41,900
the sigmoid function, is greater than or equal to 0.5.

78
00:02:41,910 --> 00:02:44,210
Since the hypothesis for

79
00:02:44,220 --> 00:02:46,380
logistic regression is H of

80
00:02:46,390 --> 00:02:48,470
X equals G of theta

81
00:02:48,480 --> 00:02:50,940
transpose X. This is

82
00:02:50,950 --> 00:02:52,170
therefore going to be greater

83
00:02:52,180 --> 00:02:54,320
than or equal to 0.5

84
00:02:54,330 --> 00:02:58,330
whenever theta transpose

85
00:02:58,340 --> 00:03:01,580
X is greater than or equal to zero.

86
00:03:01,590 --> 00:03:03,340
So what was shown, right,

87
00:03:03,350 --> 00:03:05,790
because here theta transpose X

88
00:03:05,800 --> 00:03:08,110
takes the row of Z.

89
00:03:08,120 --> 00:03:09,530
So what we're shown is that

90
00:03:09,540 --> 00:03:11,050
our hypothesis is going

91
00:03:11,060 --> 00:03:13,190
to predict Y equals one

92
00:03:13,200 --> 00:03:15,380
whenever theta transpose X

93
00:03:15,390 --> 00:03:17,870
is greater than or equal to 0.

94
00:03:17,880 --> 00:03:19,990
Let's now consider the other

95
00:03:20,000 --> 00:03:22,310
case of when a hypothesis

96
00:03:22,320 --> 00:03:25,030
will predict Y is equal to 0.

97
00:03:25,040 --> 00:03:27,200
Well, by similar argument, H

98
00:03:27,210 --> 00:03:28,950
of X is going to be

99
00:03:28,960 --> 00:03:30,720
less than 0.5 whenever G

100
00:03:30,730 --> 00:03:32,170
of Z is less than

101
00:03:32,180 --> 00:03:34,710
0.5 because the range

102
00:03:34,720 --> 00:03:36,470
of values of Z that

103
00:03:36,480 --> 00:03:38,010
calls Z to take on

104
00:03:38,020 --> 00:03:42,480
values less that 0.5, well that's when Z is negative.

105
00:03:42,550 --> 00:03:44,900
So when G of Z is less than 0.5.

106
00:03:44,910 --> 00:03:46,850
Our hypothesis will predict

107
00:03:46,860 --> 00:03:48,860
that Y is equal to zero, and

108
00:03:48,870 --> 00:03:50,500
by similar argument to what

109
00:03:50,510 --> 00:03:52,550
we had earlier, H of

110
00:03:52,560 --> 00:03:54,310
X is equal G of

111
00:03:54,320 --> 00:03:56,910
theta transpose X. And

112
00:03:56,920 --> 00:03:58,660
so, we'll predict Y equals

113
00:03:58,670 --> 00:04:00,970
zero whenever this quantity

114
00:04:00,980 --> 00:04:04,930
theta transpose X is less than zero.

115
00:04:04,940 --> 00:04:06,460
To summarize what we just

116
00:04:06,470 --> 00:04:08,340
worked out, we saw that if

117
00:04:08,350 --> 00:04:09,890
we decide to predict whether

118
00:04:09,900 --> 00:04:11,050
Y is equal to one or

119
00:04:11,060 --> 00:04:12,390
Y is equal to zero,

120
00:04:12,400 --> 00:04:14,190
depending on whether the estimated

121
00:04:14,200 --> 00:04:15,740
probability is greater than

122
00:04:15,750 --> 00:04:17,800
or equal 0.5, or whether

123
00:04:17,810 --> 00:04:19,590
it's less than 0.5, then

124
00:04:19,600 --> 00:04:20,870
that's the same as saying that

125
00:04:20,880 --> 00:04:22,860
will predict Y equals 1

126
00:04:22,870 --> 00:04:25,000
whenever theta transpose axis greater

127
00:04:25,010 --> 00:04:25,960
than or equal to 0,

128
00:04:25,970 --> 00:04:27,750
and we will predict Y is

129
00:04:27,760 --> 00:04:29,980
equal to zero whenever theta transpose X

130
00:04:29,990 --> 00:04:32,500
is less than zero.

131
00:04:32,940 --> 00:04:34,160
Let's use this to better

132
00:04:34,170 --> 00:04:36,880
understand how the hypothesis

133
00:04:36,890 --> 00:04:40,030
of logistic regression makes those predictions.

134
00:04:40,040 --> 00:04:41,510
Now, let's suppose we have

135
00:04:41,520 --> 00:04:43,060
a training set like that shown

136
00:04:43,070 --> 00:04:45,100
on the slide, and suppose

137
00:04:45,110 --> 00:04:47,220
our hypothesis is H of

138
00:04:47,230 --> 00:04:48,650
X equals G of theta

139
00:04:48,660 --> 00:04:50,250
zero, plus theta one X1

140
00:04:50,260 --> 00:04:52,840
plus theta two X2.

141
00:04:52,850 --> 00:04:54,490
We haven't talked yet about how

142
00:04:54,500 --> 00:04:56,700
to fit the parameters of this model.

143
00:04:56,710 --> 00:04:59,300
We'll talk about that in the next video.

144
00:04:59,310 --> 00:05:01,670
But suppose that variable procedure

145
00:05:01,680 --> 00:05:03,530
to be specified, we end

146
00:05:03,540 --> 00:05:06,180
up choosing the following values for the parameters.

147
00:05:06,190 --> 00:05:07,830
Let's say we choose theta zero

148
00:05:07,840 --> 00:05:09,690
equals three, theta one

149
00:05:09,700 --> 00:05:13,530
equals one, theta two equals one.

150
00:05:13,540 --> 00:05:15,380
So this means that my parameter

151
00:05:15,390 --> 00:05:17,210
vector is going to be

152
00:05:17,220 --> 00:05:22,110
theta equals minus 311.

153
00:05:24,140 --> 00:05:27,050
So, we're given this

154
00:05:27,060 --> 00:05:30,100
choice of my hypothesis parameters,

155
00:05:30,110 --> 00:05:32,270
let's try to figure out where

156
00:05:32,280 --> 00:05:33,740
a hypothesis will end up

157
00:05:33,750 --> 00:05:35,430
predicting y equals 1 and where it

158
00:05:35,440 --> 00:05:38,540
will end up predicting y equals 0.

159
00:05:39,060 --> 00:05:40,620
Using the formulas that we

160
00:05:40,630 --> 00:05:42,890
worked on the previous slide, we know

161
00:05:42,900 --> 00:05:44,490
that Y equals 1 is

162
00:05:44,500 --> 00:05:45,760
more likely, that is the

163
00:05:45,770 --> 00:05:47,390
probability that Y equals

164
00:05:47,400 --> 00:05:48,940
1 is greater than 0.5

165
00:05:48,950 --> 00:05:51,560
or greater than or equal to 0.5.

166
00:05:51,570 --> 00:05:55,230
Whenever theta transpose x

167
00:05:55,240 --> 00:05:57,220
is greater than zero.

168
00:05:57,230 --> 00:05:58,680
And this formula that I

169
00:05:58,690 --> 00:06:00,840
just underlined minus three

170
00:06:00,850 --> 00:06:03,000
plus X1 plus X2 is,

171
00:06:03,010 --> 00:06:05,210
of course, theta transpose

172
00:06:05,220 --> 00:06:07,000
X when theta is equal

173
00:06:07,010 --> 00:06:09,750
to this value of the parameters

174
00:06:09,760 --> 00:06:12,140
that we just chose.

175
00:06:12,410 --> 00:06:14,620
So, for any example, for

176
00:06:14,630 --> 00:06:16,390
any example with features X1

177
00:06:16,400 --> 00:06:19,290
and X2 that satisfy this

178
00:06:19,300 --> 00:06:21,160
equation that minus 3

179
00:06:21,170 --> 00:06:23,520
plus X1 plus X2

180
00:06:23,530 --> 00:06:24,660
is greater than or equal to 0.

181
00:06:24,670 --> 00:06:26,990
Our hypothesis will think

182
00:06:27,000 --> 00:06:28,050
that Y equals 1 is

183
00:06:28,060 --> 00:06:31,810
more likely, or will predict that Y is equal to one.

184
00:06:32,430 --> 00:06:34,480
We can also take minus three

185
00:06:34,490 --> 00:06:35,750
and bring this to the right

186
00:06:35,760 --> 00:06:37,730
and rewrite this as X1

187
00:06:37,740 --> 00:06:41,400
plus X2 is greater than or equal to three.

188
00:06:41,410 --> 00:06:43,580
And so, equivalently, we found

189
00:06:43,590 --> 00:06:45,800
that this hypothesis will predict

190
00:06:45,810 --> 00:06:47,500
Y equals one whenever X1

191
00:06:47,510 --> 00:06:51,370
plus X2 is greater than or equal to three.

192
00:06:51,870 --> 00:06:54,860
Let's see what that means on the figure.

193
00:06:54,870 --> 00:06:57,160
If I write down the equation,

194
00:06:57,170 --> 00:07:00,220
X1 plus X2 equals three,

195
00:07:00,230 --> 00:07:03,350
this defines the equation of a straight line.

196
00:07:03,360 --> 00:07:05,000
And if I draw what that straight

197
00:07:05,010 --> 00:07:07,720
line looks like, it gives

198
00:07:07,730 --> 00:07:10,080
me the following line which passes

199
00:07:10,090 --> 00:07:11,550
through 3 and 3 on

200
00:07:11,560 --> 00:07:14,600
the X1 and the X2 axis.

201
00:07:16,240 --> 00:07:17,260
So the part of the input space,

202
00:07:17,270 --> 00:07:18,760
the part of the

203
00:07:18,770 --> 00:07:21,540
X1, X2 plane that corresponds

204
00:07:21,550 --> 00:07:24,870
to when X1 plus X2 is greater than or equal to three.

205
00:07:24,880 --> 00:07:27,200
That's going to be this very top plane.

206
00:07:27,210 --> 00:07:29,370
That is everything to the

207
00:07:29,380 --> 00:07:30,640
up, and everything to the upper

208
00:07:30,650 --> 00:07:34,080
right portion of this magenta line that I just drew.

209
00:07:34,090 --> 00:07:35,600
And so, the region where our

210
00:07:35,610 --> 00:07:37,050
hypothesis will predict Y

211
00:07:37,060 --> 00:07:38,320
equals 1 is this

212
00:07:38,330 --> 00:07:40,000
region, you know, is

213
00:07:40,010 --> 00:07:41,610
really this huge region, this

214
00:07:41,620 --> 00:07:44,380
half-space over to the upper right.

215
00:07:44,390 --> 00:07:45,420
And let me just write that down.

216
00:07:45,430 --> 00:07:47,320
I'm gonna call this the Y

217
00:07:47,330 --> 00:07:50,240
equals one region, and in

218
00:07:50,250 --> 00:07:53,670
contrast the region where

219
00:07:54,290 --> 00:07:56,500
X1 plus X2 is

220
00:07:56,510 --> 00:07:58,620
less than three, that's when

221
00:07:58,630 --> 00:08:00,100
we will predict that Y,

222
00:08:00,110 --> 00:08:01,960
Y is equal to zero, and

223
00:08:01,970 --> 00:08:04,700
that corresponds to this region.

224
00:08:04,710 --> 00:08:06,070
You know, itt's really a half-plane, but

225
00:08:06,080 --> 00:08:08,480
that region on the left is

226
00:08:08,490 --> 00:08:11,730
the region where our hypothesis predict Y equals 0.

227
00:08:11,740 --> 00:08:13,420
I want to give

228
00:08:13,430 --> 00:08:16,450
this line, this magenta line that I drew a name.

229
00:08:16,460 --> 00:08:19,430
This line there is called

230
00:08:19,440 --> 00:08:23,380
the decision boundary.

231
00:08:24,580 --> 00:08:27,050
And concretely, this straight line

232
00:08:27,060 --> 00:08:28,460
X1 plus X equals 3.

233
00:08:28,470 --> 00:08:31,150
That corresponds to the set of points.

234
00:08:31,160 --> 00:08:33,300
So that corresponds to the region

235
00:08:33,310 --> 00:08:34,560
where H of X is equal

236
00:08:34,570 --> 00:08:36,960
to 0.5 exactly and

237
00:08:36,970 --> 00:08:38,740
the decision boundary, that is

238
00:08:38,750 --> 00:08:40,710
this straight line, that's the

239
00:08:40,720 --> 00:08:42,740
line that separates the region

240
00:08:42,750 --> 00:08:44,640
where the hypothesis predicts Y equals

241
00:08:44,650 --> 00:08:46,420
one from the region

242
00:08:46,430 --> 00:08:49,730
where the hypothesis predicts that Y is equal to 0.

243
00:08:49,740 --> 00:08:51,380
And just to be clear.

244
00:08:51,390 --> 00:08:53,330
The decision boundary is a

245
00:08:53,340 --> 00:08:56,190
property of the hypothesis

246
00:08:57,430 --> 00:09:00,710
including the parameters theta 0, theta 1, theta 2.

247
00:09:00,720 --> 00:09:03,230
And in the figure I drew a training set.

248
00:09:03,240 --> 00:09:06,470
I drew a data set in order to help the visualization.

249
00:09:06,480 --> 00:09:07,710
But even if we take

250
00:09:07,720 --> 00:09:09,270
away the data set, you know

251
00:09:09,280 --> 00:09:11,060
decision boundary and a

252
00:09:11,070 --> 00:09:12,290
region where we predict Y

253
00:09:12,300 --> 00:09:14,310
equals 1 versus Y equals zero.

254
00:09:14,320 --> 00:09:15,450
That's a property of the

255
00:09:15,460 --> 00:09:16,780
hypothesis and of the

256
00:09:16,790 --> 00:09:18,810
parameters of the hypothesis, and

257
00:09:18,820 --> 00:09:21,940
not a property of the data set.

258
00:09:22,140 --> 00:09:23,550
Later on, of course, we'll talk

259
00:09:23,560 --> 00:09:24,640
about how to fit the

260
00:09:24,650 --> 00:09:26,620
parameters and there we'll

261
00:09:26,630 --> 00:09:28,210
end up using the training set,

262
00:09:28,220 --> 00:09:32,520
or using our data, to determine the value of the parameters.

263
00:09:32,530 --> 00:09:34,510
But once we have particular values

264
00:09:34,520 --> 00:09:37,280
for the parameters: theta 0, theta 1, theta 2.

265
00:09:37,290 --> 00:09:39,580
Then that completely defines

266
00:09:39,590 --> 00:09:41,660
the decision boundary and we

267
00:09:41,670 --> 00:09:43,100
don't actually need to plot

268
00:09:43,110 --> 00:09:44,840
a training set in order

269
00:09:44,850 --> 00:09:47,650
to plot the decision boundary.

270
00:09:49,620 --> 00:09:50,560
Let's now look at a more

271
00:09:50,570 --> 00:09:52,410
complex example where, as

272
00:09:52,420 --> 00:09:54,030
usual, I have crosses to

273
00:09:54,040 --> 00:09:55,910
denote my positive examples and

274
00:09:55,920 --> 00:09:58,750
O's to denote my negative examples.

275
00:09:58,890 --> 00:10:00,700
Given a training set like this,

276
00:10:00,710 --> 00:10:02,890
how can I get logistic regression

277
00:10:02,900 --> 00:10:05,520
to fit this sort of data?

278
00:10:05,530 --> 00:10:07,120
Earlier, when we were talking about

279
00:10:07,130 --> 00:10:09,030
polynomial regression or when

280
00:10:09,040 --> 00:10:10,950
we're linear regression, we talked

281
00:10:10,960 --> 00:10:12,520
about how we can add extra

282
00:10:12,530 --> 00:10:15,510
higher order polynomial terms to the features.

283
00:10:15,520 --> 00:10:18,900
And we can do the same for logistic regression.

284
00:10:18,910 --> 00:10:22,210
Concretely, let's say my hypothesis looks like this.

285
00:10:22,220 --> 00:10:23,690
Where I've added two extra

286
00:10:23,700 --> 00:10:27,680
features, X1 squared and X2 squared, to my features.

287
00:10:27,690 --> 00:10:29,780
So that I now have 5 parameters,

288
00:10:29,790 --> 00:10:32,730
theta 0 through theta 4.

289
00:10:32,780 --> 00:10:34,840
As before, we'll defer to

290
00:10:34,850 --> 00:10:37,410
the next video our discussion

291
00:10:37,420 --> 00:10:39,200
on how to automatically choose

292
00:10:39,210 --> 00:10:42,500
values for the parameters theta 0 through theta 4.

293
00:10:42,510 --> 00:10:44,230
But let's say that

294
00:10:44,240 --> 00:10:46,660
very procedure to be specified,

295
00:10:46,670 --> 00:10:49,200
I end up choosing theta 0

296
00:10:49,210 --> 00:10:51,280
equals minus 1, theta 1

297
00:10:51,290 --> 00:10:52,910
equals 0, theta 2

298
00:10:52,920 --> 00:10:55,640
equals 0, theta 3 equals

299
00:10:55,650 --> 00:10:57,990
1, and theta 4 equals 1.

300
00:10:58,000 --> 00:11:00,200
What this means

301
00:11:00,210 --> 00:11:02,140
is that with this particular choice

302
00:11:02,150 --> 00:11:04,530
of parameters, my parameter

303
00:11:04,540 --> 00:11:08,630
vector theta looks like minus 1, 0, 0, 1, 1.

304
00:11:10,550 --> 00:11:12,340
Following our earlier discussion, this

305
00:11:12,350 --> 00:11:14,360
means that my hypothesis will predict

306
00:11:14,370 --> 00:11:16,370
that Y is equal to 1

307
00:11:16,380 --> 00:11:18,230
whenever minus 1 plus X1

308
00:11:18,240 --> 00:11:21,020
squared plus X2 squared is greater than or equal to 0.

309
00:11:21,030 --> 00:11:24,120
This is whenever theta transpose

310
00:11:24,130 --> 00:11:26,340
times my theta transpose

311
00:11:26,350 --> 00:11:30,050
my features is greater than or equal to 0.

312
00:11:30,060 --> 00:11:31,680
And if I take minus

313
00:11:31,690 --> 00:11:32,890
1 and just bring this to

314
00:11:32,900 --> 00:11:34,770
the right, I'm saying that

315
00:11:34,780 --> 00:11:36,610
my hypothesis will predict that

316
00:11:36,620 --> 00:11:38,110
Y is equal to 1

317
00:11:38,120 --> 00:11:40,700
whenever X1 squared plus

318
00:11:40,710 --> 00:11:43,570
X2 squared is greater than or equal to 1.

319
00:11:43,580 --> 00:11:47,930
So, what does decision boundary look like?

320
00:11:47,940 --> 00:11:49,770
Well, if you were to plot the

321
00:11:49,780 --> 00:11:51,890
curve for X1 squared plus

322
00:11:51,900 --> 00:11:53,640
X2 squared equals 1.

323
00:11:53,650 --> 00:11:55,500
Some of you will

324
00:11:55,510 --> 00:11:58,280
that is the equation for

325
00:11:58,290 --> 00:12:01,280
a circle of radius

326
00:12:01,290 --> 00:12:04,140
1 centered around the origin.

327
00:12:04,150 --> 00:12:07,790
So, that is my decision boundary.

328
00:12:10,410 --> 00:12:12,240
And everything outside the

329
00:12:12,250 --> 00:12:14,170
circle I'm going to predict

330
00:12:14,180 --> 00:12:15,380
as Y equals 1.

331
00:12:15,390 --> 00:12:17,690
So out here is, you know, my

332
00:12:17,700 --> 00:12:19,350
Y equals 1 region.

333
00:12:19,360 --> 00:12:22,660
I'm going to predict Y equals 1 out here.

334
00:12:22,670 --> 00:12:24,300
And inside the circle is where

335
00:12:24,310 --> 00:12:27,780
I'll predict Y is equal to 0.

336
00:12:27,790 --> 00:12:30,020
So, by adding these more

337
00:12:30,030 --> 00:12:33,150
complex or these polynomial terms to my features as well.

338
00:12:33,160 --> 00:12:35,010
I can get more complex decision

339
00:12:35,020 --> 00:12:36,500
boundaries that don't just

340
00:12:36,510 --> 00:12:39,540
try to separate the positive and negative examples of a straight line.

341
00:12:39,550 --> 00:12:41,290
I can get in this example

342
00:12:41,300 --> 00:12:44,180
a decision boundary that's a circle.

343
00:12:44,190 --> 00:12:45,990
Once again the decision boundary

344
00:12:46,000 --> 00:12:47,850
is a property not of

345
00:12:47,860 --> 00:12:51,630
the training set, but of the hypothesis and of the parameters.

346
00:12:51,640 --> 00:12:53,060
So long as we've

347
00:12:53,070 --> 00:12:55,360
given my parameter vector theta,

348
00:12:55,370 --> 00:12:57,140
that defines the decision

349
00:12:57,150 --> 00:12:59,200
boundary which is the circle.

350
00:12:59,210 --> 00:13:03,040
But the training set is not what we use to define decision boundary.

351
00:13:03,050 --> 00:13:06,540
The training set may be used to fit the parameters theta.

352
00:13:06,550 --> 00:13:08,600
We'll talk about how to do that later.

353
00:13:08,610 --> 00:13:09,800
But once you have the

354
00:13:09,810 --> 00:13:13,610
parameters theta, that is what defines the decision boundary.

355
00:13:13,620 --> 00:13:16,390
Let me put back the training set

356
00:13:16,400 --> 00:13:18,510
just for visualization.

357
00:13:18,520 --> 00:13:22,090
And finally, let's look at a more complex example.

358
00:13:22,320 --> 00:13:23,290
So can we come up

359
00:13:23,300 --> 00:13:26,520
with even more complex decision boundaries and this?

360
00:13:26,530 --> 00:13:28,410
If I have even higher

361
00:13:28,420 --> 00:13:31,110
order polynomial terms, so things

362
00:13:31,120 --> 00:13:34,460
like X1 squared, X1

363
00:13:34,470 --> 00:13:36,590
squared X2, X1 squared

364
00:13:36,600 --> 00:13:37,760
X2 squared, and so on.

365
00:13:37,770 --> 00:13:38,970
If I have much higher order

366
00:13:38,980 --> 00:13:41,550
polynomials, then it's possible

367
00:13:41,560 --> 00:13:42,840
to show that you can get

368
00:13:42,850 --> 00:13:45,210
even more complex decision boundaries and

369
00:13:45,220 --> 00:13:46,920
logistic regression can be

370
00:13:46,930 --> 00:13:48,490
used to find the zero boundaries

371
00:13:48,500 --> 00:13:50,080
that may, for example, be

372
00:13:50,090 --> 00:13:52,070
an ellipse like that, or

373
00:13:52,080 --> 00:13:53,470
maybe with a different setting of

374
00:13:53,480 --> 00:13:55,410
the parameters, maybe you

375
00:13:55,420 --> 00:13:57,830
can get instead a different decision boundary that

376
00:13:57,840 --> 00:13:59,760
may even look like, you know, some funny

377
00:13:59,770 --> 00:14:04,070
shape like that.

378
00:14:04,080 --> 00:14:06,320
Or for even more complex examples

379
00:14:06,330 --> 00:14:08,940
you can also get decision boundaries

380
00:14:08,950 --> 00:14:10,380
that can look like, you know,

381
00:14:10,390 --> 00:14:12,030
more complex shapes like that.

382
00:14:12,040 --> 00:14:13,320
Where everything in here you

383
00:14:13,330 --> 00:14:15,390
predict Y equals 1, and

384
00:14:15,400 --> 00:14:17,510
everything outside you predict Y equals 0.

385
00:14:17,520 --> 00:14:19,550
So these higher order polynomial

386
00:14:19,560 --> 00:14:23,060
features you can get very complex decision boundaries.

387
00:14:23,070 --> 00:14:24,760
So with these visualizations, I

388
00:14:24,770 --> 00:14:26,130
hope that gives you a

389
00:14:26,140 --> 00:14:28,590
what's the range of hypothesis

390
00:14:28,600 --> 00:14:30,640
functions we can represent using

391
00:14:30,650 --> 00:14:34,620
the representation that we have for logistic regression.

392
00:14:34,870 --> 00:14:37,680
Now that we know what H of X can represent.

393
00:14:37,690 --> 00:14:38,940
What I'd like to do next in

394
00:14:38,950 --> 00:14:40,520
the following video is talk

395
00:14:40,530 --> 00:14:44,100
about how to automatically choose the parameters theta.

396
00:14:44,110 --> 00:14:45,540
So that given a training

397
00:14:45,550 --> 00:14:49,860
set we can automatically fit the parameters to our data.

