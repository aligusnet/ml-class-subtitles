1
00:00:00,200 --> 00:00:01,610
In this video we'll talk about

2
00:00:01,620 --> 00:00:03,640
how to get logistic regression to

3
00:00:03,650 --> 00:00:06,150
work for multi-class classification problems,

4
00:00:06,160 --> 00:00:07,490
and in particular I want to

5
00:00:07,500 --> 00:00:11,370
tell you about an algorithm called one-versus-all classification.

6
00:00:12,150 --> 00:00:14,280
What's a multi-class classification problem?

7
00:00:14,290 --> 00:00:15,910
Here are some examples.

8
00:00:15,920 --> 00:00:17,310
Let's say you want a learning

9
00:00:17,320 --> 00:00:19,700
algorithm to automatically put your

10
00:00:19,710 --> 00:00:21,060
email into different folders or

11
00:00:21,070 --> 00:00:23,380
to automatically tag your emails.

12
00:00:23,390 --> 00:00:24,780
So, you might have different folders

13
00:00:24,790 --> 00:00:27,050
or different tags for work email,

14
00:00:27,060 --> 00:00:28,220
email from your friends, email

15
00:00:28,230 --> 00:00:31,580
from your family and emails about your hobby.

16
00:00:31,590 --> 00:00:33,130
And so, here, we have

17
00:00:33,140 --> 00:00:34,890
a classification problem with 4

18
00:00:34,900 --> 00:00:36,170
classes, which we might

19
00:00:36,180 --> 00:00:38,090
assign the numbers, the classes

20
00:00:38,100 --> 00:00:41,310
y1, y2, y3 and

21
00:00:41,320 --> 00:00:44,480
y4 to, Another

22
00:00:44,490 --> 00:00:45,990
example for a medical

23
00:00:46,000 --> 00:00:47,790
diagnosis: if a patient

24
00:00:47,800 --> 00:00:48,920
comes into your office with

25
00:00:48,930 --> 00:00:51,360
maybe a stuffy nose, the possible

26
00:00:51,370 --> 00:00:52,720
diagnoses could be that

27
00:00:52,730 --> 00:00:54,120
they're not ill, maybe that's

28
00:00:54,130 --> 00:00:55,480
y1; or they have

29
00:00:55,490 --> 00:00:58,970
a cold, 2; or they have the flu.

30
00:00:58,980 --> 00:01:00,480
And the third and final example,

31
00:01:00,490 --> 00:01:02,080
if you are using machine learning

32
00:01:02,090 --> 00:01:03,900
to classify the weather, you know,

33
00:01:03,910 --> 00:01:05,250
maybe you want to decide that

34
00:01:05,260 --> 00:01:07,940
the weather is sunny, cloudy, rainy

35
00:01:07,950 --> 00:01:10,220
or snow, or if there's gonna be snow.

36
00:01:10,230 --> 00:01:11,110
And so, in all of these

37
00:01:11,120 --> 00:01:12,770
examples, Y can take

38
00:01:12,780 --> 00:01:14,280
on a small number of

39
00:01:14,290 --> 00:01:16,480
discreet values, maybe 1 to

40
00:01:16,490 --> 00:01:17,880
3, 1 to 4 and so on, and

41
00:01:17,890 --> 00:01:20,610
these are multi-class classification problems.

42
00:01:20,620 --> 00:01:21,890
And by the way, it doesn't really

43
00:01:21,900 --> 00:01:23,620
matter whether we index as

44
00:01:23,630 --> 00:01:27,080
0123 or as 1234, I tend

45
00:01:27,090 --> 00:01:29,110
to index that my classes

46
00:01:29,120 --> 00:01:31,530
starting from 1 rather than starting from 0.

47
00:01:31,540 --> 00:01:33,720
But either way, where often, it really doesn't matter.

48
00:01:33,730 --> 00:01:35,230
Whereas previously, for a

49
00:01:35,240 --> 00:01:39,310
binary classification problem, our data sets look like this.

50
00:01:39,320 --> 00:01:41,580
For a multi-class classification problem, our

51
00:01:41,590 --> 00:01:42,760
data sets may look like

52
00:01:42,770 --> 00:01:44,320
this, where here, I'm using

53
00:01:44,330 --> 00:01:48,400
three different symbols to represent our three classes.

54
00:01:48,410 --> 00:01:49,840
So, the question is: Given the

55
00:01:49,850 --> 00:01:51,590
data set with three classes

56
00:01:51,600 --> 00:01:53,160
where this is a the

57
00:01:53,170 --> 00:01:54,590
example of one class, that's

58
00:01:54,600 --> 00:01:55,780
the example of the different class,

59
00:01:55,790 --> 00:01:58,400
and, that's the example of yet, the third class.

60
00:01:58,410 --> 00:02:01,370
How do we get a learning algorithm to work for the setting?

61
00:02:01,380 --> 00:02:02,560
We already know how to

62
00:02:02,570 --> 00:02:05,080
do binary classification, using logistic

63
00:02:05,090 --> 00:02:06,510
regression, we know how the,

64
00:02:06,520 --> 00:02:07,710
you know, maybe, for the straight line,

65
00:02:07,720 --> 00:02:10,560
to separate the positive and negative classes.

66
00:02:10,590 --> 00:02:12,090
Using an idea called one

67
00:02:12,100 --> 00:02:14,390
versus all classification, we can

68
00:02:14,400 --> 00:02:15,700
then take this, and, make

69
00:02:15,710 --> 00:02:18,640
it work for multi-class classification, as well.

70
00:02:18,650 --> 00:02:21,610
Here's how one versus all classification works.

71
00:02:21,620 --> 00:02:25,750
And, this is also sometimes called "one versus rest."

72
00:02:25,760 --> 00:02:26,930
Let's say, we have a training

73
00:02:26,940 --> 00:02:28,140
set, like that shown on the

74
00:02:28,150 --> 00:02:30,460
left, where we have 3 classes.

75
00:02:30,470 --> 00:02:32,280
So, if y1, we denote that

76
00:02:32,290 --> 00:02:34,380
with a triangle if y2 the

77
00:02:34,390 --> 00:02:37,970
square and, if y3 then, the cross.

78
00:02:37,980 --> 00:02:39,470
What we're going to do is,

79
00:02:39,480 --> 00:02:41,330
take a training set, and, turn

80
00:02:41,340 --> 00:02:44,790
this into three separate binary classification problems.

81
00:02:44,800 --> 00:02:46,740
So, I'll turn this into three separate

82
00:02:46,750 --> 00:02:49,430
two class classification problems.

83
00:02:49,440 --> 00:02:51,650
So let's start with Class 1, which is a triangle.

84
00:02:51,660 --> 00:02:53,040
We are going to essentially create a

85
00:02:53,050 --> 00:02:55,430
new, sort of fake training set.

86
00:02:55,440 --> 00:02:56,910
where classes 2 and 3

87
00:02:56,920 --> 00:02:58,130
get assigned to the negative

88
00:02:58,140 --> 00:02:59,840
class and class 1

89
00:02:59,850 --> 00:03:01,100
gets assigned to the positive class

90
00:03:01,110 --> 00:03:02,370
when we create a new training

91
00:03:02,380 --> 00:03:03,690
set if that's showing

92
00:03:03,700 --> 00:03:05,470
on the right and we're going

93
00:03:05,480 --> 00:03:07,530
to fit a classifier, which I'm

94
00:03:07,540 --> 00:03:10,210
going to call h subscript theta

95
00:03:10,220 --> 00:03:12,630
superscript 1 of x

96
00:03:12,640 --> 00:03:15,600
where here, the triangles

97
00:03:15,610 --> 00:03:18,980
are the positive examples and the circles are the negative examples.

98
00:03:18,990 --> 00:03:20,570
So, think of the triangles be

99
00:03:20,580 --> 00:03:21,770
assigned the value of 1

100
00:03:21,780 --> 00:03:25,290
and the circles the sum, the value of zero.

101
00:03:25,300 --> 00:03:26,690
And we're just going to train

102
00:03:26,700 --> 00:03:29,520
a standard logistic regression crossfire

103
00:03:29,530 --> 00:03:32,230
and maybe that will give us a position boundary.

104
00:03:32,240 --> 00:03:33,740
OK?

105
00:03:34,890 --> 00:03:37,670
The superscript 1 here is the class one.

106
00:03:37,680 --> 00:03:40,790
So, we're doing this for the triangle first class.

107
00:03:40,800 --> 00:03:42,280
Next, we do the same thing for class 2.

108
00:03:42,290 --> 00:03:44,010
Going to take the squares and

109
00:03:44,020 --> 00:03:45,460
assign the squares as the

110
00:03:45,470 --> 00:03:46,940
positive class and assign

111
00:03:46,950 --> 00:03:50,210
every thing else the triangles and the crosses as the negative class.

112
00:03:50,220 --> 00:03:54,130
and then we fit a second logistic regression classifier.

113
00:03:54,140 --> 00:03:56,410
I'm gonna call this H of X

114
00:03:56,420 --> 00:03:58,330
superscript 2, where the

115
00:03:58,340 --> 00:04:00,010
superscript 2 denotes that

116
00:04:00,020 --> 00:04:01,860
we're now doing this:  treating the

117
00:04:01,870 --> 00:04:03,340
square class as the positive

118
00:04:03,350 --> 00:04:07,720
class and maybe we get the classifier like that.

119
00:04:07,730 --> 00:04:08,790
And finally, we do the

120
00:04:08,800 --> 00:04:10,100
same thing for the third

121
00:04:10,110 --> 00:04:11,600
class and fit a third

122
00:04:11,610 --> 00:04:14,620
classifier H superscript 3

123
00:04:14,630 --> 00:04:16,430
of X and maybe this

124
00:04:16,440 --> 00:04:18,080
will give us a decision boundary

125
00:04:18,090 --> 00:04:19,740
or give us a classifier that separates

126
00:04:19,750 --> 00:04:22,640
the positive and negative examples like that.

127
00:04:22,870 --> 00:04:24,320
So, to summarize, what we've

128
00:04:24,330 --> 00:04:27,880
done is we fit 3 classifiers.

129
00:04:27,890 --> 00:04:29,360
So, for I equals 1

130
00:04:29,370 --> 00:04:31,870
2 3 we'll fit a classifier

131
00:04:31,880 --> 00:04:33,840
H superscript I subscript theta

132
00:04:33,850 --> 00:04:35,210
of X, thus trying to

133
00:04:35,220 --> 00:04:36,440
estimate what is the

134
00:04:36,450 --> 00:04:38,180
probability that y is

135
00:04:38,190 --> 00:04:41,600
equal to class I given X and prioritize by theta.

136
00:04:41,610 --> 00:04:41,810
Right?

137
00:04:41,820 --> 00:04:43,220
So, in the first

138
00:04:43,230 --> 00:04:44,900
instance, for this first one

139
00:04:44,910 --> 00:04:47,270
up here, this classifier

140
00:04:47,280 --> 00:04:49,330
with learning to by the triangle.

141
00:04:49,340 --> 00:04:52,050
So it's thinking of the triangles as a positive class.

142
00:04:52,060 --> 00:04:53,820
So, X superscript one is

143
00:04:53,830 --> 00:04:55,160
essentially trying to estimate what is

144
00:04:55,170 --> 00:04:57,340
the probability that the Y

145
00:04:57,350 --> 00:04:59,040
is equal to one, given

146
00:04:59,050 --> 00:05:02,000
X and parametrized by theta.

147
00:05:02,010 --> 00:05:04,470
And similarly, this is treating,

148
00:05:04,480 --> 00:05:05,820
you know, the square class as

149
00:05:05,830 --> 00:05:07,330
a positive took pause so its

150
00:05:07,340 --> 00:05:10,740
trying to estimate the probability that y2 and so on.

151
00:05:10,750 --> 00:05:13,300
So we now have 3 classifiers each

152
00:05:13,310 --> 00:05:16,660
of which was trained is one of the three crosses.

153
00:05:16,670 --> 00:05:17,850
Just to summarize, what we've

154
00:05:17,860 --> 00:05:19,690
done is we've, we want

155
00:05:19,700 --> 00:05:21,290
to train a logistic regression

156
00:05:21,300 --> 00:05:23,540
classifier, H superscript I

157
00:05:23,550 --> 00:05:24,940
of X, for each plus

158
00:05:24,950 --> 00:05:26,120
i that predicts it's probably a

159
00:05:26,130 --> 00:05:28,560
y i. Finally, to

160
00:05:28,570 --> 00:05:29,810
make a prediction when we

161
00:05:29,820 --> 00:05:31,760
give it a new input x and

162
00:05:31,770 --> 00:05:33,330
we want to make a prediction,

163
00:05:33,340 --> 00:05:34,720
we do is we just

164
00:05:34,730 --> 00:05:36,680
run Let's say three

165
00:05:36,690 --> 00:05:38,530
what run our 3 of our

166
00:05:38,540 --> 00:05:39,950
classifiers on the input

167
00:05:39,960 --> 00:05:41,480
x and we then

168
00:05:41,490 --> 00:05:44,040
pick the class i that maximizes the three.

169
00:05:44,050 --> 00:05:45,340
So, we just you know, basically

170
00:05:45,350 --> 00:05:47,160
pick the classifier, pick whichever

171
00:05:47,170 --> 00:05:49,200
one of the three classifiers is

172
00:05:49,210 --> 00:05:52,110
most confident, or most enthusistically

173
00:05:52,120 --> 00:05:54,290
says that it thinks it has a right class.

174
00:05:54,300 --> 00:05:56,180
So, whichever value of i

175
00:05:56,190 --> 00:05:58,030
gives us the highest probability, we

176
00:05:58,040 --> 00:06:00,910
then predict y to be that value.

177
00:06:02,660 --> 00:06:04,460
So, that's it for multi-class

178
00:06:04,470 --> 00:06:07,640
classification and one-versus-all method.

179
00:06:07,650 --> 00:06:09,080
And with this little method

180
00:06:09,090 --> 00:06:10,460
you can now take the logistic

181
00:06:10,470 --> 00:06:11,990
regression classifier and make

182
00:06:12,000 --> 00:06:15,890
it work on multi-class classification problems as well.

