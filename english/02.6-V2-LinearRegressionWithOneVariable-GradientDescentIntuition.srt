1
00:00:00,150 --> 00:00:01,710
In the previous video, we gave

2
00:00:01,720 --> 00:00:04,910
a mathematical definition of gradient descent.

3
00:00:04,920 --> 00:00:06,270
Let's go deeper, and in this

4
00:00:06,280 --> 00:00:08,100
video get better intuition about

5
00:00:08,110 --> 00:00:09,430
what the algorithm is doing,

6
00:00:09,440 --> 00:00:10,630
and why the steps of the

7
00:00:10,640 --> 00:00:13,670
gradient descent algorithm might make sense.

8
00:00:15,470 --> 00:00:17,080
Here's the gradient descent algorithm that

9
00:00:17,090 --> 00:00:19,280
we saw last time and

10
00:00:19,290 --> 00:00:21,670
I just remind you, this

11
00:00:21,680 --> 00:00:23,360
parameter, or this term

12
00:00:23,370 --> 00:00:24,800
alpha, is called the learning

13
00:00:24,810 --> 00:00:27,120
rate, and it controls

14
00:00:27,130 --> 00:00:28,330
how big a step we

15
00:00:28,340 --> 00:00:30,150
take when updating my parameter

16
00:00:30,160 --> 00:00:32,190
theta j and this

17
00:00:32,200 --> 00:00:37,750
second term here is the derivative term.

18
00:00:39,230 --> 00:00:40,240
And what I want to do, in

19
00:00:40,250 --> 00:00:41,820
this video, is give you

20
00:00:41,830 --> 00:00:43,440
about what each of

21
00:00:43,450 --> 00:00:44,780
these two terms is doing, and

22
00:00:44,790 --> 00:00:49,400
why, when put together, this entire update makes sense.

23
00:00:49,410 --> 00:00:51,430
In order to convey these

24
00:00:51,440 --> 00:00:52,710
intuitions, what I want

25
00:00:52,720 --> 00:00:53,900
to do is use a slightly

26
00:00:53,910 --> 00:00:56,270
simple example where we

27
00:00:56,280 --> 00:00:59,140
want to minimize a function

28
00:00:59,150 --> 00:01:00,810
of just one parameter, so we

29
00:01:00,820 --> 00:01:02,460
have a cost function

30
00:01:02,470 --> 00:01:04,280
j of just one parameter theta

31
00:01:04,290 --> 00:01:05,530
one, like we did a few

32
00:01:05,540 --> 00:01:07,210
videos back, where theta

33
00:01:07,220 --> 00:01:10,660
one is a real number ok.

34
00:01:10,710 --> 00:01:14,300
So, we can have 1d plots which are rather simple to look at.

35
00:01:14,310 --> 00:01:18,680
Let's try to understand what gradient descent will do on this function.

36
00:01:20,850 --> 00:01:22,270
So, let's say here's

37
00:01:22,280 --> 00:01:26,250
my function j of

38
00:01:26,260 --> 00:01:27,690
theta one, and so that's

39
00:01:27,700 --> 00:01:29,480
my and where theta

40
00:01:29,490 --> 00:01:32,600
one is a real number.

41
00:01:32,610 --> 00:01:33,730
Right?

42
00:01:33,740 --> 00:01:35,740
Now, let's have initialized gradient

43
00:01:35,750 --> 00:01:38,450
descent with theta one

44
00:01:38,460 --> 00:01:40,390
at this location, so imagine

45
00:01:40,400 --> 00:01:44,170
that we start off at that point of my function.

46
00:01:44,450 --> 00:01:45,930
What gradient descent will do

47
00:01:45,940 --> 00:01:50,580
is it will update theta one.

48
00:01:50,590 --> 00:01:51,800
Let's update to this, theta

49
00:01:51,810 --> 00:01:56,270
one minus alpha times dd

50
00:01:56,280 --> 00:02:00,980
theta one j of theta one.

51
00:02:00,990 --> 00:02:02,490
Right?And

52
00:02:03,690 --> 00:02:06,800
as in the side, this derivative term, right?

53
00:02:06,810 --> 00:02:09,590
And if you're wondering

54
00:02:09,600 --> 00:02:13,330
why I changed the notation from these partial derivative symbols.

55
00:02:13,340 --> 00:02:14,430
If you don't know what the difference

56
00:02:14,440 --> 00:02:16,230
is between these partial derivative symbols

57
00:02:16,240 --> 00:02:18,450
and the DD theta, don't worry about it.

58
00:02:18,460 --> 00:02:20,680
Technically, in mathematics, we call

59
00:02:20,690 --> 00:02:22,170
this a partial derivative, we call

60
00:02:22,180 --> 00:02:24,090
this a derivative, depending on

61
00:02:24,100 --> 00:02:25,960
the number of parameters in

62
00:02:25,970 --> 00:02:27,490
the function 'J', but that's

63
00:02:27,500 --> 00:02:30,320
mathematical technicality, so for

64
00:02:30,330 --> 00:02:32,140
the purpose of this lecture think

65
00:02:32,150 --> 00:02:33,830
of these partial symbols and

66
00:02:33,840 --> 00:02:35,280
gd theta one as exactly

67
00:02:35,290 --> 00:02:38,120
the same thing and don't worry about whether there is differences.

68
00:02:38,130 --> 00:02:41,990
I' m going to try to use the mathematically-precise notation.

69
00:02:42,000 --> 00:02:45,930
But for our purposes, DC notations are really the same thing.

70
00:02:46,470 --> 00:02:48,820
So let's see what this equation will do.

71
00:02:48,830 --> 00:02:51,250
So, we're going to compute this derivative.

72
00:02:51,260 --> 00:02:52,340
I'm not sure if

73
00:02:52,350 --> 00:02:53,690
you've seen derivatives in calculus before,

74
00:02:53,700 --> 00:02:55,230
but what a derivative

75
00:02:55,240 --> 00:02:56,760
at this point does, is basically

76
00:02:56,770 --> 00:02:58,710
say, let's take the

77
00:02:58,720 --> 00:03:00,980
tangent to that point, that

78
00:03:00,990 --> 00:03:04,430
straight line, that red line is just touching this function.

79
00:03:04,440 --> 00:03:06,420
Let's look at the slope of this red line.

80
00:03:06,430 --> 00:03:07,650
That's what the derivative is.

81
00:03:07,660 --> 00:03:09,280
It says, what's the slope

82
00:03:09,290 --> 00:03:11,430
Of the line that is just tangent to the function.

83
00:03:11,440 --> 00:03:12,550
OK, and the slope of

84
00:03:12,560 --> 00:03:14,610
the line, of course, is just,

85
00:03:14,620 --> 00:03:18,300
this height divided by this horizontal thing.

86
00:03:18,310 --> 00:03:22,680
Now This line has a positive slope.

87
00:03:22,690 --> 00:03:25,070
So it has a positive

88
00:03:25,080 --> 00:03:27,180
derivative, and so

89
00:03:27,190 --> 00:03:29,000
my update to theta is

90
00:03:29,010 --> 00:03:31,080
going to be theta 1

91
00:03:31,090 --> 00:03:32,810
gets updated as theta 1

92
00:03:32,820 --> 00:03:37,200
minus alpha times some positive number.

93
00:03:39,400 --> 00:03:41,230
Alpha, the learning rate, is

94
00:03:41,240 --> 00:03:43,390
always a positive number, and

95
00:03:43,400 --> 00:03:44,490
so I'm going to take theta

96
00:03:44,500 --> 00:03:45,640
one gives update as theta

97
00:03:45,650 --> 00:03:47,280
one minus something, so I'm

98
00:03:47,290 --> 00:03:49,480
going to end up moving theta 1 to the left.

99
00:03:49,490 --> 00:03:50,880
We're going to decrease theta 1,

100
00:03:50,890 --> 00:03:52,770
and we can see this

101
00:03:52,780 --> 00:03:53,880
is the right thing to do, because

102
00:03:53,890 --> 00:03:56,010
as you want to head in this direction

103
00:03:56,020 --> 00:04:00,690
to get me closer to the minimum over there.

104
00:04:00,760 --> 00:04:03,970
So gradient descent, so far, seems to be doing the right thing.

105
00:04:03,980 --> 00:04:06,070
Let's look at another example.

106
00:04:06,080 --> 00:04:07,890
So let's take my same function

107
00:04:07,900 --> 00:04:10,850
J of theta

108
00:04:10,860 --> 00:04:12,710
one and now let's say

109
00:04:12,720 --> 00:04:16,100
I had instead initialized my parameter over there on the left.

110
00:04:16,110 --> 00:04:20,310
So theta one is here, I'm going to adapt point on the surface.

111
00:04:20,590 --> 00:04:22,750
Now, my derivative term, DD

112
00:04:22,760 --> 00:04:24,170
Theta 1, J of Theta

113
00:04:24,180 --> 00:04:25,810
1, when you evaluate to that

114
00:04:25,820 --> 00:04:27,330
this point, going to look

115
00:04:27,340 --> 00:04:29,990
at the slope

116
00:04:30,000 --> 00:04:31,750
of that line, so this

117
00:04:31,760 --> 00:04:34,500
derivative term is a slope of this line.

118
00:04:34,510 --> 00:04:36,050
But this line is slanting down.

119
00:04:36,060 --> 00:04:39,050
So this line has negative slope.

120
00:04:41,110 --> 00:04:41,810
Right.

121
00:04:41,820 --> 00:04:43,040
Or alternatively, I could

122
00:04:43,050 --> 00:04:44,630
say that this function has a

123
00:04:44,640 --> 00:04:48,170
negative derivative (which just means negative slope) at that point.

124
00:04:48,180 --> 00:04:50,840
So this is less than or equal to zero.

125
00:04:50,850 --> 00:04:53,460
So when I update Theta,

126
00:04:53,470 --> 00:04:55,360
Theta is updated as

127
00:04:55,370 --> 00:04:57,990
Theta minus Alpha times a

128
00:04:58,000 --> 00:05:02,620
negative number.

129
00:05:02,630 --> 00:05:03,990
So I have theta one

130
00:05:04,000 --> 00:05:06,180
minus a negative number, which

131
00:05:06,190 --> 00:05:07,380
means I'm actually going to

132
00:05:07,390 --> 00:05:08,930
increase data, right because

133
00:05:08,940 --> 00:05:10,750
this is minus, of a negative

134
00:05:10,760 --> 00:05:11,960
number, that means I'm adding

135
00:05:11,970 --> 00:05:13,570
something with data, and what

136
00:05:13,580 --> 00:05:14,550
that means is that I'm going

137
00:05:14,560 --> 00:05:16,700
to end up increasing data, and

138
00:05:16,710 --> 00:05:18,290
so we'll start here and increase

139
00:05:18,300 --> 00:05:19,940
theta, which again seems

140
00:05:19,950 --> 00:05:21,120
like the thing I wanted to

141
00:05:21,130 --> 00:05:22,340
do, to try to get

142
00:05:22,350 --> 00:05:25,330
me closer to the minimum.

143
00:05:26,370 --> 00:05:28,240
So this hopefully explains the

144
00:05:28,250 --> 00:05:29,500
intuition behind what the

145
00:05:29,510 --> 00:05:31,500
derivative term is doing,

146
00:05:31,510 --> 00:05:32,700
let's next take a look

147
00:05:32,710 --> 00:05:34,410
at the learning rates from alpha,

148
00:05:34,420 --> 00:05:37,540
and try to figure out what that's doing.

149
00:05:38,020 --> 00:05:39,800
So, here's my theta

150
00:05:39,810 --> 00:05:41,740
descent update rule, right, it's

151
00:05:41,750 --> 00:05:44,130
this equation, so let's

152
00:05:44,140 --> 00:05:46,240
look at what can

153
00:05:46,250 --> 00:05:47,500
happen if alpha is either

154
00:05:47,510 --> 00:05:50,670
too small, or alpha is too large.

155
00:05:50,680 --> 00:05:53,730
So this first example, what happens if alpha is too.

156
00:05:53,740 --> 00:05:57,460
So here's my function J

157
00:05:57,670 --> 00:06:00,180
- J of theta.

158
00:06:00,380 --> 00:06:02,370
Let's say I start here.

159
00:06:02,380 --> 00:06:05,100
If alpha's too small, then

160
00:06:05,110 --> 00:06:06,660
what I'm going to do is multiply

161
00:06:06,670 --> 00:06:08,140
my update by some small number,

162
00:06:08,150 --> 00:06:11,210
so I end up taking a baby step like that.

163
00:06:11,220 --> 00:06:11,350
Ok?

164
00:06:11,360 --> 00:06:13,280
So that's one step we've already said.

165
00:06:13,290 --> 00:06:14,560
Then from this new point I'm

166
00:06:14,570 --> 00:06:15,700
going to take another step -but if

167
00:06:15,710 --> 00:06:17,430
alpha's too small I'll

168
00:06:17,440 --> 00:06:19,920
take another little baby

169
00:06:19,930 --> 00:06:22,560
step and so

170
00:06:22,570 --> 00:06:23,880
if my loaning rate is too

171
00:06:23,890 --> 00:06:25,630
small, I'm going to

172
00:06:25,640 --> 00:06:28,060
end up taking these tiny,

173
00:06:28,070 --> 00:06:30,130
tiny baby steps, to try

174
00:06:30,140 --> 00:06:31,970
to get to the minimum, and I'm

175
00:06:31,980 --> 00:06:33,550
going to need a lot of steps

176
00:06:33,560 --> 00:06:35,240
to get to the minimum, and so

177
00:06:35,250 --> 00:06:37,200
if alpha's too small, gradient

178
00:06:37,210 --> 00:06:38,430
descent can be slow because I'm

179
00:06:38,440 --> 00:06:39,930
just going to take these tiny, tiny

180
00:06:39,940 --> 00:06:40,920
baby steps, so it's going

181
00:06:40,930 --> 00:06:42,650
to need a lot of steps

182
00:06:42,660 --> 00:06:46,040
before it gets anywhere close to the local minimum.

183
00:06:46,670 --> 00:06:48,980
Now, how about if alpha's too large?

184
00:06:48,990 --> 00:06:52,310
So here's my function J of theta.

185
00:06:52,320 --> 00:06:53,790
Turns out, if alpha's too

186
00:06:53,800 --> 00:06:56,010
large then gradient descent

187
00:06:56,020 --> 00:06:57,670
can overshoot the minimum and

188
00:06:57,680 --> 00:06:59,050
even fail to converge,

189
00:06:59,060 --> 00:07:01,000
or even diverge, so here's what I mean.

190
00:07:01,010 --> 00:07:04,080
Let's say I start off theta there that's pretty close the minimum.

191
00:07:04,090 --> 00:07:05,390
So the derivative points to the

192
00:07:05,400 --> 00:07:07,090
right, but if alpha's

193
00:07:07,100 --> 00:07:07,990
too big, I'm going to

194
00:07:08,000 --> 00:07:09,990
take a huge step - a huge step like that.

195
00:07:10,000 --> 00:07:13,140
So I'm going to end up taking a huge step.

196
00:07:13,150 --> 00:07:14,570
And now my cost function's actually gotten

197
00:07:14,580 --> 00:07:15,760
worse cause I start with

198
00:07:15,770 --> 00:07:19,340
this value, but now, my value's actually got worse.

199
00:07:19,350 --> 00:07:20,520
Now my derivative, you know,

200
00:07:20,530 --> 00:07:23,510
points to the left and says I should decrease data.

201
00:07:23,520 --> 00:07:24,470
But if my loaning rate is

202
00:07:24,480 --> 00:07:25,500
too big, I may take a

203
00:07:25,510 --> 00:07:28,190
huge step going from here all the way out there.

204
00:07:28,200 --> 00:07:29,300
So I end up going out

205
00:07:29,310 --> 00:07:31,260
there, right, and if

206
00:07:31,270 --> 00:07:32,130
my loaning rate is too big

207
00:07:32,140 --> 00:07:34,060
I can take another huge step on

208
00:07:34,070 --> 00:07:35,310
the next acceleration, I kind of

209
00:07:35,320 --> 00:07:38,140
overshoot, and overshoot, and

210
00:07:38,150 --> 00:07:40,350
so on, until you'll notice

211
00:07:40,360 --> 00:07:41,980
I'm getting further and further

212
00:07:41,990 --> 00:07:43,930
away from the minimum.

213
00:07:43,940 --> 00:07:45,910
And so if alpha's too large

214
00:07:45,920 --> 00:07:49,460
it can fail to converge or even diverge.

215
00:07:49,470 --> 00:07:51,900
Now, I have another question for you.

216
00:07:51,910 --> 00:07:53,180
So this is a tricky

217
00:07:53,190 --> 00:07:54,110
one, and when I was

218
00:07:54,120 --> 00:07:56,690
first learning this stuff, it actually took me a long time to figure this out.

219
00:07:56,700 --> 00:07:58,140
But what if your parameter alpha

220
00:07:58,150 --> 00:08:00,670
one is already at the local minimum?

221
00:08:00,680 --> 00:08:04,700
What do you think one step of gradient descent will do?

222
00:08:06,400 --> 00:08:07,870
So, let's suppose to you

223
00:08:07,880 --> 00:08:10,210
initialize theta one at a local minimum.

224
00:08:10,220 --> 00:08:11,960
So suppose this is

225
00:08:11,970 --> 00:08:13,990
your initial value of theta one

226
00:08:14,000 --> 00:08:15,620
over here, and it's

227
00:08:15,630 --> 00:08:19,730
already at a local optimum, the local minimum.

228
00:08:19,890 --> 00:08:21,450
It depends on the local optimum, your

229
00:08:21,460 --> 00:08:22,700
derivative will be equal

230
00:08:22,710 --> 00:08:24,970
zero so that slope that

231
00:08:24,980 --> 00:08:26,530
tangent point so the

232
00:08:26,540 --> 00:08:28,120
slope of this line

233
00:08:28,130 --> 00:08:29,750
will be equal to

234
00:08:29,760 --> 00:08:32,130
zero and, thus, this

235
00:08:32,140 --> 00:08:36,330
derivative term is equal to zero.

236
00:08:36,340 --> 00:08:37,710
And so, in your gradient

237
00:08:37,720 --> 00:08:39,470
descent update, you have theta(1)

238
00:08:39,480 --> 00:08:41,700
gets updated as theta(1) minus

239
00:08:41,710 --> 00:08:43,930
alpha times 0.

240
00:08:43,940 --> 00:08:45,330
And so, what this means

241
00:08:45,340 --> 00:08:46,990
is that, if you're already

242
00:08:47,000 --> 00:08:48,670
at a local optimum, it leaves

243
00:08:48,680 --> 00:08:50,410
theta(1) unchanged, because you

244
00:08:50,420 --> 00:08:53,900
see it updates as theta(1)  equals theta(1).

245
00:08:53,910 --> 00:08:55,840
So, if your parameters

246
00:08:55,850 --> 00:08:57,540
are already at a local minimum,

247
00:08:57,550 --> 00:08:58,930
one step of gradient descent does absolutely nothing.

248
00:08:58,940 --> 00:09:00,920
It doesn't change the parameter,

249
00:09:00,930 --> 00:09:01,970
which is what you want because

250
00:09:01,980 --> 00:09:04,090
it keeps your solution at

251
00:09:04,100 --> 00:09:06,420
the local optimum.

252
00:09:06,430 --> 00:09:08,020
This also explains why gradient descent

253
00:09:08,030 --> 00:09:09,770
can converge to local minimum

254
00:09:09,780 --> 00:09:13,060
even with the learning rate alpha fixed.

255
00:09:13,070 --> 00:09:14,160
Here's what I mean by that.

256
00:09:14,170 --> 00:09:15,480
Let's look at an example.

257
00:09:15,490 --> 00:09:17,230
So, here's a cost function

258
00:09:17,240 --> 00:09:22,220
J, or theta, that maybe I wanna minimize.

259
00:09:22,230 --> 00:09:23,590
And let's say I initialize

260
00:09:23,600 --> 00:09:25,580
my algorithm, my gradient

261
00:09:25,590 --> 00:09:28,970
descent algorithm up there, at that magenta point.

262
00:09:28,980 --> 00:09:30,250
if I take one step of gradient

263
00:09:30,260 --> 00:09:31,250
descent, maybe it'll take me

264
00:09:31,260 --> 00:09:35,980
to that point, because my derivative's pretty steep out there, right?

265
00:09:35,990 --> 00:09:37,650
Now, I'm at this

266
00:09:37,660 --> 00:09:39,130
green point, and if

267
00:09:39,140 --> 00:09:40,260
I take another step on green

268
00:09:40,270 --> 00:09:42,020
descent you'll notice my

269
00:09:42,030 --> 00:09:44,520
derivative, meaning the slope

270
00:09:44,530 --> 00:09:45,740
is less steep at the

271
00:09:45,750 --> 00:09:48,050
green point compared to

272
00:09:48,060 --> 00:09:49,250
at the magenta point out there.

273
00:09:49,260 --> 00:09:50,640
Because as I approach the

274
00:09:50,650 --> 00:09:52,790
minimum, my derivative get closer

275
00:09:52,800 --> 00:09:54,000
and closer to zero as

276
00:09:54,010 --> 00:09:57,220
I approach the minimum.

277
00:09:57,230 --> 00:09:58,350
So, after one step of

278
00:09:58,360 --> 00:10:00,590
gradient descent, my new derivative

279
00:10:00,600 --> 00:10:02,570
is a little bit smaller, so when

280
00:10:02,580 --> 00:10:03,840
I take another step of gradient

281
00:10:03,850 --> 00:10:06,630
descent, I will naturally,

282
00:10:06,640 --> 00:10:08,030
take a small step from

283
00:10:08,040 --> 00:10:10,980
this green point to the magenta point.

284
00:10:10,990 --> 00:10:12,840
Now from the new point to the red point.

285
00:10:12,850 --> 00:10:14,430
Now I'm even closer to the

286
00:10:14,440 --> 00:10:16,020
minimum, so the derivative here

287
00:10:16,030 --> 00:10:19,360
will be even smaller than it was at the green point.

288
00:10:19,370 --> 00:10:21,960
So I want to take another step of gradient descent.

289
00:10:21,970 --> 00:10:25,220
Now, my derivative term is even smaller.

290
00:10:25,230 --> 00:10:26,710
And so the magnitude of the

291
00:10:26,720 --> 00:10:28,300
update to theta 1

292
00:10:28,310 --> 00:10:29,880
is even smaller so take a

293
00:10:29,890 --> 00:10:32,170
smaller step like so, and as

294
00:10:32,180 --> 00:10:37,180
gradient descent runs you will

295
00:10:37,190 --> 00:10:40,080
automatically take smaller and

296
00:10:40,090 --> 00:10:42,680
smaller steps until eventually

297
00:10:42,690 --> 00:10:44,650
you are taking very small steps,

298
00:10:44,660 --> 00:10:45,860
you know, and you find the

299
00:10:45,870 --> 00:10:50,140
conversion to the, to the local minimum.

300
00:10:50,240 --> 00:10:52,280
So, just to recap, in

301
00:10:52,290 --> 00:10:54,130
gradient descent, as we approach

302
00:10:54,140 --> 00:10:56,180
a local minimum, gradient descent

303
00:10:56,190 --> 00:10:58,240
will automatically take smaller steps,

304
00:10:58,250 --> 00:10:59,870
and that's because as we approach

305
00:10:59,880 --> 00:11:01,740
the local minimum, by definition

306
00:11:01,750 --> 00:11:03,410
the local minimum is when you

307
00:11:03,420 --> 00:11:06,010
know there is derivative equal to zero.

308
00:11:06,020 --> 00:11:07,910
So as we approach the local minimum

309
00:11:07,920 --> 00:11:10,230
this derivative term will automatically

310
00:11:10,240 --> 00:11:11,870
get smaller and so

311
00:11:11,880 --> 00:11:13,600
gradient descent will automatically take

312
00:11:13,610 --> 00:11:16,390
smaller steps, so this

313
00:11:16,400 --> 00:11:17,630
is what gradient descent looks like,

314
00:11:17,640 --> 00:11:19,460
and so actually no need

315
00:11:19,470 --> 00:11:22,310
to decrease alpha over time.

316
00:11:22,790 --> 00:11:24,290
So that's the gradient

317
00:11:24,300 --> 00:11:25,780
descent algorithm, and you can

318
00:11:25,790 --> 00:11:27,290
use it to minimise, to try

319
00:11:27,300 --> 00:11:29,320
to minimise any cost function J,

320
00:11:29,330 --> 00:11:32,860
not the cost function J we defined from linear regression.

321
00:11:32,870 --> 00:11:34,380
In the next video, we're going

322
00:11:34,390 --> 00:11:35,700
to take the function J and

323
00:11:35,710 --> 00:11:36,800
set that back to be

324
00:11:36,810 --> 00:11:39,310
exactly linear regression's cost function,

325
00:11:39,320 --> 00:11:40,670
the square cost function that we

326
00:11:40,680 --> 00:11:42,610
came up with earlier, and taking

327
00:11:42,620 --> 00:11:44,300
gradient descent and square cost

328
00:11:44,310 --> 00:11:45,790
function and putting them together,

329
00:11:45,800 --> 00:11:48,760
that will give us our first learning algorithm.

330
00:11:48,770 --> 00:11:51,800
That will give us our linear regression algorithm.

