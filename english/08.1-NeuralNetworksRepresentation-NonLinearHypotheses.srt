1
00:00:00,440 --> 00:00:01,470
In this and in the

2
00:00:01,480 --> 00:00:02,770
next set of videos, I'd like

3
00:00:02,780 --> 00:00:04,540
to tell you about a learning

4
00:00:04,550 --> 00:00:07,180
algorithm called a Neural Network.

5
00:00:07,190 --> 00:00:08,070
We're going to first talk about

6
00:00:08,080 --> 00:00:09,590
the representation and then

7
00:00:09,600 --> 00:00:10,400
in the next set of videos

8
00:00:10,410 --> 00:00:12,650
talk about learning algorithms for it.

9
00:00:12,660 --> 00:00:14,500
Neutral networks is actually

10
00:00:14,510 --> 00:00:16,280
a pretty old idea, but had

11
00:00:16,290 --> 00:00:18,190
fallen out of favor for a while.

12
00:00:18,200 --> 00:00:19,570
But today, it is the

13
00:00:19,580 --> 00:00:21,080
state of the art technique for

14
00:00:21,090 --> 00:00:23,730
many different machine learning problems.

15
00:00:23,740 --> 00:00:26,290
So why do we need yet another learning algorithm?

16
00:00:26,300 --> 00:00:28,170
We already have linear regression and

17
00:00:28,180 --> 00:00:32,270
we have logistic regression, so why do we need, you know, neural networks?

18
00:00:32,280 --> 00:00:34,780
In order to motivate the discussion

19
00:00:34,790 --> 00:00:36,110
of neural networks, let me

20
00:00:36,120 --> 00:00:37,300
start by showing you a few

21
00:00:37,310 --> 00:00:38,920
examples of machine learning

22
00:00:38,930 --> 00:00:40,290
problems where we need

23
00:00:40,300 --> 00:00:43,350
to learn complex non-linear hypotheses.

24
00:00:43,850 --> 00:00:46,520
Consider a supervised learning classification

25
00:00:46,530 --> 00:00:49,270
problem where you have a training set like this.

26
00:00:49,280 --> 00:00:50,950
If you want to apply logistic

27
00:00:50,960 --> 00:00:52,890
regression to this problem, one

28
00:00:52,900 --> 00:00:54,650
thing you could do is apply

29
00:00:54,660 --> 00:00:56,180
logistic regression with a

30
00:00:56,190 --> 00:00:58,160
lot of nonlinear features like that.

31
00:00:58,170 --> 00:01:00,060
So here, g as usual

32
00:01:00,070 --> 00:01:01,770
is the sigmoid function, and we

33
00:01:01,780 --> 00:01:05,440
can include lots of polynomial terms like these.

34
00:01:05,450 --> 00:01:07,360
And, if you include enough polynomial

35
00:01:07,370 --> 00:01:08,940
terms then, you know, maybe

36
00:01:08,950 --> 00:01:11,590
you can get a hypotheses

37
00:01:11,600 --> 00:01:14,620
that separates the positive and negative examples.

38
00:01:14,630 --> 00:01:16,460
This particular method works well

39
00:01:16,470 --> 00:01:18,610
when you have only, say, two

40
00:01:18,620 --> 00:01:20,180
features - x1 and x2

41
00:01:20,190 --> 00:01:21,490
- because you can then include

42
00:01:21,500 --> 00:01:23,390
all those polynomial terms of

43
00:01:23,400 --> 00:01:24,800
x1 and x2.

44
00:01:24,810 --> 00:01:26,510
But for many interesting machine learning

45
00:01:26,520 --> 00:01:27,900
problems would have a

46
00:01:27,910 --> 00:01:30,730
lot more features than just two.

47
00:01:30,780 --> 00:01:32,310
We've been talking for a while

48
00:01:32,320 --> 00:01:35,120
about housing prediction, and suppose

49
00:01:35,130 --> 00:01:38,010
you have a housing classification

50
00:01:38,020 --> 00:01:39,380
problem rather than a

51
00:01:39,390 --> 00:01:41,570
regression problem, like maybe

52
00:01:41,580 --> 00:01:43,430
if you have different features of

53
00:01:43,440 --> 00:01:45,000
a house, and you want

54
00:01:45,010 --> 00:01:46,040
to predict what are the

55
00:01:46,050 --> 00:01:47,690
odds that your house will

56
00:01:47,700 --> 00:01:48,900
be sold within the next

57
00:01:48,910 --> 00:01:52,090
six months, so that will be a classification problem.

58
00:01:52,100 --> 00:01:53,250
And as we saw we can

59
00:01:53,260 --> 00:01:55,250
come up with quite a

60
00:01:55,260 --> 00:01:56,830
lot of features, maybe a hundred

61
00:01:56,840 --> 00:01:59,770
different features of different houses.

62
00:02:00,130 --> 00:02:01,870
For a problem like this, if

63
00:02:01,880 --> 00:02:03,360
you were to include all the

64
00:02:03,370 --> 00:02:05,090
quadratic terms, all of

65
00:02:05,100 --> 00:02:06,530
these, even all of the

66
00:02:06,540 --> 00:02:07,920
quadratic that is the second

67
00:02:07,930 --> 00:02:10,550
or the polynomial terms, there would be a lot of them.

68
00:02:10,560 --> 00:02:12,950
There would be terms like x1 squared,

69
00:02:12,960 --> 00:02:18,740
x1x2, x1x3, you know, x1x4

70
00:02:18,750 --> 00:02:21,970
up to x1x100 and then

71
00:02:21,980 --> 00:02:25,120
you have x2 squared, x2x3

72
00:02:25,620 --> 00:02:26,500
and so on.

73
00:02:26,510 --> 00:02:28,050
And if you include just

74
00:02:28,060 --> 00:02:29,320
the second order terms, that

75
00:02:29,330 --> 00:02:30,830
is, the terms that are

76
00:02:30,840 --> 00:02:32,210
a product of, you know,

77
00:02:32,220 --> 00:02:33,500
two of these terms, x1

78
00:02:33,510 --> 00:02:35,770
times x1 and so on, then,

79
00:02:35,780 --> 00:02:38,170
for the case of n equals

80
00:02:38,180 --> 00:02:41,780
100, you end up with about five thousand features.

81
00:02:41,890 --> 00:02:44,990
And, asymptotically, the

82
00:02:45,000 --> 00:02:46,760
number of quadratic features grows

83
00:02:46,770 --> 00:02:48,810
roughly as order n

84
00:02:48,820 --> 00:02:50,450
squared, where n is the

85
00:02:50,460 --> 00:02:53,360
number of the original features,

86
00:02:53,370 --> 00:02:55,690
like x1 through x100 that we had.

87
00:02:55,700 --> 00:02:59,910
And its actually closer to n squared over two.

88
00:02:59,920 --> 00:03:01,550
So including all the

89
00:03:01,560 --> 00:03:03,210
quadratic features doesn't seem

90
00:03:03,220 --> 00:03:04,290
like it's maybe a good

91
00:03:04,300 --> 00:03:05,570
idea, because that is a

92
00:03:05,580 --> 00:03:07,210
lot of features and you

93
00:03:07,220 --> 00:03:09,320
might up overfitting the training

94
00:03:09,330 --> 00:03:10,730
set, and it can

95
00:03:10,740 --> 00:03:14,070
also be computationally expensive, you know, to

96
00:03:14,080 --> 00:03:16,440
be working with that many features.

97
00:03:16,450 --> 00:03:17,760
One thing you could do is

98
00:03:17,770 --> 00:03:19,280
include only a subset of

99
00:03:19,290 --> 00:03:21,040
these, so if you include only the

100
00:03:21,050 --> 00:03:23,580
features x1 squared, x2 squared,

101
00:03:23,590 --> 00:03:25,570
x3 squared, up to

102
00:03:25,580 --> 00:03:28,090
maybe x100 squared, then

103
00:03:28,100 --> 00:03:29,970
the number of features is much smaller.

104
00:03:29,980 --> 00:03:32,060
Here you have only 100 such

105
00:03:32,070 --> 00:03:34,110
quadratic features, but this

106
00:03:34,120 --> 00:03:36,090
is not enough features and

107
00:03:36,100 --> 00:03:37,280
certainly won't let you fit

108
00:03:37,290 --> 00:03:39,560
the data set like that on the upper left.

109
00:03:39,570 --> 00:03:41,030
In fact, if you include

110
00:03:41,040 --> 00:03:43,160
only these quadratic features together

111
00:03:43,170 --> 00:03:45,340
with the original x1, and

112
00:03:45,350 --> 00:03:47,450
so on, up to x100 features,

113
00:03:47,460 --> 00:03:48,900
then you can actually fit very

114
00:03:48,910 --> 00:03:50,320
interesting hypotheses. So, you

115
00:03:50,330 --> 00:03:52,480
can fit things like, you know, access a

116
00:03:52,490 --> 00:03:55,070
line of the ellipses like these, but

117
00:03:55,080 --> 00:03:56,330
you certainly cannot fit a more

118
00:03:56,340 --> 00:03:59,350
complex data set like that shown here.

119
00:03:59,360 --> 00:04:00,610
So 5000 features seems like

120
00:04:00,620 --> 00:04:03,220
a lot, if you were

121
00:04:03,230 --> 00:04:05,130
to include the cubic, or

122
00:04:05,140 --> 00:04:06,430
third order known of each others,

123
00:04:06,440 --> 00:04:08,390
the x1, x2, x3.

124
00:04:08,400 --> 00:04:10,300
You know, x1 squared,

125
00:04:10,310 --> 00:04:12,890
x2, x10 and

126
00:04:12,900 --> 00:04:15,690
x11, x17 and so on.

127
00:04:15,700 --> 00:04:19,030
You can imagine there are gonna be a lot of these features.

128
00:04:19,040 --> 00:04:20,040
In fact, they are going to be

129
00:04:20,050 --> 00:04:22,200
order and cube such features

130
00:04:22,210 --> 00:04:24,140
and if any is 100

131
00:04:24,150 --> 00:04:25,730
you can compute that, you

132
00:04:25,740 --> 00:04:27,720
end up with on the order

133
00:04:27,730 --> 00:04:30,030
of about 170,000 such cubic

134
00:04:30,040 --> 00:04:32,250
features and so including

135
00:04:32,260 --> 00:04:34,910
these higher auto-polynomial features when

136
00:04:34,920 --> 00:04:36,220
your original feature set end

137
00:04:36,230 --> 00:04:38,520
is large this really dramatically

138
00:04:38,530 --> 00:04:41,060
blows up your feature space and

139
00:04:41,070 --> 00:04:42,310
this doesn't seem like a

140
00:04:42,320 --> 00:04:43,550
good way to come up with

141
00:04:43,560 --> 00:04:45,230
additional features with which

142
00:04:45,240 --> 00:04:49,580
to build none many classifiers when n is large.

143
00:04:49,590 --> 00:04:53,260
For many machine learning problems, n will be pretty large.

144
00:04:53,270 --> 00:04:54,990
Here's an example.

145
00:04:55,000 --> 00:04:59,640
Let's consider the problem of computer vision.

146
00:04:59,670 --> 00:05:01,250
And suppose you want to

147
00:05:01,260 --> 00:05:02,700
use machine learning to train

148
00:05:02,710 --> 00:05:04,700
a classifier to examine an

149
00:05:04,710 --> 00:05:06,150
image and tell us whether

150
00:05:06,160 --> 00:05:09,470
or not the image is a car.

151
00:05:09,480 --> 00:05:12,380
Many people wonder why computer vision could be difficult.

152
00:05:12,390 --> 00:05:13,260
I mean when you and I

153
00:05:13,270 --> 00:05:15,890
look at this picture it is so obvious what this is.

154
00:05:15,900 --> 00:05:17,180
You wonder how is it

155
00:05:17,190 --> 00:05:18,900
that a learning algorithm could possibly

156
00:05:18,910 --> 00:05:22,100
fail to know what this picture is.

157
00:05:22,110 --> 00:05:23,710
To understand why computer vision

158
00:05:23,720 --> 00:05:25,640
is hard let's zoom

159
00:05:25,650 --> 00:05:26,930
into a small part of the

160
00:05:26,940 --> 00:05:28,500
image like that area where the

161
00:05:28,510 --> 00:05:30,390
little red rectangle is.

162
00:05:30,400 --> 00:05:31,440
It turns out that where you

163
00:05:31,450 --> 00:05:34,770
and I see a car, the computer sees that.

164
00:05:34,780 --> 00:05:36,590
What it sees is this matrix,

165
00:05:36,600 --> 00:05:38,570
or this grid, of pixel

166
00:05:38,580 --> 00:05:40,600
intensity values that tells

167
00:05:40,610 --> 00:05:43,630
us the brightness of each pixel in the image.

168
00:05:43,640 --> 00:05:45,540
So the computer vision problem is

169
00:05:45,550 --> 00:05:47,300
to look at this matrix of

170
00:05:47,310 --> 00:05:49,400
pixel intensity values, and tell

171
00:05:49,410 --> 00:05:53,940
us that these numbers represent the door handle of a car.

172
00:05:54,230 --> 00:05:56,020
Concretely, when we use

173
00:05:56,030 --> 00:05:57,420
machine learning to build a

174
00:05:57,430 --> 00:05:59,350
car detector, what we do

175
00:05:59,360 --> 00:06:00,790
is we come up with a

176
00:06:00,800 --> 00:06:02,880
label training set, with, let's

177
00:06:02,890 --> 00:06:04,720
say, a few label examples

178
00:06:04,730 --> 00:06:05,990
of cars and a few

179
00:06:06,000 --> 00:06:07,370
label examples of things that

180
00:06:07,380 --> 00:06:09,080
are not cars, then we

181
00:06:09,090 --> 00:06:10,710
give our training set to

182
00:06:10,720 --> 00:06:12,300
the learning algorithm trained a

183
00:06:12,310 --> 00:06:13,670
classifier and then, you

184
00:06:13,680 --> 00:06:14,880
know, we may test it and show

185
00:06:14,890 --> 00:06:17,970
the new image and ask, "What is this new thing?".

186
00:06:17,980 --> 00:06:21,400
And hopefully it will recognize that that is a car.

187
00:06:21,410 --> 00:06:24,110
To understand why we

188
00:06:24,120 --> 00:06:27,040
need nonlinear hypotheses, let's take

189
00:06:27,050 --> 00:06:28,180
a look at some of the

190
00:06:28,190 --> 00:06:29,470
images of cars and maybe

191
00:06:29,480 --> 00:06:32,950
non-cars that we might feed to our learning algorithm.

192
00:06:32,960 --> 00:06:34,080
Let's pick a couple of pixel

193
00:06:34,090 --> 00:06:35,740
locations in our images, so

194
00:06:35,750 --> 00:06:37,170
that's pixel one location and

195
00:06:37,180 --> 00:06:39,720
pixel two location, and let's

196
00:06:39,730 --> 00:06:42,500
plot this car, you know, at the

197
00:06:42,510 --> 00:06:44,350
location, at a certain

198
00:06:44,360 --> 00:06:46,420
point, depending on the intensities

199
00:06:46,430 --> 00:06:49,250
of pixel one and pixel two.

200
00:06:49,260 --> 00:06:51,050
And let's do this with a few other images.

201
00:06:51,060 --> 00:06:52,970
So let's take a different example

202
00:06:52,980 --> 00:06:54,070
of the car and you know,

203
00:06:54,080 --> 00:06:56,150
look at the same two pixel locations

204
00:06:56,160 --> 00:06:57,760
and that image has a

205
00:06:57,770 --> 00:06:59,220
different intensity for pixel one

206
00:06:59,230 --> 00:07:00,950
and a different intensity for pixel two.

207
00:07:00,960 --> 00:07:03,350
So, it ends up at a different location on the figure.

208
00:07:03,360 --> 00:07:05,980
And then let's plot some negative examples as well.

209
00:07:05,990 --> 00:07:07,710
That's a non-car, that's a

210
00:07:07,720 --> 00:07:09,720
non-car  .

211
00:07:09,730 --> 00:07:11,060
And if we do this for

212
00:07:11,070 --> 00:07:13,270
more and more examples using

213
00:07:13,280 --> 00:07:15,070
the pluses to denote cars

214
00:07:15,080 --> 00:07:16,880
and minuses to denote non-cars,

215
00:07:16,890 --> 00:07:18,820
what we'll find is that

216
00:07:18,830 --> 00:07:20,880
the cars and non-cars end up

217
00:07:20,890 --> 00:07:22,560
lying in different regions of

218
00:07:22,570 --> 00:07:25,170
the space, and what we

219
00:07:25,180 --> 00:07:26,740
need therefore is some sort

220
00:07:26,750 --> 00:07:28,990
of non-linear hypotheses to try

221
00:07:29,000 --> 00:07:32,400
to separate out the two classes.

222
00:07:32,480 --> 00:07:35,280
What is the dimension of the feature space?

223
00:07:35,290 --> 00:07:38,760
Suppose we were to use just 50 by 50 pixel images.

224
00:07:38,770 --> 00:07:40,510
Now that suppose our images were

225
00:07:40,520 --> 00:07:43,460
pretty small ones, just 50 pixels on the side.

226
00:07:43,470 --> 00:07:46,320
Then we would have 2500 pixels,

227
00:07:46,330 --> 00:07:47,730
and so the dimension of

228
00:07:47,740 --> 00:07:49,510
our feature size will be N

229
00:07:49,520 --> 00:07:51,690
equals 2500 where our feature

230
00:07:51,700 --> 00:07:53,170
vector x is a list

231
00:07:53,180 --> 00:07:54,700
of all the pixel testings, you

232
00:07:54,710 --> 00:07:57,070
know, the pixel brightness of pixel

233
00:07:57,080 --> 00:07:58,320
one, the brightness of pixel

234
00:07:58,330 --> 00:07:59,860
two, and so on down

235
00:07:59,870 --> 00:08:01,390
to the pixel brightness of the

236
00:08:01,400 --> 00:08:03,580
last pixel where, you know, in a

237
00:08:03,590 --> 00:08:05,530
typical computer representation, each of

238
00:08:05,540 --> 00:08:07,470
these may be values between say

239
00:08:07,480 --> 00:08:09,220
0 to 255 if it gives

240
00:08:09,230 --> 00:08:12,510
us the grayscale value.

241
00:08:12,520 --> 00:08:13,940
So we have n equals 2500,

242
00:08:13,950 --> 00:08:15,730
and that's if we

243
00:08:15,740 --> 00:08:17,780
were using grayscale images.

244
00:08:17,790 --> 00:08:19,430
If we were using RGB

245
00:08:19,440 --> 00:08:21,410
images with separate red, green

246
00:08:21,420 --> 00:08:25,370
and blue values, we would have n equals 7500.

247
00:08:27,650 --> 00:08:28,990
So, if we were to

248
00:08:29,000 --> 00:08:30,360
try to learn a nonlinear

249
00:08:30,370 --> 00:08:32,290
hypothesis by including all

250
00:08:32,300 --> 00:08:33,800
the quadratic features, that is

251
00:08:33,810 --> 00:08:35,420
all the terms of the form, you know,

252
00:08:35,430 --> 00:08:39,120
Xi times Xj, while with the

253
00:08:39,130 --> 00:08:40,570
2500 pixels we would end

254
00:08:40,580 --> 00:08:43,040
up with a total of three million features.

255
00:08:43,050 --> 00:08:44,710
And that's just too large to

256
00:08:44,720 --> 00:08:46,590
be reasonable; the computation would

257
00:08:46,600 --> 00:08:48,830
be very expensive to find and

258
00:08:48,840 --> 00:08:50,300
to represent all of these

259
00:08:50,310 --> 00:08:53,750
three million features per training example.

260
00:08:55,470 --> 00:08:58,090
So, simple logistic regression together

261
00:08:58,100 --> 00:08:59,290
with adding in maybe the

262
00:08:59,300 --> 00:09:00,920
quadratic or the cubic features

263
00:09:00,930 --> 00:09:01,970
- that's just not a

264
00:09:01,980 --> 00:09:04,540
good way to learn complex

265
00:09:04,550 --> 00:09:06,300
nonlinear hypotheses when n

266
00:09:06,310 --> 00:09:09,360
is large because you just end up with too many features.

267
00:09:09,370 --> 00:09:10,830
In the next few videos, I

268
00:09:10,840 --> 00:09:12,070
would like to tell you about Neural

269
00:09:12,080 --> 00:09:13,970
Networks, which turns out

270
00:09:13,980 --> 00:09:15,640
to be a much better way to

271
00:09:15,650 --> 00:09:17,950
learn complex hypotheses, complex nonlinear

272
00:09:17,960 --> 00:09:20,060
hypotheses even when your

273
00:09:20,070 --> 00:09:22,850
input feature space, even when n is large.

274
00:09:22,860 --> 00:09:24,410
And along the way I'll

275
00:09:24,420 --> 00:09:25,770
also get to show

276
00:09:25,780 --> 00:09:27,230
you a couple of fun videos

277
00:09:27,240 --> 00:09:30,290
of historically important applications

278
00:09:30,300 --> 00:09:32,090
of Neural networks as well that I

279
00:09:32,100 --> 00:09:33,560
hope those videos that

280
00:09:33,570 --> 00:09:36,960
we'll see later will be fun for you to watch as well.

